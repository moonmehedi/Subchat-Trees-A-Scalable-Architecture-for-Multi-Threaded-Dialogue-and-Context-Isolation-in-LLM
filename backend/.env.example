# ============================================================================
# HIERARCHICAL CHAT BACKEND - ENVIRONMENT CONFIGURATION
# ============================================================================
# Research Project: Multi-Threaded Dialogue and Context Isolation in LLMs
# Copy this file to .env and configure with your actual values
# ============================================================================

# =============================================================================
# CORE APPLICATION SETTINGS
# =============================================================================
# Environment: development, staging, production
ENVIRONMENT=development
DEBUG=true
API_VERSION=1.0.0
APP_NAME="Hierarchical Chat Research Backend"

# Security
SECRET_KEY=your-super-secret-research-key-change-this-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# Server Configuration
HOST=0.0.0.0
PORT=8000

# =============================================================================
# DATABASE CONFIGURATION (PostgreSQL for Research Data Persistence)
# =============================================================================
# PostgreSQL Database (recommended for research data integrity)
DATABASE_URL=postgresql://research_user:research_password@localhost:5432/hierarchical_chat_db

# Development SQLite (fallback for local development)
DEV_DATABASE_URL=sqlite:///./research_data.db
USE_SQLITE_FOR_DEV=true

# =============================================================================
# LLM CONFIGURATION (OpenAI 4o-mini for Research)
# =============================================================================
# OpenAI API Key (required for LLM responses)
# =============================================================================
# LLM API KEYS
# =============================================================================
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
LANGCHAIN_API_KEY=your_langchain_api_key_here

# Model Configuration (4o-mini for cost-effective research)
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2000
OPENAI_REQUEST_TIMEOUT=60

# =============================================================================
# VECTOR STORE CONFIGURATION (ChromaDB + Sentence Transformers)
# =============================================================================
# ChromaDB Configuration (for research vector storage)
CHROMA_PERSIST_DIRECTORY=./data/chromadb
CHROMA_COLLECTION_NAME=hierarchical_chat_research

# Sentence Transformers Model (local embeddings - no API costs)
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu  # or 'cuda' if you have GPU
EMBEDDING_BATCH_SIZE=32

# =============================================================================
# RESEARCH-SPECIFIC SETTINGS
# =============================================================================
# LocalBuffer Configuration (from research notebook)
DEFAULT_MAX_TURNS=50
DEFAULT_EXCLUDE_RECENT=10

# Context Assembly Settings (research parameters)
MAX_CONTEXT_TOKENS=4000
MAX_RETRIEVED_DOCS=5
CONTEXT_OVERLAP_THRESHOLD=0.8

# Forest Management (multi-tree research settings)
MAX_TREES_PER_SESSION=10
MAX_NODES_PER_TREE=100

# Research Metrics Collection
ENABLE_RESEARCH_METRICS=true
METRICS_COLLECTION_INTERVAL=60  # seconds

# =============================================================================
# FRONTEND INTEGRATION
# =============================================================================
# CORS Settings (for frontend connection)
ALLOWED_ORIGINS=["http://localhost:3000", "http://localhost:3001", "http://127.0.0.1:3000"]
ALLOW_CREDENTIALS=true

# Frontend URL (for redirects and webhooks)
FRONTEND_URL=http://localhost:3000

# =============================================================================
# REDIS CONFIGURATION (Session Management & Caching)
# =============================================================================
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=
REDIS_SESSION_TTL=86400  # 24 hours in seconds

# =============================================================================
# LOGGING & MONITORING
# =============================================================================
# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT={time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}
LOG_FILE=./logs/hierarchical_chat.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Research Analytics
ENABLE_CONVERSATION_ANALYTICS=true
ANALYTICS_EXPORT_PATH=./data/analytics

# =============================================================================
# DOCKER & DEPLOYMENT SETTINGS
# =============================================================================
# Docker Configuration
DOCKER_IMAGE_NAME=hierarchical-chat-research
DOCKER_CONTAINER_NAME=hierarchical-chat-backend

# Health Check Settings
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10
HEALTH_CHECK_RETRIES=3

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================
# Development Features
ENABLE_DEBUG_ROUTES=true
ENABLE_ADMIN_PANEL=true
AUTO_RELOAD=true

# API Documentation
ENABLE_DOCS=true
DOCS_URL=/docs
REDOC_URL=/redoc

# =============================================================================
# RESEARCH DATA EXPORT
# =============================================================================
# Data Export Settings (for research analysis)
EXPORT_FORMAT=json  # json, csv, parquet
EXPORT_DIRECTORY=./data/exports
AUTO_EXPORT_INTERVAL=3600  # hourly exports

# Research Backup
BACKUP_ENABLED=true
BACKUP_DIRECTORY=./data/backups
BACKUP_RETENTION_DAYS=30

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
# Database Connection Pool
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20
DATABASE_POOL_TIMEOUT=30

# HTTP Client Settings
HTTP_TIMEOUT=30
HTTP_MAX_CONNECTIONS=100
HTTP_MAX_KEEPALIVE_CONNECTIONS=20

# Vector Search Performance
VECTOR_SEARCH_TIMEOUT=10
VECTOR_INDEX_BATCH_SIZE=100

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60  # seconds

# Request Size Limits
MAX_REQUEST_SIZE=10485760  # 10MB
MAX_UPLOAD_SIZE=52428800   # 50MB

# HTTPS Settings (production)
FORCE_HTTPS=false
HTTPS_REDIRECT=false

# =============================================================================
# RESEARCH COLLABORATION SETTINGS
# =============================================================================
# Multi-user Research Support
ENABLE_USER_SESSIONS=true
SESSION_ISOLATION=true
SHARED_VECTOR_STORE=false

# Research Data Sharing
ENABLE_DATA_EXPORT_API=true
ENABLE_ANALYTICS_API=true
REQUIRE_AUTH_FOR_EXPORT=true

# =============================================================================
# EXAMPLE VALUES - REPLACE WITH YOUR ACTUAL CONFIGURATION
# =============================================================================
# The values above are examples for research setup.
# For actual deployment:
# 1. Copy this file to .env
# 2. Replace all placeholder values with your actual configuration
# 3. Never commit .env files to version control
# 4. Use environment-specific values for different deployments
# ============================================================================
