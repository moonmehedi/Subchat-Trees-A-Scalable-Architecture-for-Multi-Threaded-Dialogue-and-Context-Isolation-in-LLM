{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68c33d8",
   "metadata": {},
   "source": [
    "## gpt-reference : https://chatgpt.com/c/6893962c-a600-8327-b567-d868e9b2e1ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import  ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8736ea4",
   "metadata": {},
   "source": [
    "Define the AgentState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9cd75",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  typing import Any, Optional,List,Dict\n",
    "import uuid\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "class LocalBuffer:\n",
    "    \"\"\"\n",
    "\n",
    "    LocalBuffer manages a fixed-size queue of recent chat messages.\n",
    "    \n",
    "    1. Stores recent chat messages with a fixed maximum capacity.\n",
    "    2. Adds new messages along with sender role and timestamp.\n",
    "    3. Retrieves the latest or all stored messages for context building.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,max_turns:int = 10):\n",
    "        self.turns: deque[Dict[str,any]] = deque(maxlen=max_turns)\n",
    "\n",
    "    def add_messages(self,role:str,text:str):\n",
    "        \"\"\" Add a new message with role and current timestamp. \"\"\"\n",
    "        self.turns.append({\n",
    "            \"role\":role,\n",
    "            'text':text,\n",
    "            'timestamp':time.time()\n",
    "        }\n",
    "        )\n",
    "\n",
    "    def get_recent(self,n:Optional[int]=None) -> List[Dict[str,Any]]:\n",
    "        \"\"\" Get the most recent n messages, or all if n is None\"\"\"\n",
    "        return list(self.turns)[-n:] if n else list(self.turns)\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"summarize the message for better context\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # might use for testing\n",
    "    def clear(self,n:int):\n",
    "        \"\"\" clear the last n turn from the buffer\"\"\"\n",
    "\n",
    "        for _ in range(n):\n",
    "            self.turns.pop()\n",
    "\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, node_id: str, title: str, parent: Optional['TreeNode']=None):\n",
    "        \n",
    "        self.node_id:str = node_id\n",
    "        self.title = title\n",
    "        self.parent = parent\n",
    "        self.children : List['TreeNode'] = []\n",
    "        self.buffer = LocalBuffer()\n",
    "        self.summary :Optional[str] = None #including a summary is important\n",
    "        self.metadata: Dict = {} # optional i may or may not use it\n",
    "\n",
    "\n",
    "\n",
    "    def create_node(self,Parent:'TreeNode'):\n",
    "        \"\"\"\n",
    "        Parent Node is passed as parameter \n",
    "\n",
    "\n",
    "        this function creates the tree node when called \n",
    "        it will generate an unique Id\n",
    "        based on follow up its generate a title \n",
    "        and Links to the create node to its parents through self.children []\n",
    "        set parent_id for the created note\n",
    "\n",
    "        path: concatinate with the parent.path+title(will be generated by the ai )\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def get_node(self,):\n",
    "        pass\n",
    "\n",
    "\n",
    "# class Trees:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         it would create a specific tree node\n",
    "#         do one thing connect a database and insert thig object to the databases\n",
    "\n",
    "#         \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Forest:\n",
    "    def __init__():\n",
    "        '''\n",
    "        maps the root chats in the forest\n",
    "\n",
    "        self.trees_map:Dict[str,TreeNode] = {}\n",
    "        self.active_tree = Optional[str] = None\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def create_tree(self):\n",
    "        \"\"\"\n",
    "        this creates a root node of TreeNode which can be nested while chatting\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def switch_tree():\n",
    "        '''\n",
    "        i will implement this later only required in case of mutiple main conversation getting the previous conversation\n",
    "        when i can shifting from one tree to another  tree\n",
    "\n",
    "        switch tree will change active tree\n",
    "        \n",
    "        '''\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_active_tree(self) -> TreeNode:\n",
    "        \"\"\"\n",
    "        simplely return the active node\n",
    "        \"\"\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed0004",
   "metadata": {},
   "source": [
    "## VECTOR INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from typing import List ,Dict, Optional, Any\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "class GlobalVectorIndex:\n",
    "    def __init__(self,persist_dir:Optional[str]=None,model_name:str=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            persist_dir: directory to persist Chroma DB (or None for in-memory).\n",
    "            model_name: huggingface model to use for embeddings.\n",
    "        \"\"\"\n",
    "        self.emb = HuggingFaceBgeEmbeddings(model_name = model_name)\n",
    "\n",
    "        # create or connect to chroma\n",
    "\n",
    "        self.store = Chroma(\n",
    "            collection_name = \"hierarchical_chat\",\n",
    "            embedding_function = self.emb,\n",
    "            persist_directory = persist_dir,\n",
    "        )\n",
    "\n",
    "    def index_docs(self,docs:List[Dict[str,Any]]):\n",
    "        \"\"\"\n",
    "        Index a list of docs into the vector store.\n",
    "        Each doc should be a dict: {\"text\": str, \"metadata\": dict}\n",
    "        \"\"\"\n",
    "\n",
    "        if not docs:\n",
    "            return \n",
    "        \n",
    "        texts = [ d['text'] for d in docs ]\n",
    "        metadatas = [d.get('metadata',{}) for d in docs]\n",
    "\n",
    "        self.store.add_texts(texts,metadatas=metadatas)\n",
    "\n",
    "    def retriever(self, query_text:str, top_k:int = 5, filters:Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Create a retriever that skips unnecessary context using metadata filtering\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ## using this meta data i would try to avoid searching over the current context buffer or memory because LLM already have them\n",
    "\n",
    "        ## what if i retrieve the information and i summarize them then what will happen\n",
    "\n",
    "        ## can we generate a summary from here like fetch 10 doc gave to llm to summarize it as a story main main things according to the questions\n",
    "        return self.store.as_retriever(\n",
    "            search_type = 'mmr',\n",
    "            search_kwargs = {\n",
    "                'k':k,\n",
    "                'fetch_k':fetch_k,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        \"\"\"Utility to delete/cleanup the collection (useful during development).\"\"\"\n",
    "        try:\n",
    "            self.store._collection.delete()\n",
    "        except Exception:\n",
    "            # Some Chroma versions differ; ignore safe failures\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161cfaf",
   "metadata": {},
   "source": [
    "## ASSEMBLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing everything for development \n",
    "\n",
    "# from typing import List, Optional, Dict\n",
    "# from core import TreeNode\n",
    "# from vector_index import GlobalVectorIndex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ed45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "okay so far update is good i have thought to implement the rest tomorrow i have some updates which i will try to incorporate tomorrow later and i have to talk with teachers and always try to learn from my environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
