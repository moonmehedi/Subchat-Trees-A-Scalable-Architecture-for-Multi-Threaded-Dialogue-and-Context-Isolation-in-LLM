{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hierarchical Subchat System - Kaggle GPU Testing\n",
    "\n",
    "## üìã Setup Checklist (Do Once):\n",
    "\n",
    "### 1Ô∏è‚É£ **Add Kaggle Secrets** (Most Important!)\n",
    "Go to: **https://www.kaggle.com/settings** ‚Üí Add-ons ‚Üí Secrets\n",
    "\n",
    "Add these two secrets:\n",
    "- **`GROQ_API_KEY`** = Your Groq API key (for query decomposition)\n",
    "- **`GITHUB_TOKEN`** = Your GitHub personal access token (for pushing results)\n",
    "\n",
    "### 2Ô∏è‚É£ **Enable Internet in This Notebook**\n",
    "- Click \"‚öôÔ∏è Settings\" (top right)\n",
    "- Turn ON **\"Internet\"** toggle\n",
    "- Click \"Save\"\n",
    "\n",
    "### 3Ô∏è‚É£ **Make Sure This Notebook is PRIVATE**\n",
    "- Never share secrets in public notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Run Order:\n",
    "1. **Cells 2-7**: Load secrets and libraries\n",
    "2. **Cell 8**: Set environment variables (LLM_BACKEND=vllm)\n",
    "3. **Cell 9**: Load vLLM model on Kaggle GPUs (Qwen-3 14B AWQ) - **Takes 2-3 minutes**\n",
    "4. **Cells 15-17**: Clone repo, configure git, run test log push\n",
    "5. **Cell 21**: Integrate vLLM with backend (registers model globally)\n",
    "6. **Cell 23**: Test vLLM integration\n",
    "7. **Run your actual tests**: Execute test scripts in backend/dataset/\n",
    "8. **Push results**: Use git_commit_and_push() function to sync to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Loads vLLM model (Qwen-3 14B) on Kaggle's 2x GPUs with AWQ quantization\n",
    "- ‚úÖ Integrates vLLM with your backend via `VLLMClient` singleton\n",
    "- ‚úÖ Runs hierarchical subchat tests using GPU-accelerated inference\n",
    "- ‚úÖ Generates performance logs for buffer size analysis\n",
    "- ‚úÖ Automatically syncs results back to your GitHub repo\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Technical Details:\n",
    "- **vLLM Config**: Tensor parallelism across both GPUs, 91% memory utilization\n",
    "- **Backend Mode**: `LLM_BACKEND=vllm` (set in cell 8)\n",
    "- **Model**: Qwen-3 14B AWQ (5120 max tokens, prefix caching enabled)\n",
    "- **No .env needed**: Secrets loaded from Kaggle environment\n",
    "- **Git Workflow**: Clone ‚Üí Test ‚Üí Push results to `kaggle-run` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00013-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/config.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00009-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00007-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/merges.txt\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00004-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/README.md\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00005-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/tokenizer.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/vocab.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00014-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00006-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00002-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00010-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00015-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00011-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00016-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00008-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/.gitattributes\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00003-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00012-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00001-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/generation_config.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/model.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/generation_config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00003-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00001-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00002-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/generation_config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00005-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00001-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00002-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00004-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00003-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING OUT MY GPU WORKINGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** repo: https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:41.966872Z",
     "iopub.status.busy": "2025-12-13T13:14:41.966627Z",
     "iopub.status.idle": "2025-12-13T13:14:44.519520Z",
     "shell.execute_reply": "2025-12-13T13:14:44.518617Z",
     "shell.execute_reply.started": "2025-12-13T13:14:41.966849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:44.521819Z",
     "iopub.status.busy": "2025-12-13T13:14:44.521553Z",
     "iopub.status.idle": "2025-12-13T13:14:48.187393Z",
     "shell.execute_reply": "2025-12-13T13:14:48.186532Z",
     "shell.execute_reply.started": "2025-12-13T13:14:44.521795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:46:30 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "‚úÖ VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# ‚úÖ NEW: Set vLLM model path for backend config\n",
    "# This will be used by config.py when backend starts\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.187944Z",
     "iopub.status.idle": "2025-12-13T13:14:48.188268Z",
     "shell.execute_reply": "2025-12-13T13:14:48.188112Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.188098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:47:00 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 12-19 12:47:01 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 12-19 12:47:01 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-19 12:47:01 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 12:47:01 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 12:47:01 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 12:47:01 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 12:47:01 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 12-19 12:47:01 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 12-19 12:47:02 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-19 12:47:02 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-19 12:47:02 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-19 12:47:02 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "WARNING 12-19 12:47:02 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-19 12:47:02 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-19 12:47:02 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-19 12:47:02 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-19 12:47:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 12-19 12:47:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:18 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:18 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:19 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:19 [cuda.py:289] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:19 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:19 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 12:47:20.259148987 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 12:47:20.259914288 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:47:20 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:20 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-19 12:47:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-19 12:47:20 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 12:47:20.521661117 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 12:47:20.522451375 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:47:20 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 12:47:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 12:47:45 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c01d529b'), local_subscribe_addr='ipc:///tmp/333ec350-5345-4bb1-a32c-73a77464dd6d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-19 12:47:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 12:47:45 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c01d529b'), local_subscribe_addr='ipc:///tmp/333ec350-5345-4bb1-a32c-73a77464dd6d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-19 12:47:45 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-19 12:47:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "INFO 12-19 12:47:45 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-19 12:47:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:47:45 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21760d7cb6d436ba19e04dc3e9bd7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:48:50 [loader.py:458] Loading weights took 64.12 seconds\n",
      "INFO 12-19 12:48:50 [loader.py:458] Loading weights took 64.23 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:48:50 [model_runner.py:1140] Model loading took 4.6720 GiB and 64.412091 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:48:50 [model_runner.py:1140] Model loading took 4.6720 GiB and 64.412091 seconds\n",
      "INFO 12-19 12:48:50 [model_runner.py:1140] Model loading took 4.6720 GiB and 64.524202 seconds\n",
      "INFO 12-19 12:48:50 [model_runner.py:1140] Model loading took 4.6720 GiB and 64.524202 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] Memory profiling takes 10.09 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-19 12:49:01 [worker.py:287] Memory profiling takes 10.26 seconds\n",
      "INFO 12-19 12:49:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-19 12:49:01 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] Memory profiling takes 10.09 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:49:01 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-19 12:49:01 [worker.py:287] Memory profiling takes 10.26 seconds\n",
      "INFO 12-19 12:49:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-19 12:49:01 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "INFO 12-19 12:49:01 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-19 12:49:01 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-19 12:49:01 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-19 12:49:01 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-19 12:49:06 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.43 seconds\n",
      "INFO 12-19 12:49:06 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.43 seconds\n"
     ]
    }
   ],
   "source": [
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# Use 14B model (32B causes CUDA linker failures on Kaggle T4 GPUs)\n",
    "#model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.189707Z",
     "iopub.status.idle": "2025-12-13T13:14:48.190032Z",
     "shell.execute_reply": "2025-12-13T13:14:48.189881Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.189866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d145164b97d468b9982a44b901cc29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tensor parallelism is a technique used in deep learning and machine learning to speed up the training of large neural networks. Imagine you have a big puzzle, and you want to solve it faster. Instead of one person working on the whole puzzle, you divide the puzzle into smaller pieces and give each piece to a different person. Each person works on their piece independently, and then you put all the pieces together to get the final result.\n",
      "\n",
      "In the context of neural networks, tensors are like the puzzle pieces. A tensor is a multi-dimensional array that holds data, such as the weights of a neural network. Tensor parallelism involves splitting these tensors into smaller parts and distributing them across multiple processors or machines. Each processor works on its part of the tensor, performing calculations in parallel with the others. Once all the processors have finished their calculations, the results are combined to produce the final output.\n",
      "\n",
      "This approach allows you to train larger and more complex models faster by leveraging the power of multiple processors working together, rather than relying on a single processor to handle the entire workload. It's like having a team of people working together to solve a puzzle instead of just one person. This can significantly reduce the time it takes to train a model and make better use of available computing resources."
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def stream_generate(llm, prompt):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    for output in llm.generate(\n",
    "        [prompt],\n",
    "        sampling_params,\n",
    "        \n",
    "    ):\n",
    "        yield output.outputs[0].text\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "User: Explain tensor parallelism in simple terms.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.191299Z",
     "iopub.status.idle": "2025-12-13T13:14:48.191605Z",
     "shell.execute_reply": "2025-12-13T13:14:48.191474Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.191458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233b25c524cf4e0ab0c7598a8298427b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In contrast to classical computers, which use bits to represent information, quantum computers use quantum bits, or qubits. Qubits can exist in multiple states simultaneously, allowing quantum computers to perform certain calculations much faster than classical computers. This makes quantum computing particularly well-suited for solving complex problems in fields such as cryptography, optimization, and simulation. However, quantum computing is still in its early stages of development and there are many technical challenges that need to be overcome before it can be widely used. \n",
      "\n",
      "In summary, quantum computing is a new type of computing that uses quantum mechanics to perform calculations, which can potentially solve certain problems much faster than classical computers. \n",
      "\n",
      "Would you like to know more about a specific aspect of quantum computing? If so, please let me know! \n",
      "\n",
      "Here are some additional resources that you may find helpful:\n",
      "\n",
      "- [Quantum Computing for Everyone](https://www.amazon.com/Quantum-Computing-Everyone-MIT-Press/dp/0262539543)\n",
      "- [IBM Quantum](https://quantum-computing.ibm.com/)\n",
      "- [Microsoft Quantum](https://www.microsoft.com/en-us/quantum)\n",
      "- [Google Quantum AI](https://ai.google/research/teams/brain/quantum) \n",
      "\n",
      "Let me know if you have any other questions! \n",
      "\n",
      "I hope this helps! Let me know if you have any other questions or if you would like more information on a specific topic related to quantum computing. \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "\n",
      "P.S. If you're interested in learning more about quantum computing, I recommend checking out some of the resources I listed above. They provide a good introduction to the topic and can help you get started with learning about quantum computing. Additionally, you can try running some quantum algorithms on simulators provided by IBM and Google to get a feel for how quantum computing works. \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "\n",
      "P.P.S. If you have any other questions or if you would like more information on a specific topic related to quantum computing, please don't hesitate to ask. I'm here to help! \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "\n",
      "P.P.P.S. If you're interested in learning more about the technical aspects of quantum computing, I recommend checking out some of the research papers and articles available online. Some good starting points are the papers published by"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"what is quantum computing?\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê LOADING SECRETS FROM KAGGLE\n",
      "============================================================\n",
      "‚úÖ GROQ_API_KEY loaded successfully\n",
      "   Key length: 56 characters\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from Kaggle's secure environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê LOADING SECRETS FROM KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load GROQ_API_KEY\n",
    "try:\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úÖ GROQ_API_KEY loaded successfully\")\n",
    "    print(f\"   Key length: {len(GROQ_API_KEY)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GROQ_API_KEY not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "# Try to load GITHUB_TOKEN\n",
    "try:\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
    "    print(\"‚úÖ GITHUB_TOKEN loaded successfully\")\n",
    "    print(f\"   Token length: {len(GITHUB_TOKEN)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GITHUB_TOKEN not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "\n",
    "# Set LLM backend to use vLLM (local model on Kaggle GPU)\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"  # We'll use the vLLM model loaded above\n",
    "print(\"\\n‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç ENVIRONMENT CHECK\n",
      "============================================================\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ CUDA version: 12.4\n",
      "‚úÖ Number of GPUs: 2\n",
      "\n",
      "üéÆ GPU 0: Tesla T4\n",
      "   Memory: 14.74 GB\n",
      "\n",
      "üéÆ GPU 1: Tesla T4\n",
      "   Memory: 14.74 GB\n",
      "\n",
      "‚úÖ Current working directory: /kaggle/working\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and configuration\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Current working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• CLONING REPOSITORY\n",
      "============================================================\n",
      "üîÑ Cloning kaggle-run branch (skipping LFS files)...\n",
      "   No authentication required for cloning (public repo)\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üì• Pulling Git LFS scenario files...\n",
      "‚úÖ Successfully pulled scenario files from Git LFS\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clone the kaggle-run branch from GitHub (PUBLIC READ - no auth needed)\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone the specific branch (no authentication needed for public repos)\n",
    "# Skip LFS files to avoid bandwidth quota issues\n",
    "print(f\"üîÑ Cloning {BRANCH} branch (skipping LFS files)...\")\n",
    "print(\"   No authentication required for cloning (public repo)\")\n",
    "\n",
    "# Set environment variable to skip LFS files\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned {BRANCH} branch!\")\n",
    "    print(f\"üìÇ Repository location: {os.path.abspath(REPO_DIR)}\")\n",
    "    \n",
    "    # Pull Git LFS files for scenarios only (saves bandwidth)\n",
    "    print(\"\\nüì• Pulling Git LFS scenario files...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    lfs_result = subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if lfs_result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully pulled scenario files from Git LFS\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Git LFS pull returned code {lfs_result.returncode}\")\n",
    "        if lfs_result.stderr:\n",
    "            print(f\"   {lfs_result.stderr}\")\n",
    "    \n",
    "    os.chdir(\"..\")  # Return to parent directory\n",
    "    \n",
    "    # List key directories to verify\n",
    "    print(\"\\nüìÅ Key directories found:\")\n",
    "    key_dirs = [\"backend\", \"backend/src\", \"backend/dataset\"]\n",
    "    for dir_path in key_dirs:\n",
    "        full_path = os.path.join(REPO_DIR, dir_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ‚úÖ {dir_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_path} (not found)\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!git config user.name \"moonmehedi\"\n",
    "!git config user.email \"the.mehedi.hasan.moon@gmail.com\"\n",
    "\n",
    "print(\"‚úÖ Git identity configured!\")\n",
    "print(f\"   User: moonmehedi\")\n",
    "print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "\n",
    "# Verify current branch\n",
    "branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING GIT PUSH CAPABILITY\n",
      "============================================================\n",
      "‚úÖ Created detailed test log: kaggle_logs/connection_test.log\n",
      "\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-19 12:49:52\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def create_test_log(log_dir=\"kaggle_logs\", log_file=\"connection_test.log\"):\n",
    "    \"\"\"Create a detailed test log with GPU and environment info\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üî¨ KAGGLE GPU TEST RUN - CONNECTION VERIFICATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"üìÖ Test Date: {current_time.strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"‚è∞ Test Time: {current_time.strftime('%H:%M:%S UTC')}\\n\")\n",
    "        f.write(f\"üìç Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üéÆ GPU CONFIGURATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"GPU Count: {torch.cuda.device_count()}\\n\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                f.write(f\"\\nGPU {i}:\\n\")\n",
    "                f.write(f\"  - Name: {torch.cuda.get_device_name(i)}\\n\")\n",
    "                f.write(f\"  - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\\n\")\n",
    "        else:\n",
    "            f.write(\"‚ö†Ô∏è  No GPU detected\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"üìä ENVIRONMENT INFO\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "        f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
    "        f.write(f\"Working Directory: {os.getcwd()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"‚úÖ TEST STATUS: SUCCESS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"\\nThis log was generated from Kaggle notebook\\n\")\n",
    "        f.write(f\"Push attempt at: {current_time.isoformat()}\\n\")\n",
    "    \n",
    "    return log_path, current_time\n",
    "\n",
    "\n",
    "def git_commit_and_push(file_path, commit_message, branch=\"kaggle-run\"):\n",
    "    \"\"\"Commit a file and push to GitHub\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Add file\n",
    "    add_result = subprocess.run([\"git\", \"add\", file_path], capture_output=True, text=True)\n",
    "    if add_result.returncode != 0:\n",
    "        return False, f\"Git add failed: {add_result.stderr}\"\n",
    "    \n",
    "    # Commit\n",
    "    commit_result = subprocess.run([\"git\", \"commit\", \"-m\", commit_message], capture_output=True, text=True)\n",
    "    if commit_result.returncode != 0:\n",
    "        return False, f\"Git commit failed: {commit_result.stderr}\"\n",
    "    \n",
    "    # Push with token\n",
    "    if \"GITHUB_TOKEN\" not in os.environ:\n",
    "        return False, \"GITHUB_TOKEN not found in environment\"\n",
    "    \n",
    "    repo_url_with_token = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "    \n",
    "    # Set remote URL\n",
    "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", repo_url_with_token], capture_output=True)\n",
    "    \n",
    "    # Push\n",
    "    push_result = subprocess.run([\"git\", \"push\", \"origin\", branch], capture_output=True, text=True)\n",
    "    \n",
    "    if push_result.returncode == 0:\n",
    "        return True, \"Push successful\"\n",
    "    else:\n",
    "        return False, f\"Push failed: {push_result.stderr}\"\n",
    "\n",
    "\n",
    "# Main execution\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING GIT PUSH CAPABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Change to repo directory\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Create test log\n",
    "    log_path, timestamp = create_test_log()\n",
    "    print(f\"‚úÖ Created detailed test log: {log_path}\")\n",
    "    \n",
    "    # Commit and push\n",
    "    commit_msg = f\"Test: Kaggle GPU verification - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    success, message = git_commit_and_push(log_path, commit_msg, BRANCH)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Successfully pushed to GitHub!\")\n",
    "        print(f\"   üìÅ Check: {log_path}\")\n",
    "        print(f\"   üìÖ Pushed at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"   üí° Pull on your local machine to verify sync\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Always return to parent directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Step 5: Integrate vLLM with Backend\n",
    "\n",
    "**This connects the loaded vLLM model to your backend code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîó INTEGRATING vLLM WITH BACKEND\n",
      "============================================================\n",
      "‚úÖ vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM model is now available to backend services\n",
      "   Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "   GPUs: 2\n",
      "   Backend will use LLM_BACKEND=vllm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Register the vLLM model with the backend\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó INTEGRATING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Register the globally loaded vLLM model\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"‚úÖ vLLM model is now available to backend services\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Backend will use LLM_BACKEND={os.getenv('LLM_BACKEND')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi>=0.104.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.116.1)\n",
      "Requirement already satisfied: uvicorn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.35.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.6 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 10)) (0.0.20)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (2.0.41)\n",
      "Collecting psycopg2-binary>=2.9.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 14))\n",
      "Collecting psycopg2-binary>=2.9.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 14))\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: alembic>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.17.1)\n",
      "Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.12.4)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 19)) (2.11.0)\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: alembic>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.17.1)\n",
      "Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.12.4)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 20)) (1.2.1)\n",
      "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (2.7.1)\n",
      "Collecting groq>=0.4.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 24))\n",
      "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 20)) (1.2.1)\n",
      "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (2.7.1)\n",
      "Collecting groq>=0.4.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 24))\n",
      "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting ollama>=0.1.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 25))\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (0.28.1)\n",
      "Collecting ollama>=0.1.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 25))\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (0.28.1)\n",
      "Collecting aiofiles>=23.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 29))\n",
      "  Downloading aiofiles-25.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2.2.3)\n",
      "Collecting aiofiles>=23.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 29))\n",
      "  Downloading aiofiles-25.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2.2.3)\n",
      "Collecting loguru>=0.7.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 36))\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (8.4.1)\n",
      "Collecting loguru>=0.7.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 36))\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (8.4.1)\n",
      "Collecting pytest-asyncio>=0.21.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 40))\n",
      "  Downloading pytest_asyncio-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting passlib>=1.7.0 (from passlib[bcrypt]>=1.7.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 43))\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pytest-asyncio>=0.21.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 40))\n",
      "  Downloading pytest_asyncio-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting passlib>=1.7.0 (from passlib[bcrypt]>=1.7.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 43))\n",
      "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting chromadb>=0.4.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting chromadb>=0.4.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (0.9.0)\n",
      "Requirement already satisfied: black>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (25.9.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (0.9.0)\n",
      "Requirement already satisfied: black>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (25.9.0)\n",
      "Collecting isort>=5.12.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 54))\n",
      "  Downloading isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isort>=5.12.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 54))\n",
      "  Downloading isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gunicorn>=21.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 57))\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gunicorn>=21.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 57))\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting PyPDF2>=3.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 60))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-docx>=1.1.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61))\n",
      "Collecting PyPDF2>=3.0.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 60))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-docx>=1.1.0 (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61))\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 65)) (6.0.3)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.47.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.41.5)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 65)) (6.0.3)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.47.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (3.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2.4.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (3.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.19.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.2.post1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.19.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.2.post1)\n",
      "Collecting pybase64>=1.4.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.21.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.21.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.74.0)\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.74.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.16.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.16.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (9.1.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (9.1.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.11.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.53.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2.32.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (4.5.0)\n",
      "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.3.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.11.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.53.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2.32.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (4.5.0)\n",
      "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx>=1.1.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (5.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (1.17.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.0.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx>=1.1.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (5.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (1.17.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.0.0)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.8)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.8)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.47b0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.5.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.5.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.5.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.6.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.6.0)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.17.2)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.17.2)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.4.2)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.23.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.1.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.23.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.1.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.6.1)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.6.1)\n",
      "Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/4.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_asyncio-1.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_asyncio-1.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.6/21.7 MB\u001b[0m \u001b[31m201.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading isort-7.0.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isort-7.0.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/103.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=4b26863ccdef5c4f0b92faa586e37aa9b1ff09ac150ecb1c6a258699e1720424\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=4b26863ccdef5c4f0b92faa586e37aa9b1ff09ac150ecb1c6a258699e1720424\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, passlib, durationpy, urllib3, python-docx, PyPDF2, pybase64, psycopg2-binary, mmh3, loguru, isort, humanfriendly, gunicorn, cachetools, bcrypt, backoff, aiofiles, pytest-asyncio, coloredlogs, posthog, ollama, groq, kubernetes, onnxruntime, chromadb\n",
      "Installing collected packages: pypika, passlib, durationpy, urllib3, python-docx, PyPDF2, pybase64, psycopg2-binary, mmh3, loguru, isort, humanfriendly, gunicorn, cachetools, bcrypt, backoff, aiofiles, pytest-asyncio, coloredlogs, posthog, ollama, groq, kubernetes, onnxruntime, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: cachetools\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 6.2.1\n",
      "    Uninstalling cachetools-6.2.1:\n",
      "      Successfully uninstalled cachetools-6.2.1\n",
      "  Attempting uninstall: aiofiles\n",
      "    Found existing installation: cachetools 6.2.1\n",
      "    Uninstalling cachetools-6.2.1:\n",
      "      Successfully uninstalled cachetools-6.2.1\n",
      "  Attempting uninstall: aiofiles\n",
      "    Found existing installation: aiofiles 22.1.0\n",
      "    Uninstalling aiofiles-22.1.0:\n",
      "      Successfully uninstalled aiofiles-22.1.0\n",
      "    Found existing installation: aiofiles 22.1.0\n",
      "    Uninstalling aiofiles-22.1.0:\n",
      "      Successfully uninstalled aiofiles-22.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyPDF2-3.0.1 aiofiles-25.1.0 backoff-2.2.1 bcrypt-5.0.0 cachetools-5.5.2 chromadb-1.3.7 coloredlogs-15.0.1 durationpy-0.10 groq-1.0.0 gunicorn-23.0.0 humanfriendly-10.0 isort-7.0.0 kubernetes-34.1.0 loguru-0.7.3 mmh3-5.2.0 ollama-0.6.1 onnxruntime-1.23.2 passlib-1.7.4 posthog-5.4.0 psycopg2-binary-2.9.11 pybase64-1.4.3 pypika-0.48.9 pytest-asyncio-1.3.0 python-docx-1.2.0 urllib3-2.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyPDF2-3.0.1 aiofiles-25.1.0 backoff-2.2.1 bcrypt-5.0.0 cachetools-5.5.2 chromadb-1.3.7 coloredlogs-15.0.1 durationpy-0.10 groq-1.0.0 gunicorn-23.0.0 humanfriendly-10.0 isort-7.0.0 kubernetes-34.1.0 loguru-0.7.3 mmh3-5.2.0 ollama-0.6.1 onnxruntime-1.23.2 passlib-1.7.4 posthog-5.4.0 psycopg2-binary-2.9.11 pybase64-1.4.3 pypika-0.48.9 pytest-asyncio-1.3.0 python-docx-1.2.0 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r /kaggle/working/Subchat-Trees/backend/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Step 6: Test vLLM Integration\n",
    "\n",
    "**Quick test to verify backend can use vLLM on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using vLLM backend with Kaggle GPU: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "üß™ TESTING BACKEND WITH vLLM\n",
      "============================================================\n",
      "‚úÖ vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "üìä Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "üìã Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Hello, test message\n",
      "\n",
      "üìù Testing response generation...\n",
      "*******************context*********************\n",
      " [{'role': 'user', 'content': 'Hello, test message'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca79533a07946bda4b0e27379f586a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Response: Hello! It looks like you might be testing or starting a conversation. How can I assist you today? If you have any specific questions or need help with something, feel free to ask!\n",
      "üìä Token usage: {'prompt_tokens': 23, 'completion_tokens': 42, 'total_tokens': 112}\n",
      "\n",
      "============================================================\n",
      "‚úÖ Backend integration successful!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the backend with vLLM\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING BACKEND WITH vLLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple test\n",
    "llm_client = SimpleLLMClient()\n",
    "\n",
    "# Create a test node (TreeNode creates its own buffer internally)\n",
    "# Note: TreeNode uses 'node_id' (not 'id'), and buffer_size (not buffer object)\n",
    "root = TreeNode(\n",
    "    node_id=\"test\", \n",
    "    title=\"Test Conversation\", \n",
    "    buffer_size=5,\n",
    "    llm_client=llm_client\n",
    ")\n",
    "\n",
    "# Add a message to the node's buffer\n",
    "root.buffer.add_message(\"user\", \"Hello, test message\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nüìù Testing response generation...\")\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "\n",
    "print(f\"\\n‚úÖ Response: {response}\")\n",
    "print(f\"üìä Token usage: {llm_client.get_last_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Backend integration successful!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Step 7: Start Backend Server\n",
    "\n",
    "**Run the FastAPI server with vLLM backend on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SETUP COMPLETE - READY FOR SUBCHAT TREES EXECUTION\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ SETUP COMPLETE - READY FOR SUBCHAT TREES EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "üöÄ STARTING BACKEND SERVER IN NOTEBOOK KERNEL\n",
      "============================================================\n",
      "üìÇ Backend path: /kaggle/working/Subchat-Trees/backend\n",
      "üîß Backend mode: vllm\n",
      "‚úÖ vLLM model registered: True\n",
      "üéØ Server URL: http://0.0.0.0:8000\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Server starting... (will block this cell)\n",
      "üí° Stop with: Kernel ‚Üí Interrupt\n",
      "\n",
      "‚úÖ vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce88c9efb71b4bb49a8957974897180b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097b4a3118bd467db0a749726d6eccfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c86b8ab14b64ec788a3673301f6d230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363695f63c5d41d3a4f32fc2be0a257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f3ee75dc344c5dacdac35bc277b3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4db4c27d944c8d85ecbdc76fabda75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f18a3d080b74056991683321a9d13d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b994a474b4e47e9bf7cc64e340887c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e88528437e4d7c95d50f4b2bb6d294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeadf8168e224162b7496b90bf114e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512c861a002d44d19de6aee06860f7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [47]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created fresh vector collection with all-mpnet-base-v2 embeddings (0 messages)\n",
      "‚úÖ Initialized multi-query decomposition + context windows\n",
      "‚úÖ Vector index enabled for RAG\n",
      "‚úÖ All logs cleared on server startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=160)\u001b[0;0m INFO 12-19 12:59:21 [multiproc_worker_utils.py:259] Worker exiting\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Event loop stopped before Future completed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/3857644282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Start the server programmatically (same process, can access llm variable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# nest_asyncio allows this to work in Jupyter's existing event loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src.main:app\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mMultiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# pragma: full coverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Event loop stopped before Future completed."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:59:22 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: vLLM model must be registered in the SAME process as the server\n",
    "# We'll use nest_asyncio to allow uvicorn to run in Jupyter's existing event loop\n",
    "\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ensure backend is in path\n",
    "import sys\n",
    "sys.path.insert(0, f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Re-register vLLM model (in case it was lost)\n",
    "from src.services.vllm_client import VLLMClient\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING BACKEND SERVER IN NOTEBOOK KERNEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Backend path: /kaggle/working/{REPO_DIR}/backend\")\n",
    "print(f\"üîß Backend mode: {os.getenv('LLM_BACKEND')}\")\n",
    "print(f\"‚úÖ vLLM model registered: {VLLMClient.is_available()}\")\n",
    "print(f\"üéØ Server URL: http://0.0.0.0:8000\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  Server starting... (will block this cell)\")\n",
    "print(\"üí° Stop with: Kernel ‚Üí Interrupt\\n\")\n",
    "\n",
    "# Change to backend directory\n",
    "os.chdir(f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Start the server programmatically (same process, can access llm variable)\n",
    "# nest_asyncio allows this to work in Jupyter's existing event loop\n",
    "uvicorn.run(\"src.main:app\", host=\"0.0.0.0\", port=8000, reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310545,
     "sourceId": 375832,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
