{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hierarchical Subchat System - Kaggle GPU Testing\n",
    "\n",
    "## üìã Setup Checklist (Do Once):\n",
    "\n",
    "### 1Ô∏è‚É£ **Add Kaggle Secrets** (Most Important!)\n",
    "Go to: **https://www.kaggle.com/settings** ‚Üí Add-ons ‚Üí Secrets\n",
    "\n",
    "Add these two secrets:\n",
    "- **`GROQ_API_KEY`** = Your Groq API key (for query decomposition)\n",
    "- **`GITHUB_TOKEN`** = Your GitHub personal access token (for pushing results)\n",
    "\n",
    "### 2Ô∏è‚É£ **Enable Internet in This Notebook**\n",
    "- Click \"‚öôÔ∏è Settings\" (top right)\n",
    "- Turn ON **\"Internet\"** toggle\n",
    "- Click \"Save\"\n",
    "\n",
    "### 3Ô∏è‚É£ **Make Sure This Notebook is PRIVATE**\n",
    "- Never share secrets in public notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Run Order:\n",
    "1. **Cells 2-7**: Load secrets and libraries\n",
    "2. **Cell 8**: Set environment variables (LLM_BACKEND=vllm)\n",
    "3. **Cell 9**: Load vLLM model on Kaggle GPUs (Qwen-3 14B AWQ) - **Takes 2-3 minutes**\n",
    "4. **Cells 15-17**: Clone repo, configure git, run test log push\n",
    "5. **Cell 21**: Integrate vLLM with backend (registers model globally)\n",
    "6. **Cell 23**: Test vLLM integration\n",
    "7. **Run your actual tests**: Execute test scripts in backend/dataset/\n",
    "8. **Push results**: Use git_commit_and_push() function to sync to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Loads vLLM model (Qwen-3 14B) on Kaggle's 2x GPUs with AWQ quantization\n",
    "- ‚úÖ Integrates vLLM with your backend via `VLLMClient` singleton\n",
    "- ‚úÖ Runs hierarchical subchat tests using GPU-accelerated inference\n",
    "- ‚úÖ Generates performance logs for buffer size analysis\n",
    "- ‚úÖ Automatically syncs results back to your GitHub repo\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Technical Details:\n",
    "- **vLLM Config**: Tensor parallelism across both GPUs, 91% memory utilization\n",
    "- **Backend Mode**: `LLM_BACKEND=vllm` (set in cell 8)\n",
    "- **Model**: Qwen-3 14B AWQ (5120 max tokens, prefix caching enabled)\n",
    "- **No .env needed**: Secrets loaded from Kaggle environment\n",
    "- **Git Workflow**: Clone ‚Üí Test ‚Üí Push results to `kaggle-run` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/model.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/1.5b-instruct-awq/1/generation_config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00003-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00001-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/model-00002-of-00003.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1/generation_config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00005-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00001-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00002-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00004-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/model-00003-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1/generation_config.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00013-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/config.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00009-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00007-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/merges.txt\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00004-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/README.md\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00005-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/tokenizer.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/vocab.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00014-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00006-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00002-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00010-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00015-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00011-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00016-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00008-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/.gitattributes\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00003-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00012-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/model-00001-of-00016.safetensors\n",
      "/kaggle/input/qwen-3/transformers/30b-a3b-instruct-2507/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING OUT MY GPU WORKINGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** repo: https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:41.966872Z",
     "iopub.status.busy": "2025-12-13T13:14:41.966627Z",
     "iopub.status.idle": "2025-12-13T13:14:44.519520Z",
     "shell.execute_reply": "2025-12-13T13:14:44.518617Z",
     "shell.execute_reply.started": "2025-12-13T13:14:41.966849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:44.521819Z",
     "iopub.status.busy": "2025-12-13T13:14:44.521553Z",
     "iopub.status.idle": "2025-12-13T13:14:48.187393Z",
     "shell.execute_reply": "2025-12-13T13:14:48.186532Z",
     "shell.execute_reply.started": "2025-12-13T13:14:44.521795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 17:51:31 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "‚úÖ VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# ‚úÖ NEW: Set vLLM model path for backend config\n",
    "# This will be used by config.py when backend starts\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.187944Z",
     "iopub.status.idle": "2025-12-13T13:14:48.188268Z",
     "shell.execute_reply": "2025-12-13T13:14:48.188112Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.188098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 17:51:59 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 12-18 17:52:00 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 12-18 17:52:00 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-18 17:52:00 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-18 17:52:00 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-18 17:52:00 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-18 17:52:00 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-18 17:52:00 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 12-18 17:52:00 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 12-18 17:52:00 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-18 17:52:00 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-18 17:52:00 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "WARNING 12-18 17:52:00 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-18 17:52:01 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-18 17:52:01 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-18 17:52:01 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-18 17:52:01 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-18 17:52:06 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 12-18 17:52:06 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:17 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:17 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:18 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:18 [cuda.py:289] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:18 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:18 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1218 17:52:18.220254074 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1218 17:52:18.221409606 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 17:52:19 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-18 17:52:19 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-18 17:52:19 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1218 17:52:19.500196904 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1218 17:52:19.500909977 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 17:52:19 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-18 17:52:19 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_58e15f04'), local_subscribe_addr='ipc:///tmp/557016ae-2c63-419d-91aa-73024d3b5971', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-18 17:52:19 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-18 17:52:19 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_58e15f04'), local_subscribe_addr='ipc:///tmp/557016ae-2c63-419d-91aa-73024d3b5971', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-18 17:52:19 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-18 17:52:19 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "INFO 12-18 17:52:19 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:19 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4095eadb334471bae2524dd329c137d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:29 [loader.py:458] Loading weights took 9.49 seconds\n",
      "INFO 12-18 17:52:29 [loader.py:458] Loading weights took 9.53 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:29 [model_runner.py:1140] Model loading took 4.6720 GiB and 9.720133 seconds\n",
      "INFO 12-18 17:52:29 [model_runner.py:1140] Model loading took 4.6720 GiB and 9.764329 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:29 [model_runner.py:1140] Model loading took 4.6720 GiB and 9.720133 seconds\n",
      "INFO 12-18 17:52:29 [model_runner.py:1140] Model loading took 4.6720 GiB and 9.764329 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] Memory profiling takes 6.33 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-18 17:52:36 [worker.py:287] Memory profiling takes 6.44 seconds\n",
      "INFO 12-18 17:52:36 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-18 17:52:36 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] Memory profiling takes 6.33 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 17:52:36 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-18 17:52:36 [worker.py:287] Memory profiling takes 6.44 seconds\n",
      "INFO 12-18 17:52:36 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-18 17:52:36 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "INFO 12-18 17:52:36 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-18 17:52:36 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-18 17:52:36 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-18 17:52:36 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-18 17:52:40 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 11.00 seconds\n",
      "INFO 12-18 17:52:40 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 11.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# Use 14B model (32B causes CUDA linker failures on Kaggle T4 GPUs)\n",
    "#model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.189707Z",
     "iopub.status.idle": "2025-12-13T13:14:48.190032Z",
     "shell.execute_reply": "2025-12-13T13:14:48.189881Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.189866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7abf10fd57d44769f0bee21ac9ed3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tensor parallelism is a technique used in deep learning and machine learning to speed up the training of large neural networks. Imagine you have a big puzzle, and you want to solve it faster. Instead of one person working on the whole puzzle, you can split the puzzle into smaller pieces and have multiple people work on different parts simultaneously. \n",
      "\n",
      "In the context of neural networks, a tensor is like a multi-dimensional array that holds data. Tensor parallelism means splitting these tensors into smaller parts and distributing them across multiple processors or machines. Each processor works on its part of the tensor, and then the results are combined to get the final output. This way, the overall computation is faster because multiple processors are working in parallel, rather than one processor doing all the work sequentially. This is particularly useful when dealing with very large models and datasets that require significant computational resources. By distributing the workload, tensor parallelism helps to reduce the time needed for training and inference."
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def stream_generate(llm, prompt):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    for output in llm.generate(\n",
    "        [prompt],\n",
    "        sampling_params,\n",
    "        \n",
    "    ):\n",
    "        yield output.outputs[0].text\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "User: Explain tensor parallelism in simple terms.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.191299Z",
     "iopub.status.idle": "2025-12-13T13:14:48.191605Z",
     "shell.execute_reply": "2025-12-13T13:14:48.191474Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.191458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb358a939c44752b26b413f0910f955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In classical computing, data is processed using bits, which can represent either a 0 or a 1. In contrast, quantum computing uses quantum bits, or qubits, which can represent a 0, a 1, or both at the same time due to the principle of superposition. This allows quantum computers to perform certain types of calculations much faster than classical computers.\n",
      "\n",
      "Here are some key concepts in quantum computing:\n",
      "\n",
      "1. **Qubits**: The basic unit of quantum information, similar to bits in classical computing. Unlike classical bits, qubits can exist in a state of 0, 1, or any quantum superposition of these states.\n",
      "\n",
      "2. **Superposition**: The principle that allows a qubit to be in multiple states simultaneously. This means a quantum computer can process a vast amount of possibilities simultaneously.\n",
      "\n",
      "3. **Entanglement**: A quantum phenomenon where qubits become interconnected and the state of one (no matter the distance) can depend on the state of another. This allows for complex interactions between qubits.\n",
      "\n",
      "4. **Quantum Gates**: Analogous to logic gates in classical computing, quantum gates manipulate qubits to perform operations. These gates can create superpositions and entanglements.\n",
      "\n",
      "5. **Quantum Algorithms**: Special algorithms designed to take advantage of quantum properties to solve problems more efficiently than classical algorithms. Examples include Shor's algorithm for factoring large numbers and Grover's algorithm for searching unsorted databases.\n",
      "\n",
      "6. **Quantum Decoherence**: A major challenge in quantum computing where the quantum state of a system can be disrupted by its environment, leading to errors in computation.\n",
      "\n",
      "Quantum computing has the potential to revolutionize fields such as cryptography, optimization, and simulation of molecular structures. However, it is still in the early stages of development, and practical, large-scale quantum computers are not yet widely available. Research is ongoing to overcome the technical challenges and fully realize the potential of quantum computing."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"what is quantum computing?\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê LOADING SECRETS FROM KAGGLE\n",
      "============================================================\n",
      "‚úÖ GROQ_API_KEY loaded successfully\n",
      "   Key length: 56 characters\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from Kaggle's secure environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê LOADING SECRETS FROM KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load GROQ_API_KEY\n",
    "try:\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úÖ GROQ_API_KEY loaded successfully\")\n",
    "    print(f\"   Key length: {len(GROQ_API_KEY)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GROQ_API_KEY not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "# Try to load GITHUB_TOKEN\n",
    "try:\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
    "    print(\"‚úÖ GITHUB_TOKEN loaded successfully\")\n",
    "    print(f\"   Token length: {len(GITHUB_TOKEN)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GITHUB_TOKEN not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "\n",
    "# Set LLM backend to use vLLM (local model on Kaggle GPU)\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"  # We'll use the vLLM model loaded above\n",
    "print(\"\\n‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç ENVIRONMENT CHECK\n",
      "============================================================\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ CUDA version: 12.4\n",
      "‚úÖ Number of GPUs: 2\n",
      "\n",
      "üéÆ GPU 0: Tesla T4\n",
      "   Memory: 14.74 GB\n",
      "\n",
      "üéÆ GPU 1: Tesla T4\n",
      "   Memory: 14.74 GB\n",
      "\n",
      "‚úÖ Current working directory: /kaggle/working\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and configuration\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Current working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• CLONING REPOSITORY\n",
      "============================================================\n",
      "‚ö†Ô∏è  Removing existing Subchat-Trees directory...\n",
      "üîÑ Cloning kaggle-run branch (skipping LFS files)...\n",
      "   No authentication required for cloning (public repo)\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üì• Pulling Git LFS scenario files...\n",
      "‚úÖ Successfully pulled scenario files from Git LFS\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clone the kaggle-run branch from GitHub (PUBLIC READ - no auth needed)\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone the specific branch (no authentication needed for public repos)\n",
    "# Skip LFS files to avoid bandwidth quota issues\n",
    "print(f\"üîÑ Cloning {BRANCH} branch (skipping LFS files)...\")\n",
    "print(\"   No authentication required for cloning (public repo)\")\n",
    "\n",
    "# Set environment variable to skip LFS files\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned {BRANCH} branch!\")\n",
    "    print(f\"üìÇ Repository location: {os.path.abspath(REPO_DIR)}\")\n",
    "    \n",
    "    # Pull Git LFS files for scenarios only (saves bandwidth)\n",
    "    print(\"\\nüì• Pulling Git LFS scenario files...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    lfs_result = subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if lfs_result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully pulled scenario files from Git LFS\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Git LFS pull returned code {lfs_result.returncode}\")\n",
    "        if lfs_result.stderr:\n",
    "            print(f\"   {lfs_result.stderr}\")\n",
    "    \n",
    "    os.chdir(\"..\")  # Return to parent directory\n",
    "    \n",
    "    # List key directories to verify\n",
    "    print(\"\\nüìÅ Key directories found:\")\n",
    "    key_dirs = [\"backend\", \"backend/src\", \"backend/dataset\"]\n",
    "    for dir_path in key_dirs:\n",
    "        full_path = os.path.join(REPO_DIR, dir_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ‚úÖ {dir_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_path} (not found)\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!git config user.name \"moonmehedi\"\n",
    "!git config user.email \"the.mehedi.hasan.moon@gmail.com\"\n",
    "\n",
    "print(\"‚úÖ Git identity configured!\")\n",
    "print(f\"   User: moonmehedi\")\n",
    "print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "\n",
    "# Verify current branch\n",
    "branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING GIT PUSH CAPABILITY\n",
      "============================================================\n",
      "‚úÖ Created detailed test log: kaggle_logs/connection_test.log\n",
      "\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-18 17:53:20\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def create_test_log(log_dir=\"kaggle_logs\", log_file=\"connection_test.log\"):\n",
    "    \"\"\"Create a detailed test log with GPU and environment info\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üî¨ KAGGLE GPU TEST RUN - CONNECTION VERIFICATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"üìÖ Test Date: {current_time.strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"‚è∞ Test Time: {current_time.strftime('%H:%M:%S UTC')}\\n\")\n",
    "        f.write(f\"üìç Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üéÆ GPU CONFIGURATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"GPU Count: {torch.cuda.device_count()}\\n\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                f.write(f\"\\nGPU {i}:\\n\")\n",
    "                f.write(f\"  - Name: {torch.cuda.get_device_name(i)}\\n\")\n",
    "                f.write(f\"  - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\\n\")\n",
    "        else:\n",
    "            f.write(\"‚ö†Ô∏è  No GPU detected\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"üìä ENVIRONMENT INFO\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "        f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
    "        f.write(f\"Working Directory: {os.getcwd()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"‚úÖ TEST STATUS: SUCCESS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"\\nThis log was generated from Kaggle notebook\\n\")\n",
    "        f.write(f\"Push attempt at: {current_time.isoformat()}\\n\")\n",
    "    \n",
    "    return log_path, current_time\n",
    "\n",
    "\n",
    "def git_commit_and_push(file_path, commit_message, branch=\"kaggle-run\"):\n",
    "    \"\"\"Commit a file and push to GitHub\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Add file\n",
    "    add_result = subprocess.run([\"git\", \"add\", file_path], capture_output=True, text=True)\n",
    "    if add_result.returncode != 0:\n",
    "        return False, f\"Git add failed: {add_result.stderr}\"\n",
    "    \n",
    "    # Commit\n",
    "    commit_result = subprocess.run([\"git\", \"commit\", \"-m\", commit_message], capture_output=True, text=True)\n",
    "    if commit_result.returncode != 0:\n",
    "        return False, f\"Git commit failed: {commit_result.stderr}\"\n",
    "    \n",
    "    # Push with token\n",
    "    if \"GITHUB_TOKEN\" not in os.environ:\n",
    "        return False, \"GITHUB_TOKEN not found in environment\"\n",
    "    \n",
    "    repo_url_with_token = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "    \n",
    "    # Set remote URL\n",
    "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", repo_url_with_token], capture_output=True)\n",
    "    \n",
    "    # Push\n",
    "    push_result = subprocess.run([\"git\", \"push\", \"origin\", branch], capture_output=True, text=True)\n",
    "    \n",
    "    if push_result.returncode == 0:\n",
    "        return True, \"Push successful\"\n",
    "    else:\n",
    "        return False, f\"Push failed: {push_result.stderr}\"\n",
    "\n",
    "\n",
    "# Main execution\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING GIT PUSH CAPABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Change to repo directory\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Create test log\n",
    "    log_path, timestamp = create_test_log()\n",
    "    print(f\"‚úÖ Created detailed test log: {log_path}\")\n",
    "    \n",
    "    # Commit and push\n",
    "    commit_msg = f\"Test: Kaggle GPU verification - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    success, message = git_commit_and_push(log_path, commit_msg, BRANCH)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Successfully pushed to GitHub!\")\n",
    "        print(f\"   üìÅ Check: {log_path}\")\n",
    "        print(f\"   üìÖ Pushed at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"   üí° Pull on your local machine to verify sync\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Always return to parent directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Step 5: Integrate vLLM with Backend\n",
    "\n",
    "**This connects the loaded vLLM model to your backend code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîó INTEGRATING vLLM WITH BACKEND\n",
      "============================================================\n",
      "‚úÖ vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM model is now available to backend services\n",
      "   Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "   GPUs: 2\n",
      "   Backend will use LLM_BACKEND=vllm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Register the vLLM model with the backend\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó INTEGRATING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Register the globally loaded vLLM model\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"‚úÖ vLLM model is now available to backend services\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Backend will use LLM_BACKEND={os.getenv('LLM_BACKEND')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi>=0.104.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.116.1)\n",
      "Requirement already satisfied: uvicorn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.35.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.6 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 10)) (0.0.20)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (2.0.41)\n",
      "Requirement already satisfied: psycopg2-binary>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 14)) (2.9.11)\n",
      "Requirement already satisfied: alembic>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.17.1)\n",
      "Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.12.4)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 20)) (1.2.1)\n",
      "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (2.7.1)\n",
      "Requirement already satisfied: groq>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 24)) (1.0.0)\n",
      "Requirement already satisfied: ollama>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 25)) (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (0.28.1)\n",
      "Requirement already satisfied: aiofiles>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 29)) (25.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2.2.3)\n",
      "Requirement already satisfied: loguru>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 36)) (0.7.3)\n",
      "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 40)) (1.3.0)\n",
      "Requirement already satisfied: passlib>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from passlib[bcrypt]>=1.7.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 43)) (1.7.4)\n",
      "Requirement already satisfied: chromadb>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.7)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (0.9.0)\n",
      "Requirement already satisfied: black>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (25.9.0)\n",
      "Requirement already satisfied: isort>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 54)) (7.0.0)\n",
      "Requirement already satisfied: gunicorn>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 57)) (23.0.0)\n",
      "Requirement already satisfied: PyPDF2>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 60)) (3.0.1)\n",
      "Requirement already satisfied: python-docx>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 65)) (6.0.3)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.47.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2.2.3)\n",
      "Requirement already satisfied: loguru>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 36)) (0.7.3)\n",
      "Requirement already satisfied: pytest>=7.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 40)) (1.3.0)\n",
      "Requirement already satisfied: passlib>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from passlib[bcrypt]>=1.7.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 43)) (1.7.4)\n",
      "Requirement already satisfied: chromadb>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.7)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (0.9.0)\n",
      "Requirement already satisfied: black>=23.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (25.9.0)\n",
      "Requirement already satisfied: isort>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 54)) (7.0.0)\n",
      "Requirement already satisfied: gunicorn>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 57)) (23.0.0)\n",
      "Requirement already satisfied: PyPDF2>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 60)) (3.0.1)\n",
      "Requirement already satisfied: python-docx>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 65)) (6.0.3)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (0.47.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.104.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 8)) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.24.0->uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (3.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.19.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.4.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (1.3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 18)) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 23)) (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 28)) (3.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 33)) (2025.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 39)) (2.19.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.4.3)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.11.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.53.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.11.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (4.53.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2.32.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (4.5.0)\n",
      "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx>=1.1.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (5.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (1.17.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (2.32.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (1.1.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (4.5.0)\n",
      "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from black>=23.0.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 53)) (0.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx>=1.1.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 61)) (5.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 64)) (1.17.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.24.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.8)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.47b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.2.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.25.8)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.47b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.3.1.170)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.5.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 50)) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.5.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (0.5.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.4.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.12.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 15)) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.6.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 47)) (3.6.0)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.17.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.23.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.1.2)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (1.17.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.23.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 32)) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (3.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.6.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r /kaggle/working/Subchat-Trees/backend/requirements.txt (line 46)) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r /kaggle/working/Subchat-Trees/backend/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Step 6: Test vLLM Integration\n",
    "\n",
    "**Quick test to verify backend can use vLLM on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using vLLM backend with Kaggle GPU: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "üß™ TESTING BACKEND WITH vLLM\n",
      "============================================================\n",
      "‚úÖ vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "üìä Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "üìã Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Hello, test message\n",
      "\n",
      "üìù Testing response generation...\n",
      "*******************context*********************\n",
      " [{'role': 'user', 'content': 'Hello, test message'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfed998f4964f6f9d87d185c540056e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Response: Hello! It looks like you might be testing or starting a conversation. How can I assist you today? If you have any specific questions or need help with something, feel free to ask!\n",
      "üìä Token usage: {'prompt_tokens': 23, 'completion_tokens': 42, 'total_tokens': 112}\n",
      "\n",
      "============================================================\n",
      "‚úÖ Backend integration successful!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the backend with vLLM\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING BACKEND WITH vLLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple test\n",
    "llm_client = SimpleLLMClient()\n",
    "\n",
    "# Create a test node (TreeNode creates its own buffer internally)\n",
    "# Note: TreeNode uses 'node_id' (not 'id'), and buffer_size (not buffer object)\n",
    "root = TreeNode(\n",
    "    node_id=\"test\", \n",
    "    title=\"Test Conversation\", \n",
    "    buffer_size=5,\n",
    "    llm_client=llm_client\n",
    ")\n",
    "\n",
    "# Add a message to the node's buffer\n",
    "root.buffer.add_message(\"user\", \"Hello, test message\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nüìù Testing response generation...\")\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "\n",
    "print(f\"\\n‚úÖ Response: {response}\")\n",
    "print(f\"üìä Token usage: {llm_client.get_last_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Backend integration successful!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Step 7: Start Backend Server\n",
    "\n",
    "**Run the FastAPI server with vLLM backend on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SETUP COMPLETE - READY FOR SUBCHAT TREES EXECUTION\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ SETUP COMPLETE - READY FOR SUBCHAT TREES EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "üöÄ STARTING BACKEND SERVER IN NOTEBOOK KERNEL\n",
      "============================================================\n",
      "üìÇ Backend path: /kaggle/working/Subchat-Trees/backend\n",
      "üîß Backend mode: vllm\n",
      "‚úÖ vLLM model registered: True\n",
      "üéØ Server URL: http://0.0.0.0:8000\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Server starting... (will block this cell)\n",
      "üí° Stop with: Kernel ‚Üí Interrupt\n",
      "\n",
      "‚úÖ vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "INFO:     Started server process [905]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created fresh vector collection with all-mpnet-base-v2 embeddings (0 messages)\n",
      "‚úÖ Initialized multi-query decomposition + context windows\n",
      "‚úÖ Vector index enabled for RAG\n",
      "‚úÖ All logs cleared on server startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=951)\u001b[0;0m INFO 12-18 18:34:33 [multiproc_worker_utils.py:259] Worker exiting\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Event loop stopped before Future completed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_905/3857644282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Start the server programmatically (same process, can access llm variable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# nest_asyncio allows this to work in Jupyter's existing event loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src.main:app\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mMultiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# pragma: full coverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Event loop stopped before Future completed."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 18:34:34 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: vLLM model must be registered in the SAME process as the server\n",
    "# We'll use nest_asyncio to allow uvicorn to run in Jupyter's existing event loop\n",
    "\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ensure backend is in path\n",
    "import sys\n",
    "sys.path.insert(0, f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Re-register vLLM model (in case it was lost)\n",
    "from src.services.vllm_client import VLLMClient\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING BACKEND SERVER IN NOTEBOOK KERNEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Backend path: /kaggle/working/{REPO_DIR}/backend\")\n",
    "print(f\"üîß Backend mode: {os.getenv('LLM_BACKEND')}\")\n",
    "print(f\"‚úÖ vLLM model registered: {VLLMClient.is_available()}\")\n",
    "print(f\"üéØ Server URL: http://0.0.0.0:8000\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  Server starting... (will block this cell)\")\n",
    "print(\"üí° Stop with: Kernel ‚Üí Interrupt\\n\")\n",
    "\n",
    "# Change to backend directory\n",
    "os.chdir(f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Start the server programmatically (same process, can access llm variable)\n",
    "# nest_asyncio allows this to work in Jupyter's existing event loop\n",
    "uvicorn.run(\"src.main:app\", host=\"0.0.0.0\", port=8000, reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310545,
     "sourceId": 375832,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
