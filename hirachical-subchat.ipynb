{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hierarchical Subchat System - Kaggle GPU Testing\n",
    "\n",
    "## üìã Setup Checklist (Do Once):\n",
    "\n",
    "### 1Ô∏è‚É£ **Add Kaggle Secrets** (Most Important!)\n",
    "Go to: **https://www.kaggle.com/settings** ‚Üí Add-ons ‚Üí Secrets\n",
    "\n",
    "Add these two secrets:\n",
    "- **`GROQ_API_KEY`** = Your Groq API key (for query decomposition)\n",
    "- **`GITHUB_TOKEN`** = Your GitHub personal access token (for pushing results)\n",
    "\n",
    "### 2Ô∏è‚É£ **Enable Internet in This Notebook**\n",
    "- Click \"‚öôÔ∏è Settings\" (top right)\n",
    "- Turn ON **\"Internet\"** toggle\n",
    "- Click \"Save\"\n",
    "\n",
    "### 3Ô∏è‚É£ **Make Sure This Notebook is PRIVATE**\n",
    "- Never share secrets in public notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Run Order:\n",
    "1. Run all cells in order (the notebook will automatically handle everything)\n",
    "2. The vLLM model (Qwen-3 14B) will load on Kaggle's GPUs\n",
    "3. Your repo will be cloned from GitHub\n",
    "4. Tests will run and results will be pushed back to GitHub\n",
    "5. Pull the results on your local machine to analyze\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Uses Kaggle's free GPU to run large language models\n",
    "- ‚úÖ Tests your hierarchical subchat architecture with real models\n",
    "- ‚úÖ Generates performance logs for buffer size analysis\n",
    "- ‚úÖ Automatically syncs results back to your GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING OUT MY GPU WORKINGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** repo: https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:41.966872Z",
     "iopub.status.busy": "2025-12-13T13:14:41.966627Z",
     "iopub.status.idle": "2025-12-13T13:14:44.519520Z",
     "shell.execute_reply": "2025-12-13T13:14:44.518617Z",
     "shell.execute_reply.started": "2025-12-13T13:14:41.966849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system  'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:44.521819Z",
     "iopub.status.busy": "2025-12-13T13:14:44.521553Z",
     "iopub.status.idle": "2025-12-13T13:14:48.187393Z",
     "shell.execute_reply": "2025-12-13T13:14:48.186532Z",
     "shell.execute_reply.started": "2025-12-13T13:14:44.521795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._subclasses.fake_tensor' has no attribute 'UnsupportedMutationAliasingException'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_110/1275964227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# For Qwen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlogits_processor_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultipleChoiceLogitsProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# modules to ensure that the environment variables are set before any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# other modules are imported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_override\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m MODULE_ATTRS = {\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/env_override.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TORCHINDUCTOR_COMPILE_THREADS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# see https://github.com/vllm-project/vllm/issues/10619\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# ===================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstandalone_compile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompiledArtifact\u001b[0m  \u001b[0;31m# noqa: TC001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/standalone_compile.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_path_separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudagraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoxedDeviceIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtemporary_cache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/cpp_builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_vec_isa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minvalid_vec_isa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecISA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShortenTraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0maot_compile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/aot_compile.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompile_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrecompileContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObservedException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperatorException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedFakeTensorException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMutationAliasingException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._subclasses.fake_tensor' has no attribute 'UnsupportedMutationAliasingException'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.187944Z",
     "iopub.status.idle": "2025-12-13T13:14:48.188268Z",
     "shell.execute_reply": "2025-12-13T13:14:48.188112Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.188098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "#model_path = \"/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1\"\n",
    "model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.189707Z",
     "iopub.status.idle": "2025-12-13T13:14:48.190032Z",
     "shell.execute_reply": "2025-12-13T13:14:48.189881Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.189866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def stream_generate(llm, prompt):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    for output in llm.generate(\n",
    "        [prompt],\n",
    "        sampling_params,\n",
    "        \n",
    "    ):\n",
    "        yield output.outputs[0].text\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "User: Explain tensor parallelism in simple terms.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.191299Z",
     "iopub.status.idle": "2025-12-13T13:14:48.191605Z",
     "shell.execute_reply": "2025-12-13T13:14:48.191474Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.191458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is quantum computing?\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê LOADING SECRETS FROM KAGGLE\n",
      "============================================================\n",
      "‚úÖ GROQ_API_KEY loaded successfully\n",
      "   Key length: 56 characters\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from Kaggle's secure environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê LOADING SECRETS FROM KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load GROQ_API_KEY\n",
    "try:\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úÖ GROQ_API_KEY loaded successfully\")\n",
    "    print(f\"   Key length: {len(GROQ_API_KEY)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GROQ_API_KEY not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "# Try to load GITHUB_TOKEN\n",
    "try:\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
    "    print(\"‚úÖ GITHUB_TOKEN loaded successfully\")\n",
    "    print(f\"   Token length: {len(GITHUB_TOKEN)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GITHUB_TOKEN not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "\n",
    "# Set LLM backend to use vLLM (local model on Kaggle GPU)\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"  # We'll use the vLLM model loaded above\n",
    "print(\"\\n‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç ENVIRONMENT CHECK\n",
      "============================================================\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: False\n",
      "‚úÖ CUDA version: 12.4\n",
      "‚úÖ Number of GPUs: 0\n",
      "\n",
      "‚úÖ Current working directory: /kaggle/working\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and configuration\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Current working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• CLONING REPOSITORY\n",
      "============================================================\n",
      "‚ö†Ô∏è  Removing existing Subchat-Trees directory...\n",
      "üîÑ Cloning kaggle-run branch (skipping LFS files)...\n",
      "   No authentication required for cloning (public repo)\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clone the kaggle-run branch from GitHub (PUBLIC READ - no auth needed)\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone the specific branch (no authentication needed for public repos)\n",
    "# Skip LFS files to avoid bandwidth quota issues\n",
    "print(f\"üîÑ Cloning {BRANCH} branch (skipping LFS files)...\")\n",
    "print(\"   No authentication required for cloning (public repo)\")\n",
    "\n",
    "# Set environment variable to skip LFS files\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned {BRANCH} branch!\")\n",
    "    print(f\"üìÇ Repository location: {os.path.abspath(REPO_DIR)}\")\n",
    "    \n",
    "    # List key directories to verify\n",
    "    print(\"\\nüìÅ Key directories found:\")\n",
    "    key_dirs = [\"backend\", \"backend/src\", \"backend/dataset\"]\n",
    "    for dir_path in key_dirs:\n",
    "        full_path = os.path.join(REPO_DIR, dir_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ‚úÖ {dir_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_path} (not found)\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!git config user.name \"moonmehedi\"\n",
    "!git config user.email \"the.mehedi.hasan.moon@gmail.com\"\n",
    "\n",
    "print(\"‚úÖ Git identity configured!\")\n",
    "print(f\"   User: moonmehedi\")\n",
    "print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "\n",
    "# Verify current branch\n",
    "branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING GIT PUSH CAPABILITY\n",
      "============================================================\n",
      "‚úÖ Created detailed test log: kaggle_logs/connection_test.log\n",
      "[kaggle-run 7a97502] Test: Kaggle GPU verification - 2025-12-13 16:32:42\n",
      " 1 file changed, 27 insertions(+)\n",
      " create mode 100644 kaggle_logs/connection_test.log\n",
      "[kaggle-run 7a97502] Test: Kaggle GPU verification - 2025-12-13 16:32:42\n",
      " 1 file changed, 27 insertions(+)\n",
      " create mode 100644 kaggle_logs/connection_test.log\n",
      "\n",
      "üîÑ Pushing to GitHub...\n",
      "\n",
      "üîÑ Pushing to GitHub...\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-13 16:32:42\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-13 16:32:42\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test git push by creating a test log file\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING GIT PUSH CAPABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(\"kaggle_logs\", exist_ok=True)\n",
    "\n",
    "# Create a detailed test log file\n",
    "test_log_file = \"kaggle_logs/connection_test.log\"\n",
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "\n",
    "with open(test_log_file, \"w\") as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"üî¨ KAGGLE GPU TEST RUN - CONNECTION VERIFICATION\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"üìÖ Test Date: {current_time.strftime('%Y-%m-%d')}\\n\")\n",
    "    f.write(f\"‚è∞ Test Time: {current_time.strftime('%H:%M:%S UTC')}\\n\")\n",
    "    f.write(f\"üìç Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"üéÆ GPU CONFIGURATION\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"GPU Count: {torch.cuda.device_count()}\\n\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            f.write(f\"\\nGPU {i}:\\n\")\n",
    "            f.write(f\"  - Name: {torch.cuda.get_device_name(i)}\\n\")\n",
    "            f.write(f\"  - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\\n\")\n",
    "    else:\n",
    "        f.write(\"‚ö†Ô∏è  No GPU detected\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"üìä ENVIRONMENT INFO\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "    f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
    "    f.write(f\"Working Directory: {os.getcwd()}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"‚úÖ TEST STATUS: SUCCESS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"\\nThis log was generated from Kaggle notebook\\n\")\n",
    "    f.write(f\"Push attempt at: {current_time.isoformat()}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Created detailed test log: {test_log_file}\")\n",
    "\n",
    "# Add and commit\n",
    "!git add kaggle_logs/connection_test.log\n",
    "!git commit -m \"Test: Kaggle GPU verification - {current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Set up remote URL with token\n",
    "if \"GITHUB_TOKEN\" in os.environ:\n",
    "    repo_url_with_token = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "    !git remote set-url origin {repo_url_with_token}\n",
    "    print(\"\\nüîÑ Pushing to GitHub...\")\n",
    "    push_result = subprocess.run([\"git\", \"push\", \"origin\", BRANCH], capture_output=True, text=True)\n",
    "    \n",
    "    if push_result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully pushed to GitHub!\")\n",
    "        print(f\"   üìÅ Check: kaggle_logs/connection_test.log\")\n",
    "        print(f\"   üìÖ Pushed at: {current_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"   üí° Pull on your local machine to verify sync\")\n",
    "    else:\n",
    "        print(f\"‚ùå Push failed: {push_result.stderr}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot push: GitHub token not configured\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing ends"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310545,
     "sourceId": 375832,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
