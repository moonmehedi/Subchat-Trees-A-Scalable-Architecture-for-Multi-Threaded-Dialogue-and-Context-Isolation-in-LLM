{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hierarchical Subchat System - Kaggle GPU Testing\n",
    "\n",
    "## üìã Setup Checklist (Do Once):\n",
    "\n",
    "### 1Ô∏è‚É£ **Add Kaggle Secrets** (Most Important!)\n",
    "Go to: **https://www.kaggle.com/settings** ‚Üí Add-ons ‚Üí Secrets\n",
    "\n",
    "Add these two secrets:\n",
    "- **`GROQ_API_KEY`** = Your Groq API key (for query decomposition)\n",
    "- **`GITHUB_TOKEN`** = Your GitHub personal access token (for pushing results)\n",
    "\n",
    "### 2Ô∏è‚É£ **Enable Internet in This Notebook**\n",
    "- Click \"‚öôÔ∏è Settings\" (top right)\n",
    "- Turn ON **\"Internet\"** toggle\n",
    "- Click \"Save\"\n",
    "\n",
    "### 3Ô∏è‚É£ **Make Sure This Notebook is PRIVATE**\n",
    "- Never share secrets in public notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Run Order:\n",
    "1. **Cells 2-7**: Load secrets and libraries\n",
    "2. **Cell 8**: Set environment variables (LLM_BACKEND=vllm)\n",
    "3. **Cell 9**: Load vLLM model on Kaggle GPUs (Qwen-3 14B AWQ) - **Takes 2-3 minutes**\n",
    "4. **Cells 15-17**: Clone repo, configure git, run test log push\n",
    "5. **Cell 21**: Integrate vLLM with backend (registers model globally)\n",
    "6. **Cell 23**: Test vLLM integration\n",
    "7. **Run your actual tests**: Execute test scripts in backend/dataset/\n",
    "8. **Push results**: Use git_commit_and_push() function to sync to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Loads vLLM model (Qwen-3 14B) on Kaggle's 2x GPUs with AWQ quantization\n",
    "- ‚úÖ Integrates vLLM with your backend via `VLLMClient` singleton\n",
    "- ‚úÖ Runs hierarchical subchat tests using GPU-accelerated inference\n",
    "- ‚úÖ Generates performance logs for buffer size analysis\n",
    "- ‚úÖ Automatically syncs results back to your GitHub repo\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Technical Details:\n",
    "- **vLLM Config**: Tensor parallelism across both GPUs, 91% memory utilization\n",
    "- **Backend Mode**: `LLM_BACKEND=vllm` (set in cell 8)\n",
    "- **Model**: Qwen-3 14B AWQ (5120 max tokens, prefix caching enabled)\n",
    "- **No .env needed**: Secrets loaded from Kaggle environment\n",
    "- **Git Workflow**: Clone ‚Üí Test ‚Üí Push results to `kaggle-run` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING OUT MY GPU WORKINGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** repo: https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:41.966872Z",
     "iopub.status.busy": "2025-12-13T13:14:41.966627Z",
     "iopub.status.idle": "2025-12-13T13:14:44.519520Z",
     "shell.execute_reply": "2025-12-13T13:14:44.518617Z",
     "shell.execute_reply.started": "2025-12-13T13:14:41.966849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:44.521819Z",
     "iopub.status.busy": "2025-12-13T13:14:44.521553Z",
     "iopub.status.idle": "2025-12-13T13:14:48.187393Z",
     "shell.execute_reply": "2025-12-13T13:14:48.186532Z",
     "shell.execute_reply.started": "2025-12-13T13:14:44.521795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# ‚úÖ NEW: Set vLLM model path for backend config\n",
    "# This will be used by config.py when backend starts\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.187944Z",
     "iopub.status.idle": "2025-12-13T13:14:48.188268Z",
     "shell.execute_reply": "2025-12-13T13:14:48.188112Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.188098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# Use 14B model (32B causes CUDA linker failures on Kaggle T4 GPUs)\n",
    "#model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.189707Z",
     "iopub.status.idle": "2025-12-13T13:14:48.190032Z",
     "shell.execute_reply": "2025-12-13T13:14:48.189881Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.189866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def stream_generate(llm, prompt):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    for output in llm.generate(\n",
    "        [prompt],\n",
    "        sampling_params,\n",
    "        \n",
    "    ):\n",
    "        yield output.outputs[0].text\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "User: Explain tensor parallelism in simple terms.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.191299Z",
     "iopub.status.idle": "2025-12-13T13:14:48.191605Z",
     "shell.execute_reply": "2025-12-13T13:14:48.191474Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.191458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is quantum computing?\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from Kaggle's secure environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê LOADING SECRETS FROM KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load GROQ_API_KEY\n",
    "try:\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úÖ GROQ_API_KEY loaded successfully\")\n",
    "    print(f\"   Key length: {len(GROQ_API_KEY)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GROQ_API_KEY not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "# Try to load GITHUB_TOKEN\n",
    "try:\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
    "    print(\"‚úÖ GITHUB_TOKEN loaded successfully\")\n",
    "    print(f\"   Token length: {len(GITHUB_TOKEN)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GITHUB_TOKEN not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "\n",
    "# Set LLM backend to use vLLM (local model on Kaggle GPU)\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"  # We'll use the vLLM model loaded above\n",
    "print(\"\\n‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and configuration\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Current working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the kaggle-run branch from GitHub (PUBLIC READ - no auth needed)\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone the specific branch (no authentication needed for public repos)\n",
    "# Skip LFS files to avoid bandwidth quota issues\n",
    "print(f\"üîÑ Cloning {BRANCH} branch (skipping LFS files)...\")\n",
    "print(\"   No authentication required for cloning (public repo)\")\n",
    "\n",
    "# Set environment variable to skip LFS files\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned {BRANCH} branch!\")\n",
    "    print(f\"üìÇ Repository location: {os.path.abspath(REPO_DIR)}\")\n",
    "    \n",
    "    # Pull Git LFS files for scenarios only (saves bandwidth)\n",
    "    print(\"\\nüì• Pulling Git LFS scenario files...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    lfs_result = subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if lfs_result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully pulled scenario files from Git LFS\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Git LFS pull returned code {lfs_result.returncode}\")\n",
    "        if lfs_result.stderr:\n",
    "            print(f\"   {lfs_result.stderr}\")\n",
    "    \n",
    "    os.chdir(\"..\")  # Return to parent directory\n",
    "    \n",
    "    # List key directories to verify\n",
    "    print(\"\\nüìÅ Key directories found:\")\n",
    "    key_dirs = [\"backend\", \"backend/src\", \"backend/dataset\"]\n",
    "    for dir_path in key_dirs:\n",
    "        full_path = os.path.join(REPO_DIR, dir_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ‚úÖ {dir_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_path} (not found)\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure git identity\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!git config user.name \"moonmehedi\"\n",
    "!git config user.email \"the.mehedi.hasan.moon@gmail.com\"\n",
    "\n",
    "print(\"‚úÖ Git identity configured!\")\n",
    "print(f\"   User: moonmehedi\")\n",
    "print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "\n",
    "# Verify current branch\n",
    "branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_log(log_dir=\"kaggle_logs\", log_file=\"connection_test.log\"):\n",
    "    \"\"\"Create a detailed test log with GPU and environment info\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üî¨ KAGGLE GPU TEST RUN - CONNECTION VERIFICATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"üìÖ Test Date: {current_time.strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"‚è∞ Test Time: {current_time.strftime('%H:%M:%S UTC')}\\n\")\n",
    "        f.write(f\"üìç Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üéÆ GPU CONFIGURATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"GPU Count: {torch.cuda.device_count()}\\n\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                f.write(f\"\\nGPU {i}:\\n\")\n",
    "                f.write(f\"  - Name: {torch.cuda.get_device_name(i)}\\n\")\n",
    "                f.write(f\"  - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\\n\")\n",
    "        else:\n",
    "            f.write(\"‚ö†Ô∏è  No GPU detected\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"üìä ENVIRONMENT INFO\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "        f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
    "        f.write(f\"Working Directory: {os.getcwd()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"‚úÖ TEST STATUS: SUCCESS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"\\nThis log was generated from Kaggle notebook\\n\")\n",
    "        f.write(f\"Push attempt at: {current_time.isoformat()}\\n\")\n",
    "    \n",
    "    return log_path, current_time\n",
    "\n",
    "\n",
    "def git_commit_and_push(file_path, commit_message, branch=\"kaggle-run\"):\n",
    "    \"\"\"Commit a file and push to GitHub\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Add file\n",
    "    add_result = subprocess.run([\"git\", \"add\", file_path], capture_output=True, text=True)\n",
    "    if add_result.returncode != 0:\n",
    "        return False, f\"Git add failed: {add_result.stderr}\"\n",
    "    \n",
    "    # Commit\n",
    "    commit_result = subprocess.run([\"git\", \"commit\", \"-m\", commit_message], capture_output=True, text=True)\n",
    "    if commit_result.returncode != 0:\n",
    "        return False, f\"Git commit failed: {commit_result.stderr}\"\n",
    "    \n",
    "    # Push with token\n",
    "    if \"GITHUB_TOKEN\" not in os.environ:\n",
    "        return False, \"GITHUB_TOKEN not found in environment\"\n",
    "    \n",
    "    repo_url_with_token = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "    \n",
    "    # Set remote URL\n",
    "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", repo_url_with_token], capture_output=True)\n",
    "    \n",
    "    # Push\n",
    "    push_result = subprocess.run([\"git\", \"push\", \"origin\", branch], capture_output=True, text=True)\n",
    "    \n",
    "    if push_result.returncode == 0:\n",
    "        return True, \"Push successful\"\n",
    "    else:\n",
    "        return False, f\"Push failed: {push_result.stderr}\"\n",
    "\n",
    "\n",
    "# Main execution\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING GIT PUSH CAPABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Change to repo directory\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Create test log\n",
    "    log_path, timestamp = create_test_log()\n",
    "    print(f\"‚úÖ Created detailed test log: {log_path}\")\n",
    "    \n",
    "    # Commit and push\n",
    "    commit_msg = f\"Test: Kaggle GPU verification - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    success, message = git_commit_and_push(log_path, commit_msg, BRANCH)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Successfully pushed to GitHub!\")\n",
    "        print(f\"   üìÅ Check: {log_path}\")\n",
    "        print(f\"   üìÖ Pushed at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"   üí° Pull on your local machine to verify sync\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Always return to parent directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Step 5: Integrate vLLM with Backend\n",
    "\n",
    "**This connects the loaded vLLM model to your backend code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the vLLM model with the backend\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó INTEGRATING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Register the globally loaded vLLM model\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"‚úÖ vLLM model is now available to backend services\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Backend will use LLM_BACKEND={os.getenv('LLM_BACKEND')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r /kaggle/working/Subchat-Trees/backend/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Step 6: Test vLLM Integration\n",
    "\n",
    "**Quick test to verify backend can use vLLM on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the backend with vLLM\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING BACKEND WITH vLLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple test\n",
    "llm_client = SimpleLLMClient()\n",
    "\n",
    "# Create a test node (TreeNode creates its own buffer internally)\n",
    "# Note: TreeNode uses 'node_id' (not 'id'), and buffer_size (not buffer object)\n",
    "root = TreeNode(\n",
    "    node_id=\"test\", \n",
    "    title=\"Test Conversation\", \n",
    "    buffer_size=5,\n",
    "    llm_client=llm_client\n",
    ")\n",
    "\n",
    "# Add a message to the node's buffer\n",
    "root.buffer.add_message(\"user\", \"Hello, test message\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nüìù Testing response generation...\")\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "\n",
    "print(f\"\\n‚úÖ Response: {response}\")\n",
    "print(f\"üìä Token usage: {llm_client.get_last_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Backend integration successful!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Step 7: Start Backend Server\n",
    "\n",
    "**Run the FastAPI server with vLLM backend on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ SETUP COMPLETE - READY FOR SUBCHAT TREES EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: vLLM model must be registered in the SAME process as the server\n",
    "# We'll use nest_asyncio to allow uvicorn to run in Jupyter's existing event loop\n",
    "\n",
    "import uvicorn\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ensure backend is in path\n",
    "import sys\n",
    "sys.path.insert(0, f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Re-register vLLM model (in case it was lost)\n",
    "from src.services.vllm_client import VLLMClient\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING BACKEND SERVER IN NOTEBOOK KERNEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Backend path: /kaggle/working/{REPO_DIR}/backend\")\n",
    "print(f\"üîß Backend mode: {os.getenv('LLM_BACKEND')}\")\n",
    "print(f\"‚úÖ vLLM model registered: {VLLMClient.is_available()}\")\n",
    "print(f\"üéØ Server URL: http://0.0.0.0:8000\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  Server starting... (will block this cell)\")\n",
    "print(\"üí° Stop with: Kernel ‚Üí Interrupt\\n\")\n",
    "\n",
    "# Change to backend directory\n",
    "os.chdir(f\"/kaggle/working/{REPO_DIR}/backend\")\n",
    "\n",
    "# Start the server programmatically (same process, can access llm variable)\n",
    "# nest_asyncio allows this to work in Jupyter's existing event loop\n",
    "uvicorn.run(\"src.main:app\", host=\"0.0.0.0\", port=8000, reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310545,
     "sourceId": 375832,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
