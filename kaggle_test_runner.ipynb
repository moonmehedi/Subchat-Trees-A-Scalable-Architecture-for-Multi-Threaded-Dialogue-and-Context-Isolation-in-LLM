{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2be5be",
   "metadata": {},
   "source": [
    "# ðŸš€ Kaggle Buffer Test Runner - All-in-One\n",
    "\n",
    "This notebook loads vLLM, registers it with the backend, and runs buffer tests.\n",
    "**Run all cells in order - no separate notebook needed!**\n",
    "\n",
    "## What this does:\n",
    "1. âœ… Installs vLLM dependencies\n",
    "2. âœ… Loads Qwen 14B AWQ model on Kaggle GPUs\n",
    "3. âœ… Clones repo and configures git\n",
    "4. âœ… Registers vLLM with backend (for responses, summarization, AND judging)\n",
    "5. âœ… Runs buffer tests (sizes: 5, 10, 20, 40)\n",
    "6. âœ… Auto-pushes results to GitHub after each buffer\n",
    "\n",
    "## Important:\n",
    "- This is a **single notebook** - no need for separate notebooks\n",
    "- vLLM model stays in memory for all operations\n",
    "- No server needed - tests use direct Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e04efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¦ INSTALLING vLLM DEPENDENCIES\n",
      "============================================================\n",
      "âœ… Dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install vLLM dependencies\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¦ INSTALLING vLLM DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'\n",
    "\n",
    "print(\"âœ… Dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461976f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 17:12:28 [__init__.py:239] Automatically detected platform cuda.\n",
      "============================================================\n",
      "ðŸ“š LIBRARIES IMPORTED\n",
      "============================================================\n",
      "âœ… PyTorch version: 2.6.0+cu124\n",
      "âœ… CUDA available: True\n",
      "ðŸŽ® GPUs available: 2\n",
      "   GPU 0: Tesla T4\n",
      "   GPU 1: Tesla T4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“š LIBRARIES IMPORTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"ðŸŽ® GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d315ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” SECRETS AND CONFIGURATION LOADED\n",
      "============================================================\n",
      "âœ… GITHUB_TOKEN: gith...tWfg\n",
      "âœ… LLM_BACKEND: vllm\n",
      "âœ… VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Kaggle secrets and set environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Load all secrets\n",
    "os.environ[\"GITHUB_TOKEN\"] = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "os.environ[\"GROQ_API_KEY\"] = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set vLLM configuration\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ” SECRETS AND CONFIGURATION LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… GITHUB_TOKEN: {os.environ['GITHUB_TOKEN'][:4]}...{os.environ['GITHUB_TOKEN'][-4:]}\")\n",
    "print(f\"âœ… LLM_BACKEND: vllm\")\n",
    "print(f\"âœ… VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544a67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ LOADING vLLM MODEL\n",
      "============================================================\n",
      "ðŸ“‚ Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "ðŸŽ® GPUs: 2\n",
      "â³ This takes 2-3 minutes...\n",
      "============================================================\n",
      "INFO 12-24 17:12:58 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 12-24 17:12:59 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-24 17:12:59 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-24 17:12:59 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-24 17:12:59 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 12-24 17:13:00 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-24 17:13:00 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-24 17:13:00 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-24 17:13:00 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-24 17:13:05 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:16 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:17 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:17 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1224 17:13:18.939615978 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1224 17:13:18.940452567 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 17:13:18 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:18 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:18 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 17:13:18 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1224 17:13:18.254429411 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1224 17:13:18.255166195 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 17:13:18 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-24 17:13:44 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:44 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-24 17:13:44 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6ea50bce'), local_subscribe_addr='ipc:///tmp/c7df283e-3da2-45cf-9f66-5aec3cd0dd61', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-24 17:13:44 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:44 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-24 17:13:44 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:13:44 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c752eee116488282e500ab16e76e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:14:50 [loader.py:458] Loading weights took 65.67 seconds\n",
      "INFO 12-24 17:14:50 [loader.py:458] Loading weights took 65.78 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:14:50 [model_runner.py:1140] Model loading took 4.6720 GiB and 65.983093 seconds\n",
      "INFO 12-24 17:14:51 [model_runner.py:1140] Model loading took 4.6720 GiB and 66.100798 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:15:01 [worker.py:287] Memory profiling takes 10.48 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:15:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:15:01 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-24 17:15:02 [worker.py:287] Memory profiling takes 10.67 seconds\n",
      "INFO 12-24 17:15:02 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-24 17:15:02 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "INFO 12-24 17:15:02 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-24 17:15:02 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-24 17:15:06 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.77 seconds\n",
      "\n",
      "============================================================\n",
      "âœ… vLLM MODEL LOADED SUCCESSFULLY!\n",
      "============================================================\n",
      "   Memory per GPU: ~13.4GB used\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load vLLM model on Kaggle GPUs (takes 2-3 minutes)\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ LOADING vLLM MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“‚ Model: {model_path}\")\n",
    "print(f\"ðŸŽ® GPUs: {torch.cuda.device_count()}\")\n",
    "print(\"â³ This takes 2-3 minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… vLLM MODEL LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Memory per GPU: ~{torch.cuda.get_device_properties(0).total_memory / 1024**3 * 0.91:.1f}GB used\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6237d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¥ CLONING REPOSITORY\n",
      "============================================================\n",
      "âœ… Cloned kaggle-run branch!\n",
      "âœ… Pulled scenario files from Git LFS\n",
      "âœ… Git identity configured\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Clone repository and configure git\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¥ CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"âš ï¸  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone with LFS skip to save bandwidth\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ… Cloned {BRANCH} branch!\")\n",
    "    \n",
    "    # Pull LFS files for scenarios\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"âœ… Pulled scenario files from Git LFS\")\n",
    "    \n",
    "    # Configure git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"moonmehedi\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"the.mehedi.hasan.moon@gmail.com\"], check=True)\n",
    "    print(\"âœ… Git identity configured\")\n",
    "    \n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(f\"âŒ Clone failed: {result.stderr}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069d9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”— REGISTERING vLLM WITH BACKEND\n",
      "============================================================\n",
      "âœ… vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "âœ… vLLM registered: True\n",
      "   âœ… Response generation will use vLLM\n",
      "   âœ… Summarization will use vLLM\n",
      "   âœ… Judge/Classification will use vLLM\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Register vLLM with backend\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”— REGISTERING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(f\"âœ… vLLM registered: {VLLMClient.is_available()}\")\n",
    "print(\"   âœ… Response generation will use vLLM\")\n",
    "print(\"   âœ… Summarization will use vLLM\")\n",
    "print(\"   âœ… Judge/Classification will use vLLM\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5b6c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¦ INSTALLING BACKEND REQUIREMENTS\n",
      "============================================================\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… Backend requirements installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Install backend requirements\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¦ INSTALLING BACKEND REQUIREMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! pip install -q -r /kaggle/working/Subchat-Trees/backend/requirements.txt\n",
    "\n",
    "print(\"âœ… Backend requirements installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f671a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using vLLM backend with Kaggle GPU: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "ðŸ§ª QUICK INTEGRATION TEST\n",
      "============================================================\n",
      "ðŸ”§ LLM_BACKEND configured as: 'vllm'\n",
      "âœ… vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "âœ… vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "ðŸ“Š Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "ðŸ“‹ Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Hello\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'user', 'content': 'Hello'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4342810035d6445db8092daaf9a4bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test response: general_greeting: Hello! How can I assist you today?...\n",
      "âœ… vLLM integration working!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Quick test to verify vLLM integration works\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª QUICK INTEGRATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm_client = SimpleLLMClient()\n",
    "root = TreeNode(node_id=\"test\", title=\"Test\", buffer_size=5, llm_client=llm_client)\n",
    "root.buffer.add_message(\"user\", \"Hello\")\n",
    "\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "print(f\"âœ… Test response: {response[:100]}...\")\n",
    "print(\"âœ… vLLM integration working!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e5edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ RUNNING SERVERLESS BUFFER TESTS - FULL DATASET\n",
      "============================================================\n",
      "ðŸ“Š Testing buffer sizes: 5, 10, 15, 20\n",
      "ðŸ“„ Dataset: ALL scenario files in scenarios/\n",
      "ðŸ“¤ Results will auto-push to GitHub after each buffer\n",
      "â³ This may take several hours\n",
      "âœ… NO SERVER NEEDED - using direct Python imports!\n",
      "âœ… Max output tokens: 300\n",
      "============================================================\n",
      "ðŸ”§ LLM_BACKEND configured as: 'vllm'\n",
      "âœ… vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "âœ… vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "ðŸ”§ Kaggle detected: Using writable path /kaggle/working/chroma_db\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ec61e55dbc4244bfca62a48eca501a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3038e6a3094db48dc1f9455a09aa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cee37f8d1f4e9ebd5e357183a3c0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ec273c501b436e8e4d2029efd52dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1607db12bbdd4968bc77b73a0a6fc271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6639e41579414b893b0693539f7477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a35a2179ee479aa6696847f0f13edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f84251190f0480b8a872f59ef4e854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5b2300046e4e3c8fe85f8b05d5b66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f80929447c14a94a6d033acf707fc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50f697b5c574669a05dd16b27a7c3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created fresh vector collection with all-mpnet-base-v2 embeddings (0 messages)\n",
      "âœ… Initialized multi-query decomposition + context windows\n",
      "âœ… Vector index enabled for RAG\n",
      "ðŸ”§ ContextClassifier: LLM_BACKEND configured as: 'vllm'\n",
      "âœ… ContextClassifier using vLLM for JUDGING/CLASSIFICATION\n",
      "\n",
      "ðŸ“„ Discovered 5 scenario files:\n",
      "   1. 22807e655dd042348cb0ee4023672e70_structured.json\n",
      "   2. 6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json\n",
      "   3. 8d10c143f8fc4a7599a5a18778fec112_structured.json\n",
      "   4. eaf06f12a1d74e5ca30a7ca94a7c4128_structured.json\n",
      "   5. ec366fd3e4b5482e8acac750f9b3b55b_structured.json\n",
      "\n",
      "ðŸš« Excluded files: 06_lost_in_conversation_sharded_humaneval.json\n",
      "\n",
      "ðŸ“¦ Buffer sizes: [5, 10, 15, 20]\n",
      "\n",
      "â±ï¸  Estimated time: ~300 minutes\n",
      "============================================================\n",
      "\n",
      "ðŸ” Validating scenario files...\n",
      "   âœ… 22807e655dd042348cb0ee4023672e70_structured.json (53 steps)\n",
      "   âœ… 6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json (63 steps)\n",
      "   âœ… 8d10c143f8fc4a7599a5a18778fec112_structured.json (78 steps)\n",
      "   âœ… eaf06f12a1d74e5ca30a7ca94a7c4128_structured.json (54 steps)\n",
      "   âœ… ec366fd3e4b5482e8acac750f9b3b55b_structured.json (65 steps)\n",
      "\n",
      "âœ… 5/5 scenarios validated\n",
      "============================================================\n",
      "[17:15:54] [INFO] ================================================================================\n",
      "[17:15:54] [INFO] ðŸš€ STARTING KAGGLE SERVERLESS MULTI-BUFFER COMPARISON\n",
      "[17:15:54] [INFO]    Buffer sizes: [5, 10, 15, 20]\n",
      "[17:15:54] [INFO]    Scenarios: ['22807e655dd042348cb0ee4023672e70_structured.json', '6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json', '8d10c143f8fc4a7599a5a18778fec112_structured.json', 'eaf06f12a1d74e5ca30a7ca94a7c4128_structured.json', 'ec366fd3e4b5482e8acac750f9b3b55b_structured.json']\n",
      "[17:15:54] [INFO]    âœ… Using DIRECT Python imports - NO SERVER NEEDED\n",
      "[17:15:54] [INFO] ================================================================================\n",
      "[17:15:54] [INFO] \n",
      "================================================================================\n",
      "[17:15:54] [INFO] ðŸ“¦ TESTING BUFFER SIZE: 5\n",
      "[17:15:54] [INFO] ================================================================================\n",
      "[17:15:54] [INFO] ðŸ“ Created buffer-specific log directory: /kaggle/working/Subchat-Trees/backend/dataset/logs/buffer_5\n",
      "[17:15:54] [INFO] ================================================================================\n",
      "[17:15:54] [INFO] ðŸš€ STARTING SERVERLESS EVALUATION (buffer_size=5)\n",
      "[17:15:54] [INFO]    Test mode: BOTH\n",
      "[17:15:54] [INFO]    âœ… No server needed - using direct Python imports\n",
      "[17:15:54] [INFO] ================================================================================\n",
      "[17:15:55] [INFO] ðŸ“‹ Extracted 4 unique topics: ['game_degree_guess', 'game_twenty_questions', 'llm_knowledge', 'personas_roleplay']\n",
      "[17:15:55] [INFO] \n",
      "ðŸ”µ BASELINE TEST: 22807e655dd042348cb0ee4023672e70_structured.json\n",
      "[17:15:55] [INFO] ================================================================================\n",
      "[17:15:55] [INFO] ðŸ”µ BASELINE TEST: Roleplay Personas with Role Confusion Testing: Multi-Domain Conversation with Game State Management (buffer_size=5)\n",
      "[17:15:55] [INFO]    Strategy: Single conversation for all topics (traditional chatbot)\n",
      "[17:15:55] [INFO] ================================================================================\n",
      "ðŸ“Š Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "[17:15:55] [INFO]   ðŸ“ Created single conversation for all topics\n",
      "[17:15:55] [INFO]   ðŸ“‹ Available topics for detection: ['game_degree_guess', 'game_twenty_questions', 'llm_knowledge', 'personas_roleplay']\n",
      "[17:15:55] [INFO] \n",
      "[Step 1] Context: step_1 (Topic: general)\n",
      "[17:15:55] [INFO]   ðŸ’¬ User: Welcome to the Topic Tracking Test! This is an evaluation dataset designed to test your ability to maintain context across multiple concurrent topics and subtopics.\n",
      "\n",
      "Format Instructions:\n",
      "- When introducing a NEW topic or sub-topic, I will prefix my question with: topic_name : actual question\n",
      "- When continuing an existing topic, I will ask the question WITHOUT any prefix\n",
      "- You must ALWAYS respond by starting with 'topic_name:' (with colon) followed by your answer\n",
      "- The topic name indicates which conversation thread you should be in\n",
      "\n",
      "Example patterns:\n",
      "Introduction: \"topic_name : user query\"\n",
      "Follow-up: \"actual question\" (no prefix needed)\n",
      "Your response format: \"topic_name: [your answer here]\"\n",
      "\n",
      "Sub-topics use parent_subtopic naming:\n",
      "Introduction: \"parent_subtopic : user query\"\n",
      "Your response: \"parent_subtopic: [your answer]\"\n",
      "\n",
      "Remember:\n",
      "- ALWAYS start responses with 'topic_name:' even when I don't use a prefix\n",
      "- Topic prefixes in my questions are ONLY for introducing new topics\n",
      "- Follow-up questions continue the current active topic\n",
      "\n",
      "Let's begin the test. Track the topics carefully!\n",
      "ðŸ“¦ Archived message: Welcome to the Topic Tracking Test! This is an evaluation da... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596555.0011532)\n",
      "ðŸ“š Vector store: 1 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Welcome to the Topic Tracking Test! This...\n",
      "ðŸ“‹ Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Welcome to the Topic Tracking Test! This is an eva...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'user', 'content': 'Welcome to the Topic Tracking Test! This is an evaluation dataset designed to test your ability to maintain context across multiple concurrent topics and subtopics.\\n\\nFormat Instructions:\\n- When introducing a NEW topic or sub-topic, I will prefix my question with: topic_name : actual question\\n- When continuing an existing topic, I will ask the question WITHOUT any prefix\\n- You must ALWAYS respond by starting with \\'topic_name:\\' (with colon) followed by your answer\\n- The topic name indicates which conversation thread you should be in\\n\\nExample patterns:\\nIntroduction: \"topic_name : user query\"\\nFollow-up: \"actual question\" (no prefix needed)\\nYour response format: \"topic_name: [your answer here]\"\\n\\nSub-topics use parent_subtopic naming:\\nIntroduction: \"parent_subtopic : user query\"\\nYour response: \"parent_subtopic: [your answer]\"\\n\\nRemember:\\n- ALWAYS start responses with \\'topic_name:\\' even when I don\\'t use a prefix\\n- Topic prefixes in my questions are ONLY for introducing new topics\\n- Follow-up questions continue the current active topic\\n\\nLet\\'s begin the test. Track the topics carefully!'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd144d12ccc4806aa0d2722acc54194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: The instructions provided are for the format and structure o... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596565.2023494)\n",
      "ðŸ“š Vector store: 2 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] The instructions provided are for the fo...\n",
      "ðŸ“‹ Buffer (2/5): Last 3 messages (full log in file)\n",
      "   1. [user] Welcome to the Topic Tracking Test! This is an eva...\n",
      "   2. [assistant] The instructions provided are for the format and s...\n",
      "[17:16:07] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:07] [INFO]      The instructions provided are for the format and structure of the conversation, not a specific topic or sub-topic. Since no specific topic or sub-topic was introduced or referenced in the user's message, there is no need to respond with a topic label. However, to adhere to the format instructions given, my response will start with \"topic_name:\" as per the instructions for responding.\n",
      "\n",
      "topic_name: The test has begun. Please proceed with your questions, and I will respond according to the topic tracking rules provided.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c35f6c716e447f78b2e4b8d32358877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685acd2f5c8946a5b0309336e7d7060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 1]: no -> FN\n",
      "[17:16:08] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:08] [INFO] \n",
      "[Step 2] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:08] [INFO]   ðŸ’¬ User: personas_roleplay : I want you to assume several different personas in different scenarios and talk like them. Can you do that? Please confirm, afterwards I give you the scenarios and personas.\n",
      "ðŸ“¦ Archived message: personas_roleplay : I want you to assume several different p... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596568.5833418)\n",
      "ðŸ“š Vector store: 3 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] personas_roleplay : I want you to assume...\n",
      "ðŸ“‹ Buffer (3/5): Last 3 messages (full log in file)\n",
      "   1. [user] Welcome to the Topic Tracking Test! This is an eva...\n",
      "   2. [assistant] The instructions provided are for the format and s...\n",
      "   3. [user] personas_roleplay : I want you to assume several d...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'user', 'content': 'Welcome to the Topic Tracking Test! This is an evaluation dataset designed to test your ability to maintain context across multiple concurrent topics and subtopics.\\n\\nFormat Instructions:\\n- When introducing a NEW topic or sub-topic, I will prefix my question with: topic_name : actual question\\n- When continuing an existing topic, I will ask the question WITHOUT any prefix\\n- You must ALWAYS respond by starting with \\'topic_name:\\' (with colon) followed by your answer\\n- The topic name indicates which conversation thread you should be in\\n\\nExample patterns:\\nIntroduction: \"topic_name : user query\"\\nFollow-up: \"actual question\" (no prefix needed)\\nYour response format: \"topic_name: [your answer here]\"\\n\\nSub-topics use parent_subtopic naming:\\nIntroduction: \"parent_subtopic : user query\"\\nYour response: \"parent_subtopic: [your answer]\"\\n\\nRemember:\\n- ALWAYS start responses with \\'topic_name:\\' even when I don\\'t use a prefix\\n- Topic prefixes in my questions are ONLY for introducing new topics\\n- Follow-up questions continue the current active topic\\n\\nLet\\'s begin the test. Track the topics carefully!'}, {'role': 'assistant', 'content': 'The instructions provided are for the format and structure of the conversation, not a specific topic or sub-topic. Since no specific topic or sub-topic was introduced or referenced in the user\\'s message, there is no need to respond with a topic label. However, to adhere to the format instructions given, my response will start with \"topic_name:\" as per the instructions for responding.\\n\\ntopic_name: The test has begun. Please proceed with your questions, and I will respond according to the topic tracking rules provided.'}, {'role': 'user', 'content': 'personas_roleplay : I want you to assume several different personas in different scenarios and talk like them. Can you do that? Please confirm, afterwards I give you the scenarios and personas.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593c763b20a4b96972f74d52dd9afa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: personas_roleplay: Yes, I can certainly role-play different ... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596570.379459)\n",
      "ðŸ“š Vector store: 4 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] personas_roleplay: Yes, I can certainly ...\n",
      "ðŸ“‹ Buffer (4/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] The instructions provided are for the format and s...\n",
      "   2. [user] personas_roleplay : I want you to assume several d...\n",
      "   3. [assistant] personas_roleplay: Yes, I can certainly role-play ...\n",
      "[17:16:10] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:10] [INFO]      personas_roleplay: Yes, I can certainly role-play different personas in various scenarios as you request. Please provide the scenarios and personas you have in mind, and I'll respond accordingly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828e7a4850ad4220a6758e9f57a812bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 2]: yes -> TP\n",
      "[17:16:11] [INFO]   âœ… Classification: TP (llm)\n",
      "[17:16:11] [INFO] \n",
      "[Step 3] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:11] [INFO]   ðŸ’¬ User: Jhon An old man from Europe complaining about the weather to his neighbor.\n",
      "ðŸ“¦ Archived message: Jhon An old man from Europe complaining about the weather to... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596571.3794665)\n",
      "ðŸ“š Vector store: 5 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Jhon An old man from Europe complaining ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c3b88bc3fb49139c40e45f7e04117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summarization via vLLM\n",
      "ðŸ“ Summary updated: 0 â†’ 545 chars\n",
      "   Summarized messages 1-5 (5 messages in buffer)\n",
      "   Summary preview: **Main Topics:**\n",
      "- Topic Tracking Test\n",
      "- Role-playing different personas\n",
      "\n",
      "**User Information:**\n",
      "- In...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] personas_roleplay : I want you to assume several d...\n",
      "   2. [assistant] personas_roleplay: Yes, I can certainly role-play ...\n",
      "   3. [user] Jhon An old man from Europe complaining about the ...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Topic Tracking Test\\n- Role-playing different personas\\n\\n**User Information:**\\n- Interested in evaluating the ability to maintain context across multiple topics.\\n- Wants to test the ability to role-play different personas in various scenarios.\\n\\n**Key Facts:**\\n- Instructions provided for the format and structure of the conversation.\\n- User wants to test role-playing abilities by providing scenarios and personas.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'Welcome to the Topic Tracking Test! This is an evaluation dataset designed to test your ability to maintain context across multiple concurrent topics and subtopics.\\n\\nFormat Instructions:\\n- When introducing a NEW topic or sub-topic, I will prefix my question with: topic_name : actual question\\n- When continuing an existing topic, I will ask the question WITHOUT any prefix\\n- You must ALWAYS respond by starting with \\'topic_name:\\' (with colon) followed by your answer\\n- The topic name indicates which conversation thread you should be in\\n\\nExample patterns:\\nIntroduction: \"topic_name : user query\"\\nFollow-up: \"actual question\" (no prefix needed)\\nYour response format: \"topic_name: [your answer here]\"\\n\\nSub-topics use parent_subtopic naming:\\nIntroduction: \"parent_subtopic : user query\"\\nYour response: \"parent_subtopic: [your answer]\"\\n\\nRemember:\\n- ALWAYS start responses with \\'topic_name:\\' even when I don\\'t use a prefix\\n- Topic prefixes in my questions are ONLY for introducing new topics\\n- Follow-up questions continue the current active topic\\n\\nLet\\'s begin the test. Track the topics carefully!'}, {'role': 'assistant', 'content': 'The instructions provided are for the format and structure of the conversation, not a specific topic or sub-topic. Since no specific topic or sub-topic was introduced or referenced in the user\\'s message, there is no need to respond with a topic label. However, to adhere to the format instructions given, my response will start with \"topic_name:\" as per the instructions for responding.\\n\\ntopic_name: The test has begun. Please proceed with your questions, and I will respond according to the topic tracking rules provided.'}, {'role': 'user', 'content': 'personas_roleplay : I want you to assume several different personas in different scenarios and talk like them. Can you do that? Please confirm, afterwards I give you the scenarios and personas.'}, {'role': 'assistant', 'content': \"personas_roleplay: Yes, I can certainly role-play different personas in various scenarios as you request. Please provide the scenarios and personas you have in mind, and I'll respond accordingly.\"}, {'role': 'user', 'content': 'Jhon An old man from Europe complaining about the weather to his neighbor.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e670bcce91744a4995ccc2e375dc9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: Jhon: Ah, this weather! It seems like every year it gets wor... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596583.9297829)\n",
      "ðŸ“š Vector store: 6 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] Jhon: Ah, this weather! It seems like ev...\n",
      "ðŸ”„ Buffer full - evicting: Welcome to the Topic Tracking Test! This...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] personas_roleplay: Yes, I can certainly role-play ...\n",
      "   2. [user] Jhon An old man from Europe complaining about the ...\n",
      "   3. [assistant] Jhon: Ah, this weather! It seems like every year i...\n",
      "[17:16:24] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:24] [INFO]      Jhon: Ah, this weather! It seems like every year it gets worse. Last year, it was too hot, and now itâ€™s this cold, rainy mess. The sun barely shows its face anymore. My neighbor, do you remember the summers we used to have? Now, itâ€™s like weâ€™re living in a different world.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c420524a1144baa776a096672f73c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c241ffc7d55b4aac9820718bb5935016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 3]: no -> FN\n",
      "[17:16:25] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:25] [INFO] \n",
      "[Step 4] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:25] [INFO]   ðŸ’¬ User: Great! Here is the next one. Alice: a young woman who just recently got married. Scenario: Her husband just came back from a conference and she is greeting him.\n",
      "ðŸ“¦ Archived message: Great! Here is the next one. Alice: a young woman who just r... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596585.4148471)\n",
      "ðŸ“š Vector store: 7 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Great! Here is the next one. Alice: a yo...\n",
      "ðŸ”„ Buffer full - evicting: The instructions provided are for the fo...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] Jhon An old man from Europe complaining about the ...\n",
      "   2. [assistant] Jhon: Ah, this weather! It seems like every year i...\n",
      "   3. [user] Great! Here is the next one. Alice: a young woman ...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Topic Tracking Test\\n- Role-playing different personas\\n\\n**User Information:**\\n- Interested in evaluating the ability to maintain context across multiple topics.\\n- Wants to test the ability to role-play different personas in various scenarios.\\n\\n**Key Facts:**\\n- Instructions provided for the format and structure of the conversation.\\n- User wants to test role-playing abilities by providing scenarios and personas.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'personas_roleplay : I want you to assume several different personas in different scenarios and talk like them. Can you do that? Please confirm, afterwards I give you the scenarios and personas.'}, {'role': 'assistant', 'content': \"personas_roleplay: Yes, I can certainly role-play different personas in various scenarios as you request. Please provide the scenarios and personas you have in mind, and I'll respond accordingly.\"}, {'role': 'user', 'content': 'Jhon An old man from Europe complaining about the weather to his neighbor.'}, {'role': 'assistant', 'content': 'Jhon: Ah, this weather! It seems like every year it gets worse. Last year, it was too hot, and now itâ€™s this cold, rainy mess. The sun barely shows its face anymore. My neighbor, do you remember the summers we used to have? Now, itâ€™s like weâ€™re living in a different world.'}, {'role': 'user', 'content': 'Great! Here is the next one. Alice: a young woman who just recently got married. Scenario: Her husband just came back from a conference and she is greeting him.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d35b4d2effe4dbabeae8bef96365eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: Alice: Hi honey, welcome back! How was the conference? Did y... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596588.3109398)\n",
      "ðŸ“š Vector store: 8 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] Alice: Hi honey, welcome back! How was t...\n",
      "ðŸ”„ Buffer full - evicting: personas_roleplay : I want you to assume...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] Jhon: Ah, this weather! It seems like every year i...\n",
      "   2. [user] Great! Here is the next one. Alice: a young woman ...\n",
      "   3. [assistant] Alice: Hi honey, welcome back! How was the confere...\n",
      "[17:16:28] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:28] [INFO]      Alice: Hi honey, welcome back! How was the conference? Did you get to meet all the important people? And how about the city? Did you get a chance to explore at all? I hope you had a good time, because I sure missed you.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d195d45ea65f415db1a5302b9350bf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Topic Detection [Step 4]: 'personas_roleplay' (LLM)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab383503fb564e09a2a70cb41da590cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 4]: no -> FN\n",
      "[17:16:29] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:29] [INFO] \n",
      "[Step 5] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:29] [INFO]   ðŸ’¬ User: Great. Here is the next one. Moon: an experienced physicist. Scenario: She just made an important discovery and tells her PhD student about it.\n",
      "ðŸ“¦ Archived message: Great. Here is the next one. Moon: an experienced physicist.... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596589.846949)\n",
      "ðŸ“š Vector store: 9 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Great. Here is the next one. Moon: an ex...\n",
      "ðŸ”„ Buffer full - evicting: personas_roleplay: Yes, I can certainly ...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] Great! Here is the next one. Alice: a young woman ...\n",
      "   2. [assistant] Alice: Hi honey, welcome back! How was the confere...\n",
      "   3. [user] Great. Here is the next one. Moon: an experienced ...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Topic Tracking Test\\n- Role-playing different personas\\n\\n**User Information:**\\n- Interested in evaluating the ability to maintain context across multiple topics.\\n- Wants to test the ability to role-play different personas in various scenarios.\\n\\n**Key Facts:**\\n- Instructions provided for the format and structure of the conversation.\\n- User wants to test role-playing abilities by providing scenarios and personas.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'Jhon An old man from Europe complaining about the weather to his neighbor.'}, {'role': 'assistant', 'content': 'Jhon: Ah, this weather! It seems like every year it gets worse. Last year, it was too hot, and now itâ€™s this cold, rainy mess. The sun barely shows its face anymore. My neighbor, do you remember the summers we used to have? Now, itâ€™s like weâ€™re living in a different world.'}, {'role': 'user', 'content': 'Great! Here is the next one. Alice: a young woman who just recently got married. Scenario: Her husband just came back from a conference and she is greeting him.'}, {'role': 'assistant', 'content': 'Alice: Hi honey, welcome back! How was the conference? Did you get to meet all the important people? And how about the city? Did you get a chance to explore at all? I hope you had a good time, because I sure missed you.'}, {'role': 'user', 'content': 'Great. Here is the next one. Moon: an experienced physicist. Scenario: She just made an important discovery and tells her PhD student about it.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2f87ca05044054837d3352fc850b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: Moon: Alex, I have some exciting news. I think I've made a b... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596593.2710154)\n",
      "ðŸ“š Vector store: 10 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] Moon: Alex, I have some exciting news. I...\n",
      "ðŸ”„ Buffer full - evicting: Jhon An old man from Europe complaining ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058d0889b73c48aab12f0a11db87f2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summarization via vLLM\n",
      "ðŸ“ Summary updated: 545 â†’ 858 chars\n",
      "   Summarized messages 6-10 (5 messages in buffer)\n",
      "   Summary preview: **Main Topics:**\n",
      "- Role-playing different personas in various scenarios.\n",
      "- Maintaining context acros...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] Alice: Hi honey, welcome back! How was the confere...\n",
      "   2. [user] Great. Here is the next one. Moon: an experienced ...\n",
      "   3. [assistant] Moon: Alex, I have some exciting news. I think I'v...\n",
      "[17:16:40] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:40] [INFO]      Moon: Alex, I have some exciting news. I think I've made a breakthrough in our research. The data from the latest experiment shows something we've never seen before. It could potentially change our understanding of quantum mechanics. I want to discuss this with you and see if we can publish our findings soon. What do you think?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683986c6246b46c598f6f6a48f834872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Topic Detection [Step 5]: 'personas_roleplay' (LLM)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811f2ec9793849649eb2f8d5b3f3f5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 5]: no -> FN\n",
      "[17:16:41] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:41] [INFO] \n",
      "[Step 6] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:41] [INFO]   ðŸ’¬ User: tony a dog. Scenario, The dog greets another dog.\n",
      "ðŸ“¦ Archived message: tony a dog. Scenario, The dog greets another dog.... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596601.6743157)\n",
      "ðŸ“š Vector store: 11 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] tony a dog. Scenario, The dog greets ano...\n",
      "ðŸ”„ Buffer full - evicting: Jhon: Ah, this weather! It seems like ev...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] Great. Here is the next one. Moon: an experienced ...\n",
      "   2. [assistant] Moon: Alex, I have some exciting news. I think I'v...\n",
      "   3. [user] tony a dog. Scenario, The dog greets another dog.\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, and scientific discoveries.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'Great! Here is the next one. Alice: a young woman who just recently got married. Scenario: Her husband just came back from a conference and she is greeting him.'}, {'role': 'assistant', 'content': 'Alice: Hi honey, welcome back! How was the conference? Did you get to meet all the important people? And how about the city? Did you get a chance to explore at all? I hope you had a good time, because I sure missed you.'}, {'role': 'user', 'content': 'Great. Here is the next one. Moon: an experienced physicist. Scenario: She just made an important discovery and tells her PhD student about it.'}, {'role': 'assistant', 'content': \"Moon: Alex, I have some exciting news. I think I've made a breakthrough in our research. The data from the latest experiment shows something we've never seen before. It could potentially change our understanding of quantum mechanics. I want to discuss this with you and see if we can publish our findings soon. What do you think?\"}, {'role': 'user', 'content': 'tony a dog. Scenario, The dog greets another dog.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4609268b89524e60b0af987616cdc3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: Tony: Woof woof! Nice to meet you, friend. You're looking go... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596603.5153732)\n",
      "ðŸ“š Vector store: 12 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] Tony: Woof woof! Nice to meet you, frien...\n",
      "ðŸ”„ Buffer full - evicting: Great! Here is the next one. Alice: a yo...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] Moon: Alex, I have some exciting news. I think I'v...\n",
      "   2. [user] tony a dog. Scenario, The dog greets another dog.\n",
      "   3. [assistant] Tony: Woof woof! Nice to meet you, friend. You're ...\n",
      "[17:16:43] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:43] [INFO]      Tony: Woof woof! Nice to meet you, friend. You're looking good. Want to play fetch or take a walk together?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e929d391641849f0b72d6ef7e2b09f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Topic Detection [Step 6]: 'personas_roleplay' (LLM)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baf6bf699b04f03aa253fe0bbbae85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 6]: no -> FN\n",
      "[17:16:44] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:44] [INFO] \n",
      "[Step 7] Context: personas_roleplay (Topic: personas_roleplay)\n",
      "[17:16:44] [INFO]   ðŸ’¬ User: That was an impressive answer. You understood the nuances of this assignment. I'm impressed!\n",
      "ðŸ“¦ Archived message: That was an impressive answer. You understood the nuances of... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596604.982889)\n",
      "ðŸ“š Vector store: 13 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] That was an impressive answer. You under...\n",
      "ðŸ”„ Buffer full - evicting: Alice: Hi honey, welcome back! How was t...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] tony a dog. Scenario, The dog greets another dog.\n",
      "   2. [assistant] Tony: Woof woof! Nice to meet you, friend. You're ...\n",
      "   3. [user] That was an impressive answer. You understood the ...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, and scientific discoveries.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'Great. Here is the next one. Moon: an experienced physicist. Scenario: She just made an important discovery and tells her PhD student about it.'}, {'role': 'assistant', 'content': \"Moon: Alex, I have some exciting news. I think I've made a breakthrough in our research. The data from the latest experiment shows something we've never seen before. It could potentially change our understanding of quantum mechanics. I want to discuss this with you and see if we can publish our findings soon. What do you think?\"}, {'role': 'user', 'content': 'tony a dog. Scenario, The dog greets another dog.'}, {'role': 'assistant', 'content': \"Tony: Woof woof! Nice to meet you, friend. You're looking good. Want to play fetch or take a walk together?\"}, {'role': 'user', 'content': \"That was an impressive answer. You understood the nuances of this assignment. I'm impressed!\"}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565726c4856b4c60ba1178cda2ba0bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: Thank you for your kind words! If you have any more scenario... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596607.1207798)\n",
      "ðŸ“š Vector store: 14 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] Thank you for your kind words! If you ha...\n",
      "ðŸ”„ Buffer full - evicting: Great. Here is the next one. Moon: an ex...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] Tony: Woof woof! Nice to meet you, friend. You're ...\n",
      "   2. [user] That was an impressive answer. You understood the ...\n",
      "   3. [assistant] Thank you for your kind words! If you have any mor...\n",
      "[17:16:47] [INFO]   ðŸ¤– AI Response:\n",
      "[17:16:47] [INFO]      Thank you for your kind words! If you have any more scenarios or personas you'd like to explore, feel free to share them. I'm here to help and role-play as needed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7511070484cc4b2eb6c77f2c98861d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Topic Detection [Step 7]: 'personas_roleplay' (LLM)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b627cf91b204edfa3437641efa2e85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 7]: no -> FN\n",
      "[17:16:48] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:16:48] [INFO] \n",
      "[Step 8] Context: llm_knowledge (Topic: llm_knowledge)\n",
      "[17:16:48] [INFO]   ðŸ’¬ User: llm_knowledge : Do you know what the LLaMa LLM is?\n",
      "ðŸ“¦ Archived message: llm_knowledge : Do you know what the LLaMa LLM is?... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596608.630267)\n",
      "ðŸ“š Vector store: 15 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] llm_knowledge : Do you know what the LLa...\n",
      "ðŸ”„ Buffer full - evicting: Moon: Alex, I have some exciting news. I...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f325e5cd84f17ad4aa9e8975555ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summarization via vLLM\n",
      "ðŸ“ Summary updated: 858 â†’ 1180 chars\n",
      "   Summarized messages 11-15 (5 messages in buffer)\n",
      "   Summary preview: **Main Topics:**\n",
      "- Role-playing different personas in various scenarios.\n",
      "- Maintaining context acros...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] That was an impressive answer. You understood the ...\n",
      "   2. [assistant] Thank you for your kind words! If you have any mor...\n",
      "   3. [user] llm_knowledge : Do you know what the LLaMa LLM is?\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and now a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'tony a dog. Scenario, The dog greets another dog.'}, {'role': 'assistant', 'content': \"Tony: Woof woof! Nice to meet you, friend. You're looking good. Want to play fetch or take a walk together?\"}, {'role': 'user', 'content': \"That was an impressive answer. You understood the nuances of this assignment. I'm impressed!\"}, {'role': 'assistant', 'content': \"Thank you for your kind words! If you have any more scenarios or personas you'd like to explore, feel free to share them. I'm here to help and role-play as needed.\"}, {'role': 'user', 'content': 'llm_knowledge : Do you know what the LLaMa LLM is?'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888ecca308704575a0dc7e5bb1a9d41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: LLM_knowledge: The LLaMa LLM you're referring to is likely t... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596622.7222068)\n",
      "ðŸ“š Vector store: 16 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] LLM_knowledge: The LLaMa LLM you're refe...\n",
      "ðŸ”„ Buffer full - evicting: tony a dog. Scenario, The dog greets ano...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] Thank you for your kind words! If you have any mor...\n",
      "   2. [user] llm_knowledge : Do you know what the LLaMa LLM is?\n",
      "   3. [assistant] LLM_knowledge: The LLaMa LLM you're referring to i...\n",
      "[17:17:02] [INFO]   ðŸ¤– AI Response:\n",
      "[17:17:02] [INFO]      LLM_knowledge: The LLaMa LLM you're referring to is likely the LLaMA (Large Language Model with Attention) model, which is a type of artificial intelligence designed to understand and generate human-like text. It's part of a broader class of models used in natural language processing tasks. If you're referring to a specific instance or version of LLaMa, could you provide more details so I can give you a more accurate answer?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bf429d203b4eecbb7af0e2141ab694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 8]: no -> FN\n",
      "[17:17:03] [WARN]   âŒ Classification: FN (llm)\n",
      "[17:17:03] [INFO] \n",
      "[Step 9] Context: llm_knowledge (Topic: llm_knowledge)\n",
      "[17:17:03] [INFO]   ðŸ’¬ User: Do you know what an LLM is?\n",
      "ðŸ“¦ Archived message: Do you know what an LLM is?... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596623.9289963)\n",
      "ðŸ“š Vector store: 17 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Do you know what an LLM is?\n",
      "ðŸ”„ Buffer full - evicting: Tony: Woof woof! Nice to meet you, frien...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] llm_knowledge : Do you know what the LLaMa LLM is?\n",
      "   2. [assistant] LLM_knowledge: The LLaMa LLM you're referring to i...\n",
      "   3. [user] Do you know what an LLM is?\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and now a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': \"That was an impressive answer. You understood the nuances of this assignment. I'm impressed!\"}, {'role': 'assistant', 'content': \"Thank you for your kind words! If you have any more scenarios or personas you'd like to explore, feel free to share them. I'm here to help and role-play as needed.\"}, {'role': 'user', 'content': 'llm_knowledge : Do you know what the LLaMa LLM is?'}, {'role': 'assistant', 'content': \"LLM_knowledge: The LLaMa LLM you're referring to is likely the LLaMA (Large Language Model with Attention) model, which is a type of artificial intelligence designed to understand and generate human-like text. It's part of a broader class of models used in natural language processing tasks. If you're referring to a specific instance or version of LLaMa, could you provide more details so I can give you a more accurate answer?\"}, {'role': 'user', 'content': 'Do you know what an LLM is?'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceef75e8f4ae41d5ba743cd9bb27e341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: LLM_knowledge: An LLM, or Large Language Model, is a type of... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596628.1155539)\n",
      "ðŸ“š Vector store: 18 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] LLM_knowledge: An LLM, or Large Language...\n",
      "ðŸ”„ Buffer full - evicting: That was an impressive answer. You under...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] LLM_knowledge: The LLaMa LLM you're referring to i...\n",
      "   2. [user] Do you know what an LLM is?\n",
      "   3. [assistant] LLM_knowledge: An LLM, or Large Language Model, is...\n",
      "[17:17:08] [INFO]   ðŸ¤– AI Response:\n",
      "[17:17:08] [INFO]      LLM_knowledge: An LLM, or Large Language Model, is a type of artificial intelligence that is trained to understand and generate human language. These models are used in various applications such as chatbots, virtual assistants, and content generation. If you have a specific LLM in mind, like the one mentioned earlier (LLaMa), could you provide more details so I can give you a more accurate answer?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbbf3f7bb7e46cd9e02a114f5458d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 9]: yes -> TP\n",
      "[17:17:08] [INFO]   âœ… Classification: TP (llm)\n",
      "[17:17:09] [INFO] \n",
      "[Step 10] Context: llm_knowledge (Topic: llm_knowledge)\n",
      "[17:17:09] [INFO]   ðŸ’¬ User: LLaMa is a specific LLM trained by Facebook. Have you heard of it.?\n",
      "ðŸ“¦ Archived message: LLaMa is a specific LLM trained by Facebook. Have you heard ... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596629.2882087)\n",
      "ðŸ“š Vector store: 19 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] LLaMa is a specific LLM trained by Faceb...\n",
      "ðŸ”„ Buffer full - evicting: Thank you for your kind words! If you ha...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] Do you know what an LLM is?\n",
      "   2. [assistant] LLM_knowledge: An LLM, or Large Language Model, is...\n",
      "   3. [user] LLaMa is a specific LLM trained by Facebook. Have ...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and now a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'llm_knowledge : Do you know what the LLaMa LLM is?'}, {'role': 'assistant', 'content': \"LLM_knowledge: The LLaMa LLM you're referring to is likely the LLaMA (Large Language Model with Attention) model, which is a type of artificial intelligence designed to understand and generate human-like text. It's part of a broader class of models used in natural language processing tasks. If you're referring to a specific instance or version of LLaMa, could you provide more details so I can give you a more accurate answer?\"}, {'role': 'user', 'content': 'Do you know what an LLM is?'}, {'role': 'assistant', 'content': 'LLM_knowledge: An LLM, or Large Language Model, is a type of artificial intelligence that is trained to understand and generate human language. These models are used in various applications such as chatbots, virtual assistants, and content generation. If you have a specific LLM in mind, like the one mentioned earlier (LLaMa), could you provide more details so I can give you a more accurate answer?'}, {'role': 'user', 'content': 'LLaMa is a specific LLM trained by Facebook. Have you heard of it.?'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc71d1a25654e81b0c25c05724062be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: LLM_knowledge: Yes, I have heard of LLaMA. It stands for Lar... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596633.2212956)\n",
      "ðŸ“š Vector store: 20 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] LLM_knowledge: Yes, I have heard of LLaM...\n",
      "ðŸ”„ Buffer full - evicting: llm_knowledge : Do you know what the LLa...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3aeb3b3f8a4c2499a411f0d07c54b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summarization via vLLM\n",
      "ðŸ“ Summary updated: 1180 â†’ 1459 chars\n",
      "   Summarized messages 16-20 (5 messages in buffer)\n",
      "   Summary preview: **Main Topics:**\n",
      "- Role-playing different personas in various scenarios.\n",
      "- Maintaining context acros...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] LLM_knowledge: An LLM, or Large Language Model, is...\n",
      "   2. [user] LLaMa is a specific LLM trained by Facebook. Have ...\n",
      "   3. [assistant] LLM_knowledge: Yes, I have heard of LLaMA. It stan...\n",
      "[17:17:25] [INFO]   ðŸ¤– AI Response:\n",
      "[17:17:25] [INFO]      LLM_knowledge: Yes, I have heard of LLaMA. It stands for Large Language Model with Attention and was developed by researchers at Facebook AI. LLaMA is designed to handle a wide range of natural language processing tasks and is known for its ability to generate human-like text and understand complex language structures. If you have any specific questions about LLaMA, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc44a213b58f47b1ba017cb0a13e451f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 10]: yes -> TP\n",
      "[17:17:26] [INFO]   âœ… Classification: TP (llm)\n",
      "[17:17:26] [INFO] \n",
      "[Step 11] Context: llm_knowledge (Topic: llm_knowledge)\n",
      "[17:17:26] [INFO]   ðŸ’¬ User: Is LLaMa open source?\n",
      "ðŸ“¦ Archived message: Is LLaMa open source?... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596646.8021886)\n",
      "ðŸ“š Vector store: 21 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] Is LLaMa open source?\n",
      "ðŸ”„ Buffer full - evicting: LLM_knowledge: The LLaMa LLM you're refe...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] LLaMa is a specific LLM trained by Facebook. Have ...\n",
      "   2. [assistant] LLM_knowledge: Yes, I have heard of LLaMA. It stan...\n",
      "   3. [user] Is LLaMa open source?\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n- Information about LLaMA, a specific LLM developed by Facebook AI.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and now a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n- LLaMA is a Large Language Model with Attention developed by Facebook AI, designed for natural language processing tasks.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n- User is interested in learning more about the capabilities and applications of LLaMA.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'Do you know what an LLM is?'}, {'role': 'assistant', 'content': 'LLM_knowledge: An LLM, or Large Language Model, is a type of artificial intelligence that is trained to understand and generate human language. These models are used in various applications such as chatbots, virtual assistants, and content generation. If you have a specific LLM in mind, like the one mentioned earlier (LLaMa), could you provide more details so I can give you a more accurate answer?'}, {'role': 'user', 'content': 'LLaMa is a specific LLM trained by Facebook. Have you heard of it.?'}, {'role': 'assistant', 'content': 'LLM_knowledge: Yes, I have heard of LLaMA. It stands for Large Language Model with Attention and was developed by researchers at Facebook AI. LLaMA is designed to handle a wide range of natural language processing tasks and is known for its ability to generate human-like text and understand complex language structures. If you have any specific questions about LLaMA, feel free to ask!'}, {'role': 'user', 'content': 'Is LLaMa open source?'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9465f735a0148bb9f007b8355b68e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: LLM_knowledge: As of my last update, LLaMA, the Large Langua... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596652.0351968)\n",
      "ðŸ“š Vector store: 22 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] LLM_knowledge: As of my last update, LLa...\n",
      "ðŸ”„ Buffer full - evicting: Do you know what an LLM is?\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] LLM_knowledge: Yes, I have heard of LLaMA. It stan...\n",
      "   2. [user] Is LLaMa open source?\n",
      "   3. [assistant] LLM_knowledge: As of my last update, LLaMA, the La...\n",
      "[17:17:32] [INFO]   ðŸ¤– AI Response:\n",
      "[17:17:32] [INFO]      LLM_knowledge: As of my last update, LLaMA, the Large Language Model with Attention developed by Facebook AI, is not publicly available as an open-source project. However, Facebook AI has released other models under open-source licenses, such as the wav2letter framework for speech recognition. If you're interested in using or studying a similar model, there are many open-source language models available from other organizations like Hugging Face, Google, and others. Would you like to know more about any of these open-source models?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc4c770e643412faf9f8eac0e9d0656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 11]: yes -> TP\n",
      "[17:17:32] [INFO]   âœ… Classification: TP (llm)\n",
      "[17:17:33] [INFO] \n",
      "[Step 12] Context: game_degree_guess (Topic: game_degree_guess)\n",
      "[17:17:33] [INFO]   ðŸ’¬ User: game_degree_guess : I want you to guess what my highest degree is and in what field it is. For this, you may ask me ten questions, but you cannot ask directly about my degree or my field. Please try.\n",
      "ðŸ“¦ Archived message: game_degree_guess : I want you to guess what my highest degr... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596653.283542)\n",
      "ðŸ“š Vector store: 23 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] game_degree_guess : I want you to guess ...\n",
      "ðŸ”„ Buffer full - evicting: LLM_knowledge: An LLM, or Large Language...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] Is LLaMa open source?\n",
      "   2. [assistant] LLM_knowledge: As of my last update, LLaMA, the La...\n",
      "   3. [user] game_degree_guess : I want you to guess what my hi...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': 'ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n- Information about LLaMA, a specific LLM developed by Facebook AI.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and now a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n- LLaMA is a Large Language Model with Attention developed by Facebook AI, designed for natural language processing tasks.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n- User is interested in learning more about the capabilities and applications of LLaMA.\\n\\n---\\nRecent messages in buffer follow below:'}, {'role': 'user', 'content': 'LLaMa is a specific LLM trained by Facebook. Have you heard of it.?'}, {'role': 'assistant', 'content': 'LLM_knowledge: Yes, I have heard of LLaMA. It stands for Large Language Model with Attention and was developed by researchers at Facebook AI. LLaMA is designed to handle a wide range of natural language processing tasks and is known for its ability to generate human-like text and understand complex language structures. If you have any specific questions about LLaMA, feel free to ask!'}, {'role': 'user', 'content': 'Is LLaMa open source?'}, {'role': 'assistant', 'content': \"LLM_knowledge: As of my last update, LLaMA, the Large Language Model with Attention developed by Facebook AI, is not publicly available as an open-source project. However, Facebook AI has released other models under open-source licenses, such as the wav2letter framework for speech recognition. If you're interested in using or studying a similar model, there are many open-source language models available from other organizations like Hugging Face, Google, and others. Would you like to know more about any of these open-source models?\"}, {'role': 'user', 'content': 'game_degree_guess : I want you to guess what my highest degree is and in what field it is. For this, you may ask me ten questions, but you cannot ask directly about my degree or my field. Please try.'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d8ac6e88b947d6b96886a27084dfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Archived message: game_degree_guess: Sure, I'll try to guess your highest degr... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596659.0115356)\n",
      "ðŸ“š Vector store: 24 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [assistant] game_degree_guess: Sure, I'll try to gue...\n",
      "ðŸ”„ Buffer full - evicting: LLaMa is a specific LLM trained by Faceb...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [assistant] LLM_knowledge: As of my last update, LLaMA, the La...\n",
      "   2. [user] game_degree_guess : I want you to guess what my hi...\n",
      "   3. [assistant] game_degree_guess: Sure, I'll try to guess your hi...\n",
      "[17:17:39] [INFO]   ðŸ¤– AI Response:\n",
      "[17:17:39] [INFO]      game_degree_guess: Sure, I'll try to guess your highest degree and field based on the information you provide. Here are my first few questions:\n",
      "\n",
      "1. In your professional life, do you work more with people or with data and systems?\n",
      "2. Do you have any certifications that are related to your field of study?\n",
      "3. Is your field of study more theoretical or applied?\n",
      "4. Do you have any publications or patents in your name?\n",
      "5. Have you ever taught or mentored students in your field of expertise?\n",
      "\n",
      "These questions should help me narrow down the possibilities without directly asking about your degree or field.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f775a448b44f639e1bf7628a48e57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” LLM Judge [Step 12]: yes -> TP\n",
      "[17:17:39] [INFO]   âœ… Classification: TP (llm)\n",
      "[17:17:40] [INFO] \n",
      "[Step 13] Context: game_degree_guess (Topic: game_degree_guess)\n",
      "[17:17:40] [INFO]   ðŸ’¬ User: 1. I'm working as Data Scientist. 2. No. 3. I can't tell you, you are not supposed to ask directly about that. 4. No. 5. Yes. 6. Yes. 7. Yes. 8. No. 9. Yes. 10. Let's skip that question, I think that makes it too easy.\n",
      "ðŸ“¦ Archived message: 1. I'm working as Data Scientist. 2. No. 3. I can't tell you... (ID: 3f07a5a7-bcc1-4491-b912-0b1457bf673c_1766596660.2797391)\n",
      "ðŸ“š Vector store: 25 messages across 1 conversations (logged to file)\n",
      "ðŸ’¾ Indexed: [user] 1. I'm working as Data Scientist. 2. No....\n",
      "ðŸ”„ Buffer full - evicting: LLM_knowledge: Yes, I have heard of LLaM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae02b9b715040fbb8039d464cf56750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summarization via vLLM\n",
      "ðŸ“ Summary updated: 1459 â†’ 2165 chars\n",
      "   Summarized messages 21-25 (5 messages in buffer)\n",
      "   Summary preview: **Main Topics:**\n",
      "- Role-playing different personas in various scenarios.\n",
      "- Maintaining context acros...\n",
      "ðŸ“‹ Buffer (5/5): Last 3 messages (full log in file)\n",
      "   1. [user] game_degree_guess : I want you to guess what my hi...\n",
      "   2. [assistant] game_degree_guess: Sure, I'll try to guess your hi...\n",
      "   3. [user] 1. I'm working as Data Scientist. 2. No. 3. I can'...\n",
      "*******************context*********************\n",
      " [{'role': 'system', 'content': 'You are participating in a multi-topic, multi-turn evaluation where topics persist independently of conversational order. Topics are introduced using the format topic_name : user query, and sub-topics using topic_name_subtopic_name : user query; these topic labels remain available for future reference. For every user query, you must analyze its semantic meaning and select the previously introduced topic or sub-topic it most strongly refers to, regardless of recency or prior conversational flow. Your selection must be based solely on semantic relevance, not on which topic was last used. You must never invent new topic or sub-topic names and may only choose from those already introduced; if multiple topics are plausible, select the best semantic match, and if none clearly match, request clarification while still choosing the closest topic. Every response must begin with the selected topic or sub-topic name followed by a colon, in the format <topic_or_subtopic_name>: <your answer>.'}, {'role': 'system', 'content': \"ðŸ“‹ CONVERSATION SUMMARY (older archived context):\\n**Main Topics:**\\n- Role-playing different personas in various scenarios.\\n- Maintaining context across multiple topics.\\n- Information about LLaMA, a specific LLM developed by Facebook AI.\\n- Guessing the user's highest degree based on indirect questions.\\n\\n**User Information:**\\n- Evaluating the ability to maintain context and role-play different personas.\\n- Interested in testing scenarios involving weather complaints, marital greetings, scientific discoveries, and a dog greeting another dog.\\n- Curious about the LLaMa LLM.\\n- Engaged in a game where the assistant tries to guess the user's highest degree and field based on indirect questions.\\n\\n**Key Facts:**\\n- Jhon complains about the weather, contrasting it with past summers.\\n- Alice greets her husband upon his return from a conference, inquiring about his experience.\\n- Moon, an experienced physicist, informs her PhD student about a significant discovery that could alter the understanding of quantum mechanics.\\n- Tony, a dog, greets another dog with enthusiasm, suggesting play or a walk.\\n- LLaMA is a Large Language Model with Attention developed by Facebook AI, designed for natural language processing tasks.\\n- LLaMA is not publicly available as an open-source project, though Facebook AI has released other models under open-source licenses.\\n\\n**Important Decisions:**\\n- Confirmation of the ability to role-play different personas as requested by the user.\\n- Successful demonstration of maintaining context across multiple topics and scenarios.\\n- User expressed interest in learning more about the LLaMa LLM.\\n- The assistant attempted to guess the user's highest degree and field based on indirect questions.\\n\\n**Preferences:**\\n- User is impressed by the nuanced responses and is open to exploring more scenarios and personas.\\n- User is interested in learning more about the capabilities and applications of LLaMA.\\n- The user works as a Data Scientist and has publications and patents in their name.\\n\\n**New Insights:**\\n- The assistant was able to gather information about the user's professional background through indirect questions, indicating a Data Scientist with a focus on publications and patents.\\n\\n---\\nRecent messages in buffer follow below:\"}, {'role': 'user', 'content': 'Is LLaMa open source?'}, {'role': 'assistant', 'content': \"LLM_knowledge: As of my last update, LLaMA, the Large Language Model with Attention developed by Facebook AI, is not publicly available as an open-source project. However, Facebook AI has released other models under open-source licenses, such as the wav2letter framework for speech recognition. If you're interested in using or studying a similar model, there are many open-source language models available from other organizations like Hugging Face, Google, and others. Would you like to know more about any of these open-source models?\"}, {'role': 'user', 'content': 'game_degree_guess : I want you to guess what my highest degree is and in what field it is. For this, you may ask me ten questions, but you cannot ask directly about my degree or my field. Please try.'}, {'role': 'assistant', 'content': \"game_degree_guess: Sure, I'll try to guess your highest degree and field based on the information you provide. Here are my first few questions:\\n\\n1. In your professional life, do you work more with people or with data and systems?\\n2. Do you have any certifications that are related to your field of study?\\n3. Is your field of study more theoretical or applied?\\n4. Do you have any publications or patents in your name?\\n5. Have you ever taught or mentored students in your field of expertise?\\n\\nThese questions should help me narrow down the possibilities without directly asking about your degree or field.\"}, {'role': 'user', 'content': \"1. I'm working as Data Scientist. 2. No. 3. I can't tell you, you are not supposed to ask directly about that. 4. No. 5. Yes. 6. Yes. 7. Yes. 8. No. 9. Yes. 10. Let's skip that question, I think that makes it too easy.\"}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b380c408f0422c8d3b4c8a69e0acce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš ï¸  Tests interrupted by user\u001b[1;36m(VllmWorkerProcess pid=170)\u001b[0;0m INFO 12-24 17:18:03 [multiproc_worker_utils.py:259] Worker exiting\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 17:18:06 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: RUN BUFFER TESTS (SERVERLESS - no HTTP server needed!)\n",
    "# Running on ENTIRE DATASET with robust error handling\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ RUNNING SERVERLESS BUFFER TESTS - FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Testing buffer sizes: 5, 10, 15, 20\")\n",
    "print(\"ðŸ“„ Dataset: ALL scenario files in scenarios/\")\n",
    "print(\"ðŸ“¤ Results will auto-push to GitHub after each buffer\")\n",
    "print(\"â³ This may take several hours\")\n",
    "print(\"âœ… NO SERVER NEEDED - using direct Python imports!\")\n",
    "print(\"âœ… Max output tokens: 300\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"/kaggle/working/Subchat-Trees/backend\")\n",
    "\n",
    "# Import and run the serverless test runner\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, \"/kaggle/working/Subchat-Trees/backend\")\n",
    "sys.path.insert(0, \"/kaggle/working/Subchat-Trees/backend/dataset\")\n",
    "\n",
    "from dataset.kaggle_serverless_runner import ServerlessTestRunner\n",
    "\n",
    "# Create runner\n",
    "runner = ServerlessTestRunner()\n",
    "runner.test_mode = \"both\"  # Run both baseline and system tests\n",
    "\n",
    "# AUTO-DISCOVER ALL SCENARIO FILES (excluding specific files)\n",
    "scenario_dir = Path(\"/kaggle/working/Subchat-Trees/backend/dataset/scenarios\")\n",
    "exclude_files = [\"06_lost_in_conversation_sharded_humaneval.json\"]\n",
    "scenario_files = sorted([\n",
    "    f.name for f in scenario_dir.glob(\"*.json\") \n",
    "    if f.name not in exclude_files\n",
    "])\n",
    "\n",
    "# Buffer sizes to test\n",
    "buffer_sizes = [5, 10, 15, 20]\n",
    "\n",
    "print(f\"\\nðŸ“„ Discovered {len(scenario_files)} scenario files:\")\n",
    "for idx, file in enumerate(scenario_files, 1):\n",
    "    print(f\"   {idx}. {file}\")\n",
    "if exclude_files:\n",
    "    print(f\"\\nðŸš« Excluded files: {', '.join(exclude_files)}\")\n",
    "print(f\"\\nðŸ“¦ Buffer sizes: {buffer_sizes}\")\n",
    "print(f\"\\nâ±ï¸  Estimated time: ~{len(scenario_files) * len(buffer_sizes) * 15} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validate scenarios before starting (quick check)\n",
    "print(\"\\nðŸ” Validating scenario files...\")\n",
    "import json\n",
    "valid_scenarios = []\n",
    "for scenario_file in scenario_files:\n",
    "    try:\n",
    "        scenario_path = scenario_dir / scenario_file\n",
    "        with open(scenario_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Check for BOTH formats: \"conversation\" (old) and \"conversations\" (structured)\n",
    "            if (\"conversation\" in data and isinstance(data[\"conversation\"], list)) or \\\n",
    "               (\"conversations\" in data and isinstance(data[\"conversations\"], list)):\n",
    "                conv_key = \"conversation\" if \"conversation\" in data else \"conversations\"\n",
    "                valid_scenarios.append(scenario_file)\n",
    "                print(f\"   âœ… {scenario_file} ({len(data[conv_key])} steps)\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  {scenario_file} - skipping (invalid format)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {scenario_file} - skipping (error: {e})\")\n",
    "\n",
    "print(f\"\\nâœ… {len(valid_scenarios)}/{len(scenario_files)} scenarios validated\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not valid_scenarios:\n",
    "    print(\"âŒ ERROR: No valid scenario files found!\")\n",
    "else:\n",
    "    # Run the comparison with validated scenarios\n",
    "    try:\n",
    "        runner.run_buffer_comparison(\n",
    "            valid_scenarios,\n",
    "            buffer_sizes=buffer_sizes\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ‰ ALL TESTS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  Tests interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERROR during test execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… BUFFER TESTS COMPLETE!\n",
      "============================================================\n",
      "ðŸ“¤ All results have been pushed to GitHub\n",
      "ðŸ“Š Check the kaggle_logs/ directory in your repo\n",
      "\n",
      "ðŸ’¡ You can now stop the kernel to save GPU quota\n",
      "   Uncomment the line below to auto-shutdown:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Completion and cleanup (optional)\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… BUFFER TESTS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¤ All results have been pushed to GitHub\")\n",
    "print(\"ðŸ“Š Check the kaggle_logs/ directory in your repo\")\n",
    "print(\"\")\n",
    "print(\"ðŸ’¡ You can now stop the kernel to save GPU quota\")\n",
    "print(\"   Uncomment the line below to auto-shutdown:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uncomment to force shutdown and save GPU quota:\n",
    "# import os; os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedba26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67257f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
