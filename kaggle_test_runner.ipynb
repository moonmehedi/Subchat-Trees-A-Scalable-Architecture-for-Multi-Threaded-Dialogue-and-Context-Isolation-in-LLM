{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2be5be",
   "metadata": {},
   "source": [
    "# üöÄ Kaggle Buffer Test Runner - All-in-One\n",
    "\n",
    "This notebook loads vLLM, registers it with the backend, and runs buffer tests.\n",
    "**Run all cells in order - no separate notebook needed!**\n",
    "\n",
    "## What this does:\n",
    "1. ‚úÖ Installs vLLM dependencies\n",
    "2. ‚úÖ Loads Qwen 14B AWQ model on Kaggle GPUs\n",
    "3. ‚úÖ Clones repo and configures git\n",
    "4. ‚úÖ Registers vLLM with backend (for responses, summarization, AND judging)\n",
    "5. ‚úÖ Runs buffer tests (sizes: 5, 10, 20, 40)\n",
    "6. ‚úÖ Auto-pushes results to GitHub after each buffer\n",
    "\n",
    "## Important:\n",
    "- This is a **single notebook** - no need for separate notebooks\n",
    "- vLLM model stays in memory for all operations\n",
    "- No server needed - tests use direct Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e04efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì¶ INSTALLING vLLM DEPENDENCIES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install vLLM dependencies\n",
    "print(\"=\"*60)\n",
    "print(\"üì¶ INSTALLING vLLM DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461976f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö LIBRARIES IMPORTED\n",
      "============================================================\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: True\n",
      "üéÆ GPUs available: 2\n",
      "   GPU 0: Tesla T4\n",
      "   GPU 1: Tesla T4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìö LIBRARIES IMPORTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"üéÆ GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39d315ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS AND CONFIGURATION LOADED\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "‚úÖ VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Kaggle secrets and set environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Load all secrets\n",
    "os.environ[\"GITHUB_TOKEN\"] = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "os.environ[\"GROQ_API_KEY\"] = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set vLLM configuration\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS AND CONFIGURATION LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {os.environ['GITHUB_TOKEN'][:4]}...{os.environ['GITHUB_TOKEN'][-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ LOADING vLLM MODEL\n",
      "============================================================\n",
      "üìÇ Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "üéÆ GPUs: 2\n",
      "‚è≥ This takes 2-3 minutes...\n",
      "============================================================\n",
      "INFO 12-19 13:57:33 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 12-19 13:57:33 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-19 13:57:33 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 13:57:33 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-19 13:57:33 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-19 13:57:33 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 13:57:33 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 13:57:33 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 12-19 13:57:33 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 12-19 13:57:42 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 12-19 13:57:42 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:52 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:52 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:53 [cuda.py:289] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=505)\u001b[0;0m INFO 12-19 13:57:53 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 13:57:54.601612537 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:55.096564257 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:55.096564257 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:56.773504792 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:56.773504792 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:57.162529560 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:57.162529560 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:58.160452221 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:57:58.160452221 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:00.000361425 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:00.000361425 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:03.351290066 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:03.351290066 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:10.214300034 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:10.214300034 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:15.568167316 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:15.568167316 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:23.540068363 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:23.540068363 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:43.253093816 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:58:43.253093816 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:59:17.457031418 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 13:59:17.457031418 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:00:06.125972969 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:00:06.125972969 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:00:41.022913166 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:00:41.022913166 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:01:54.212922331 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:01:54.212922331 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:03:03.926045072 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:03:03.926045072 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:04:03.421109319 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:04:03.421109319 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:05:27.645138374 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:05:27.645138374 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-19 14:05:39 [multiproc_worker_utils.py:259] Worker exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 14:05:39.640909346 TCPStore.cpp:330] [c10d] TCP client failed to connect/validate to host 172.19.2.2:54329 - retrying (try=0, timeout=600000ms, delay=85874ms): Interrupted system call\n",
      "Exception raised from delay at /pytorch/torch/csrc/distributed/c10d/socket.cpp:119 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b062f36c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x1615466 (0x7b0612fda466 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x6350386 (0x7b0617d15386 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x63507f4 (0x7b0617d157f4 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x630d216 (0x7b0617cd2216 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x20c (0x7b0617cd514c in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0xe486ef (0x7b06278c96ef in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)\n",
      "frame #7: <unknown function> + 0x51a017 (0x7b0626f9b017 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)\n",
      "frame #8: /usr/bin/python3() [0x55577b]\n",
      "frame #9: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
      "frame #10: /usr/bin/python3() [0x5854ce]\n",
      "frame #11: _PyObject_Call + 0xea (0x570b2a in /usr/bin/python3)\n",
      "frame #12: /usr/bin/python3() [0x56e459]\n",
      "frame #13: /usr/bin/python3() [0x52fc40]\n",
      "frame #14: <unknown function> + 0x51880b (0x7b0626f9980b in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)\n",
      "frame #15: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
      "frame #16: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
      "frame #17: /usr/bin/python3() [0x5d579d]\n",
      "frame #18: /usr/bin/python3() [0x55c1eb]\n",
      "frame #19: PyObject_Vectorcall + 0x35 (0x54b305 in /usr/bin/python3)\n",
      "frame #20: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
      "frame #21: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
      "frame #22: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
      "frame #23: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
      "frame #24: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
      "frame #25: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
      "frame #26: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
      "frame #27: /usr/bin/python3() [0x585ce7]\n",
      "frame #28: /usr/bin/python3() [0x585520]\n",
      "frame #29: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
      "frame #30: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
      "frame #31: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
      "frame #33: /usr/bin/python3() [0x613cd4]\n",
      "frame #34: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
      "frame #35: /usr/bin/python3() [0x63415b]\n",
      "frame #36: /usr/bin/python3() [0x6303b4]\n",
      "frame #37: PyRun_StringFlags + 0x63 (0x623253 in /usr/bin/python3)\n",
      "frame #38: PyRun_SimpleStringFlags + 0x3f (0x622fef in /usr/bin/python3)\n",
      "frame #39: Py_RunMain + 0x366 (0x63ea86 in /usr/bin/python3)\n",
      "frame #40: Py_BytesMain + 0x2d (0x6045dd in /usr/bin/python3)\n",
      "frame #41: <unknown function> + 0x29d90 (0x7b0630762d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #42: __libc_start_main + 0x80 (0x7b0630762e40 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #43: _start + 0x25 (0x604465 in /usr/bin/python3)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/2876653525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m llm = vllm.LLM(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mquantization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'awq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_worker_tasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAwaitable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     def execute_model(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mall_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         self._run_workers(\"load_model\",\n\u001b[1;32m    126\u001b[0m                           \u001b[0mmax_concurrent_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_run_workers\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Get the results of the workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return [driver_worker_output\n\u001b[0;32m--> 190\u001b[0;31m                 ] + [output.get() for output in worker_outputs]\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_health\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Get the results of the workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return [driver_worker_output\n\u001b[0;32m--> 190\u001b[0;31m                 ] + [output.get() for output in worker_outputs]\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_health\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/multiproc_worker_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 12-19 14:05:39 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 505 died, exit code: -15\n",
      "INFO 12-19 14:05:39 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n",
      "INFO 12-19 14:05:39 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n",
      "ERROR 12-19 14:05:40 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 163 died, exit code: -15\n",
      "ERROR 12-19 14:05:40 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 163 died, exit code: -15\n",
      "INFO 12-19 14:05:40 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n",
      "INFO 12-19 14:05:40 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load vLLM model on Kaggle GPUs (takes 2-3 minutes)\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ LOADING vLLM MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Model: {model_path}\")\n",
    "print(f\"üéÆ GPUs: {torch.cuda.device_count()}\")\n",
    "print(\"‚è≥ This takes 2-3 minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ vLLM MODEL LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Memory per GPU: ~{torch.cuda.get_device_properties(0).total_memory / 1024**3 * 0.91:.1f}GB used\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• CLONING REPOSITORY\n",
      "============================================================\n",
      "‚úÖ Cloned kaggle-run branch!\n",
      "‚úÖ Pulled scenario files from Git LFS\n",
      "‚úÖ Git identity configured\n",
      "============================================================\n",
      "‚úÖ Cloned kaggle-run branch!\n",
      "‚úÖ Pulled scenario files from Git LFS\n",
      "‚úÖ Git identity configured\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Clone repository and configure git\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone with LFS skip to save bandwidth\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Cloned {BRANCH} branch!\")\n",
    "    \n",
    "    # Pull LFS files for scenarios\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"‚úÖ Pulled scenario files from Git LFS\")\n",
    "    \n",
    "    # Configure git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"moonmehedi\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"the.mehedi.hasan.moon@gmail.com\"], check=True)\n",
    "    print(\"‚úÖ Git identity configured\")\n",
    "    \n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîó REGISTERING vLLM WITH BACKEND\n",
      "============================================================\n",
      "‚úÖ vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM registered: True\n",
      "   ‚úÖ Response generation will use vLLM\n",
      "   ‚úÖ Summarization will use vLLM\n",
      "   ‚úÖ Judge/Classification will use vLLM\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Register vLLM with backend\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó REGISTERING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(f\"‚úÖ vLLM registered: {VLLMClient.is_available()}\")\n",
    "print(\"   ‚úÖ Response generation will use vLLM\")\n",
    "print(\"   ‚úÖ Summarization will use vLLM\")\n",
    "print(\"   ‚úÖ Judge/Classification will use vLLM\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b6c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì¶ INSTALLING BACKEND REQUIREMENTS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backend requirements installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Install backend requirements\n",
    "print(\"=\"*60)\n",
    "print(\"üì¶ INSTALLING BACKEND REQUIREMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! pip install -q -r /kaggle/working/Subchat-Trees/backend/requirements.txt\n",
    "\n",
    "print(\"‚úÖ Backend requirements installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f671a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ QUICK INTEGRATION TEST\n",
      "============================================================\n",
      "‚úÖ vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "‚úÖ vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "üìä Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "üìã Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Hello\n",
      "*******************context*********************\n",
      " [{'role': 'user', 'content': 'Hello'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9d557297174caca9106639d5a6bd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test response: Hello! How can I assist you today?...\n",
      "‚úÖ vLLM integration working!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Quick test to verify vLLM integration works\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ QUICK INTEGRATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm_client = SimpleLLMClient()\n",
    "root = TreeNode(node_id=\"test\", title=\"Test\", buffer_size=5, llm_client=llm_client)\n",
    "root.buffer.add_message(\"user\", \"Hello\")\n",
    "\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "print(f\"‚úÖ Test response: {response[:100]}...\")\n",
    "print(\"‚úÖ vLLM integration working!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ RUNNING BUFFER TESTS\n",
      "============================================================\n",
      "üìä Testing buffer sizes: 5, 10, 20, 40\n",
      "üì§ Results will auto-push to GitHub after each buffer\n",
      "‚è≥ This may take several hours depending on scenario count\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/1366112426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Run the test script in the SAME process (so it can access the loaded vLLM model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/kaggle_buffer_test_runner.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 9: RUN BUFFER TESTS\n",
    "# Using exec() to run in the SAME process so it can access the loaded vLLM model\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ RUNNING BUFFER TESTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä Testing buffer sizes: 5, 10, 20, 40\")\n",
    "print(\"üì§ Results will auto-push to GitHub after each buffer\")\n",
    "print(\"‚è≥ This may take several hours depending on scenario count\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"/kaggle/working/Subchat-Trees/backend\")\n",
    "\n",
    "# Run the test script in the SAME process (so it can access the loaded vLLM model)\n",
    "exec(open(\"dataset/kaggle_buffer_test_runner.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Completion and cleanup (optional)\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ BUFFER TESTS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üì§ All results have been pushed to GitHub\")\n",
    "print(\"üìä Check the kaggle_logs/ directory in your repo\")\n",
    "print(\"\")\n",
    "print(\"üí° You can now stop the kernel to save GPU quota\")\n",
    "print(\"   Uncomment the line below to auto-shutdown:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uncomment to force shutdown and save GPU quota:\n",
    "# import os; os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
