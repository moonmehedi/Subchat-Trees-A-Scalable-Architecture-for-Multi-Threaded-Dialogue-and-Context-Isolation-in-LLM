{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2be5be",
   "metadata": {},
   "source": [
    "# ðŸš€ Kaggle Buffer Test Runner - All-in-One\n",
    "\n",
    "This notebook loads vLLM, registers it with the backend, and runs buffer tests.\n",
    "**Run all cells in order - no separate notebook needed!**\n",
    "\n",
    "## What this does:\n",
    "1. âœ… Installs vLLM dependencies\n",
    "2. âœ… Loads Qwen 14B AWQ model on Kaggle GPUs\n",
    "3. âœ… Clones repo and configures git\n",
    "4. âœ… Registers vLLM with backend (for responses, summarization, AND judging)\n",
    "5. âœ… Runs buffer tests (sizes: 5, 10, 20, 40)\n",
    "6. âœ… Auto-pushes results to GitHub after each buffer\n",
    "\n",
    "## Important:\n",
    "- This is a **single notebook** - no need for separate notebooks\n",
    "- vLLM model stays in memory for all operations\n",
    "- No server needed - tests use direct Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e04efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¦ INSTALLING vLLM DEPENDENCIES\n",
      "============================================================\n",
      "âœ… Dependencies installed\n",
      "============================================================\n",
      "âœ… Dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install vLLM dependencies\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¦ INSTALLING vLLM DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system 'vllm' 'triton==3.2.0' 'logits-processor-zoo' 'numpy<2'\n",
    "\n",
    "print(\"âœ… Dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461976f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 14:50:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "============================================================\n",
      "ðŸ“š LIBRARIES IMPORTED\n",
      "============================================================\n",
      "âœ… PyTorch version: 2.6.0+cu124\n",
      "âœ… CUDA available: True\n",
      "ðŸŽ® GPUs available: 2\n",
      "   GPU 0: Tesla T4\n",
      "   GPU 1: Tesla T4\n",
      "============================================================\n",
      "============================================================\n",
      "ðŸ“š LIBRARIES IMPORTED\n",
      "============================================================\n",
      "âœ… PyTorch version: 2.6.0+cu124\n",
      "âœ… CUDA available: True\n",
      "ðŸŽ® GPUs available: 2\n",
      "   GPU 0: Tesla T4\n",
      "   GPU 1: Tesla T4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“š LIBRARIES IMPORTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"ðŸŽ® GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d315ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” SECRETS AND CONFIGURATION LOADED\n",
      "============================================================\n",
      "âœ… GITHUB_TOKEN: gith...tWfg\n",
      "âœ… LLM_BACKEND: vllm\n",
      "âœ… VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Kaggle secrets and set environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Load all secrets\n",
    "os.environ[\"GITHUB_TOKEN\"] = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "os.environ[\"GROQ_API_KEY\"] = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set vLLM configuration\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ” SECRETS AND CONFIGURATION LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… GITHUB_TOKEN: {os.environ['GITHUB_TOKEN'][:4]}...{os.environ['GITHUB_TOKEN'][-4:]}\")\n",
    "print(f\"âœ… LLM_BACKEND: vllm\")\n",
    "print(f\"âœ… VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544a67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ LOADING vLLM MODEL\n",
      "============================================================\n",
      "ðŸ“‚ Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "ðŸŽ® GPUs: 2\n",
      "â³ This takes 2-3 minutes...\n",
      "============================================================\n",
      "INFO 12-19 14:51:05 [config.py:717] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 12-19 14:51:05 [config.py:717] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 12-19 14:51:06 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 12-19 14:51:06 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-19 14:51:06 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 14:51:06 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 14:51:06 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 14:51:06 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 14:51:06 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 12-19 14:51:06 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 12-19 14:51:07 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-19 14:51:07 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-19 14:51:07 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-19 14:51:07 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "WARNING 12-19 14:51:07 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-19 14:51:07 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-19 14:51:07 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-19 14:51:07 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-19 14:51:12 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 12-19 14:51:12 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:23 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:23 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:24 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:24 [cuda.py:289] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:24 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:24 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 14:51:25.848256715 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:51:25.849197133 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 14:51:25 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:25 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:25 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-19 14:51:25 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:25 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:25 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-19 14:51:25 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 14:51:25.137557381 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 14:51:25.138249470 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 14:51:26 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 14:51:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 14:51:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8b4f3683'), local_subscribe_addr='ipc:///tmp/15ebcb6d-4d94-4cab-89d8-2a34d40de54d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-19 14:51:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 14:51:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8b4f3683'), local_subscribe_addr='ipc:///tmp/15ebcb6d-4d94-4cab-89d8-2a34d40de54d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-19 14:51:51 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-19 14:51:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "INFO 12-19 14:51:51 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-19 14:51:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:51:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ca954b60144c4b9c277f6262ad653c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:15 [loader.py:458] Loading weights took 83.24 seconds\n",
      "INFO 12-19 14:53:15 [loader.py:458] Loading weights took 83.36 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:15 [model_runner.py:1140] Model loading took 4.6720 GiB and 83.537182 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:15 [model_runner.py:1140] Model loading took 4.6720 GiB and 83.537182 seconds\n",
      "INFO 12-19 14:53:15 [model_runner.py:1140] Model loading took 4.6720 GiB and 83.658558 seconds\n",
      "INFO 12-19 14:53:15 [model_runner.py:1140] Model loading took 4.6720 GiB and 83.658558 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] Memory profiling takes 10.56 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-19 14:53:26 [worker.py:287] Memory profiling takes 10.73 seconds\n",
      "INFO 12-19 14:53:26 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-19 14:53:26 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] Memory profiling takes 10.56 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=166)\u001b[0;0m INFO 12-19 14:53:26 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 8.15GiB.\n",
      "INFO 12-19 14:53:26 [worker.py:287] Memory profiling takes 10.73 seconds\n",
      "INFO 12-19 14:53:26 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.91) = 13.41GiB\n",
      "INFO 12-19 14:53:26 [worker.py:287] model weights take 4.67GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 7.18GiB.\n",
      "INFO 12-19 14:53:27 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-19 14:53:27 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-19 14:53:27 [executor_base.py:112] # cuda blocks: 4904, # CPU blocks: 2730\n",
      "INFO 12-19 14:53:27 [executor_base.py:117] Maximum concurrency for 5120 tokens per request: 15.32x\n",
      "INFO 12-19 14:53:31 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.92 seconds\n",
      "INFO 12-19 14:53:31 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 15.92 seconds\n",
      "\n",
      "============================================================\n",
      "âœ… vLLM MODEL LOADED SUCCESSFULLY!\n",
      "============================================================\n",
      "   Memory per GPU: ~13.4GB used\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "âœ… vLLM MODEL LOADED SUCCESSFULLY!\n",
      "============================================================\n",
      "   Memory per GPU: ~13.4GB used\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load vLLM model on Kaggle GPUs (takes 2-3 minutes)\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ LOADING vLLM MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“‚ Model: {model_path}\")\n",
    "print(f\"ðŸŽ® GPUs: {torch.cuda.device_count()}\")\n",
    "print(\"â³ This takes 2-3 minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… vLLM MODEL LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Memory per GPU: ~{torch.cuda.get_device_properties(0).total_memory / 1024**3 * 0.91:.1f}GB used\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6237d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¥ CLONING REPOSITORY\n",
      "============================================================\n",
      "âœ… Cloned kaggle-run branch!\n",
      "âœ… Pulled scenario files from Git LFS\n",
      "âœ… Git identity configured\n",
      "============================================================\n",
      "âœ… Cloned kaggle-run branch!\n",
      "âœ… Pulled scenario files from Git LFS\n",
      "âœ… Git identity configured\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Clone repository and configure git\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¥ CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"âš ï¸  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone with LFS skip to save bandwidth\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ… Cloned {BRANCH} branch!\")\n",
    "    \n",
    "    # Pull LFS files for scenarios\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"âœ… Pulled scenario files from Git LFS\")\n",
    "    \n",
    "    # Configure git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"moonmehedi\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"the.mehedi.hasan.moon@gmail.com\"], check=True)\n",
    "    print(\"âœ… Git identity configured\")\n",
    "    \n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(f\"âŒ Clone failed: {result.stderr}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069d9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”— REGISTERING vLLM WITH BACKEND\n",
      "============================================================\n",
      "âœ… vLLM model registered: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "âœ… vLLM registered: True\n",
      "   âœ… Response generation will use vLLM\n",
      "   âœ… Summarization will use vLLM\n",
      "   âœ… Judge/Classification will use vLLM\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Register vLLM with backend\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”— REGISTERING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(f\"âœ… vLLM registered: {VLLMClient.is_available()}\")\n",
    "print(\"   âœ… Response generation will use vLLM\")\n",
    "print(\"   âœ… Summarization will use vLLM\")\n",
    "print(\"   âœ… Judge/Classification will use vLLM\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5b6c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¦ INSTALLING BACKEND REQUIREMENTS\n",
      "============================================================\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/4.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/21.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/17.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires aiofiles<25.0,>=22.0, but you have aiofiles 25.1.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… Backend requirements installed\n",
      "============================================================\n",
      "âœ… Backend requirements installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Install backend requirements\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¦ INSTALLING BACKEND REQUIREMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "! pip install -q -r /kaggle/working/Subchat-Trees/backend/requirements.txt\n",
    "\n",
    "print(\"âœ… Backend requirements installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f671a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using vLLM backend with Kaggle GPU: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n",
      "ðŸ§ª QUICK INTEGRATION TEST\n",
      "============================================================\n",
      "âœ… vLLM connected for RESPONSES: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "âœ… vLLM will be used for SUMMARIZATION: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "ðŸ“Š Buffer size: 5 messages | Summarization will trigger every 5 messages\n",
      "ðŸ“‹ Buffer (1/5): Last 3 messages (full log in file)\n",
      "   1. [user] Hello\n",
      "*******************context*********************\n",
      " [{'role': 'user', 'content': 'Hello'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25675ab7be6b4564847599c12174aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test response: Hello! How can I assist you today?...\n",
      "âœ… vLLM integration working!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Quick test to verify vLLM integration works\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª QUICK INTEGRATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm_client = SimpleLLMClient()\n",
    "root = TreeNode(node_id=\"test\", title=\"Test\", buffer_size=5, llm_client=llm_client)\n",
    "root.buffer.add_message(\"user\", \"Hello\")\n",
    "\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "print(f\"âœ… Test response: {response[:100]}...\")\n",
    "print(\"âœ… vLLM integration working!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e5edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ RUNNING BUFFER TESTS\n",
      "============================================================\n",
      "ðŸ“Š Testing buffer sizes: 5, 10, 20, 40\n",
      "ðŸ“¤ Results will auto-push to GitHub after each buffer\n",
      "â³ This may take several hours depending on scenario count\n",
      "============================================================\n",
      "âœ… ContextClassifier using vLLM for JUDGING/CLASSIFICATION\n",
      "[14:54:07] [INFO] ================================================================================\n",
      "[14:54:07] [INFO] ðŸš€ STARTING KAGGLE MULTI-BUFFER COMPARISON\n",
      "[14:54:07] [INFO]    Buffer sizes: [5, 10, 20, 40]\n",
      "[14:54:07] [INFO]    Git branch: kaggle-run\n",
      "[14:54:07] [INFO] ================================================================================\n",
      "[14:54:07] [INFO] \n",
      "================================================================================\n",
      "[14:54:07] [INFO] ðŸ“¦ TESTING BUFFER SIZE: 5\n",
      "[14:54:07] [INFO] ================================================================================\n",
      "[14:54:07] [INFO] ================================================================================\n",
      "[14:54:07] [INFO] ðŸš€ STARTING METRICS-BASED EVALUATION (buffer_size=5)\n",
      "[14:54:07] [INFO] ================================================================================\n",
      "[14:54:07] [INFO] â³ Checking if server is ready...\n",
      "[14:54:17] [ERROR] âŒ Server not responding!\n",
      "[14:54:17] [ERROR] âŒ Server is not running. Please start it first.\n",
      "[14:54:17] [INFO] \n",
      "================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ PUSHING BUFFER 5 RESULTS TO GITHUB\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] \n",
      "================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ PUSHING RESULTS FOR BUFFER SIZE 5\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ GIT PUSH: Starting commit and push\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/TABLE_1_CONTEXT_ISOLATION.md\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/TABLE_3_SYSTEM_PERFORMANCE.md\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/raw_metrics.json\n",
      "[14:54:17] [ERROR] âŒ Push failed: Git add failed for backend/dataset/logs/metrics_tests/buffer_5/baseline_test.log: The following paths are ignored by one of your .gitignore files:\n",
      "backend/dataset/logs/metrics_tests/buffer_5/baseline_test.log\n",
      "hint: Use -f if you really want to add them.\n",
      "hint: Turn this message off by running\n",
      "hint: \"git config advice.addIgnoredFile false\"\n",
      "\n",
      "[14:54:17] [ERROR] \n",
      "================================================================================\n",
      "[14:54:17] [ERROR] âŒ CRITICAL ERROR: Git push failed!\n",
      "[14:54:17] [ERROR]    Stopping test execution as per requirements\n",
      "[14:54:17] [ERROR]    Already completed buffers have been pushed\n",
      "[14:54:17] [ERROR]    Failed at buffer size: 5\n",
      "[14:54:17] [ERROR] ================================================================================\n",
      "[14:54:17] [ERROR] âŒ Server not responding!\n",
      "[14:54:17] [ERROR] âŒ Server is not running. Please start it first.\n",
      "[14:54:17] [INFO] \n",
      "================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ PUSHING BUFFER 5 RESULTS TO GITHUB\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] \n",
      "================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ PUSHING RESULTS FOR BUFFER SIZE 5\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO] ðŸ“¤ GIT PUSH: Starting commit and push\n",
      "[14:54:17] [INFO] ================================================================================\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/TABLE_1_CONTEXT_ISOLATION.md\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/TABLE_3_SYSTEM_PERFORMANCE.md\n",
      "[14:54:17] [INFO]   âœ… Added: backend/dataset/logs/metrics_tests/tables/buffer_5/raw_metrics.json\n",
      "[14:54:17] [ERROR] âŒ Push failed: Git add failed for backend/dataset/logs/metrics_tests/buffer_5/baseline_test.log: The following paths are ignored by one of your .gitignore files:\n",
      "backend/dataset/logs/metrics_tests/buffer_5/baseline_test.log\n",
      "hint: Use -f if you really want to add them.\n",
      "hint: Turn this message off by running\n",
      "hint: \"git config advice.addIgnoredFile false\"\n",
      "\n",
      "[14:54:17] [ERROR] \n",
      "================================================================================\n",
      "[14:54:17] [ERROR] âŒ CRITICAL ERROR: Git push failed!\n",
      "[14:54:17] [ERROR]    Stopping test execution as per requirements\n",
      "[14:54:17] [ERROR]    Already completed buffers have been pushed\n",
      "[14:54:17] [ERROR]    Failed at buffer size: 5\n",
      "[14:54:17] [ERROR] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: RUN BUFFER TESTS\n",
    "# Using exec() to run in the SAME process so it can access the loaded vLLM model\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ RUNNING BUFFER TESTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Testing buffer sizes: 5, 10, 20, 40\")\n",
    "print(\"ðŸ“¤ Results will auto-push to GitHub after each buffer\")\n",
    "print(\"â³ This may take several hours depending on scenario count\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"/kaggle/working/Subchat-Trees/backend\")\n",
    "\n",
    "# Run the test script in the SAME process (so it can access the loaded vLLM model)\n",
    "exec(open(\"dataset/kaggle_buffer_test_runner.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c372ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… BUFFER TESTS COMPLETE!\n",
      "============================================================\n",
      "ðŸ“¤ All results have been pushed to GitHub\n",
      "ðŸ“Š Check the kaggle_logs/ directory in your repo\n",
      "\n",
      "ðŸ’¡ You can now stop the kernel to save GPU quota\n",
      "   Uncomment the line below to auto-shutdown:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Completion and cleanup (optional)\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… BUFFER TESTS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¤ All results have been pushed to GitHub\")\n",
    "print(\"ðŸ“Š Check the kaggle_logs/ directory in your repo\")\n",
    "print(\"\")\n",
    "print(\"ðŸ’¡ You can now stop the kernel to save GPU quota\")\n",
    "print(\"   Uncomment the line below to auto-shutdown:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uncomment to force shutdown and save GPU quota:\n",
    "# import os; os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
