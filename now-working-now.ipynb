{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hierarchical Subchat System - Kaggle GPU Testing\n",
    "\n",
    "## üìã Setup Checklist (Do Once):\n",
    "\n",
    "### 1Ô∏è‚É£ **Add Kaggle Secrets** (Most Important!)\n",
    "Go to: **https://www.kaggle.com/settings** ‚Üí Add-ons ‚Üí Secrets\n",
    "\n",
    "Add these two secrets:\n",
    "- **`GROQ_API_KEY`** = Your Groq API key (for query decomposition)\n",
    "- **`GITHUB_TOKEN`** = Your GitHub personal access token (for pushing results)\n",
    "\n",
    "### 2Ô∏è‚É£ **Enable Internet in This Notebook**\n",
    "- Click \"‚öôÔ∏è Settings\" (top right)\n",
    "- Turn ON **\"Internet\"** toggle\n",
    "- Click \"Save\"\n",
    "\n",
    "### 3Ô∏è‚É£ **Make Sure This Notebook is PRIVATE**\n",
    "- Never share secrets in public notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è Run Order:\n",
    "1. **Cells 2-7**: Load secrets and libraries\n",
    "2. **Cell 8**: Set environment variables (LLM_BACKEND=vllm)\n",
    "3. **Cell 9**: Load vLLM model on Kaggle GPUs (Qwen-3 14B AWQ) - **Takes 2-3 minutes**\n",
    "4. **Cells 15-17**: Clone repo, configure git, run test log push\n",
    "5. **Cell 21**: Integrate vLLM with backend (registers model globally)\n",
    "6. **Cell 23**: Test vLLM integration\n",
    "7. **Run your actual tests**: Execute test scripts in backend/dataset/\n",
    "8. **Push results**: Use git_commit_and_push() function to sync to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Loads vLLM model (Qwen-3 14B) on Kaggle's 2x GPUs with AWQ quantization\n",
    "- ‚úÖ Integrates vLLM with your backend via `VLLMClient` singleton\n",
    "- ‚úÖ Runs hierarchical subchat tests using GPU-accelerated inference\n",
    "- ‚úÖ Generates performance logs for buffer size analysis\n",
    "- ‚úÖ Automatically syncs results back to your GitHub repo\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Technical Details:\n",
    "- **vLLM Config**: Tensor parallelism across both GPUs, 91% memory utilization\n",
    "- **Backend Mode**: `LLM_BACKEND=vllm` (set in cell 8)\n",
    "- **Model**: Qwen-3 14B AWQ (5120 max tokens, prefix caching enabled)\n",
    "- **No .env needed**: Secrets loaded from Kaggle environment\n",
    "- **Git Workflow**: Clone ‚Üí Test ‚Üí Push results to `kaggle-run` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/qwen-3/transformers/32b-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/model-00003-of-00004.safetensors\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/config.json\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/merges.txt\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/LICENSE\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/model-00001-of-00004.safetensors\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/README.md\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/vocab.json\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/model-00004-of-00004.safetensors\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/model-00002-of-00004.safetensors\n",
      "/kaggle/input/qwen-3/transformers/32b-awq/1/generation_config.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/config.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/merges.txt\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/LICENSE\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/README.md\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/vocab.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3/transformers/14b-awq/1/generation_config.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/config.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/merges.txt\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/LICENSE\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model-00005-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model-00001-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/README.md\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model-00002-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/tokenizer.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/vocab.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/tokenizer_config.json\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model-00004-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/model-00003-of-00005.safetensors\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/.gitattributes\n",
      "/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING OUT MY GPU WORKINGW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** repo: https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:41.966872Z",
     "iopub.status.busy": "2025-12-13T13:14:41.966627Z",
     "iopub.status.idle": "2025-12-13T13:14:44.519520Z",
     "shell.execute_reply": "2025-12-13T13:14:44.518617Z",
     "shell.execute_reply.started": "2025-12-13T13:14:41.966849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall -q --system 'tensorflow'\n",
    "! uv pip install -q --system  'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:14:44.521819Z",
     "iopub.status.busy": "2025-12-13T13:14:44.521553Z",
     "iopub.status.idle": "2025-12-13T13:14:48.187393Z",
     "shell.execute_reply": "2025-12-13T13:14:48.186532Z",
     "shell.execute_reply.started": "2025-12-13T13:14:44.521795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.187944Z",
     "iopub.status.idle": "2025-12-13T13:14:48.188268Z",
     "shell.execute_reply": "2025-12-13T13:14:48.188112Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.188098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-13 17:22:16 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'half', 'seed': None, 'max_model_len': 5120, 'tensor_parallel_size': 2, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.91, 'disable_log_stats': True, 'quantization': 'awq', 'enforce_eager': True, 'model': '/kaggle/input/qwen-3/transformers/32b-awq/1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-13 17:22:16 [model.py:637] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 12-13 17:22:16 [model.py:1750] Using max model len 5120\n",
      "INFO 12-13 17:22:16 [model.py:1750] Using max model len 5120\n",
      "INFO 12-13 17:22:16 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-13 17:22:16 [vllm.py:601] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 12-13 17:22:16 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "INFO 12-13 17:22:16 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-13 17:22:16 [vllm.py:601] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 12-13 17:22:16 [vllm.py:707] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m INFO 12-13 17:22:26 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/kaggle/input/qwen-3/transformers/32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/kaggle/input/qwen-3/transformers/32b-awq/1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m WARNING 12-13 17:22:26 [multiproc_executor.py:880] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "ERROR 12-13 17:22:34 [fa_utils.py:72] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 12-13 17:22:34 [fa_utils.py:72] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 12-13 17:22:34 [fa_utils.py:72] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 12-13 17:22:34 [fa_utils.py:72] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1200] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38551 backend=nccl\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1200] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38551 backend=nccl\n",
      "INFO 12-13 17:22:35 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1200] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38551 backend=nccl\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1200] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38551 backend=nccl\n",
      "INFO 12-13 17:22:35 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 12-13 17:22:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 12-13 17:22:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 12-13 17:22:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-13 17:22:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1408] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1408] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 12-13 17:22:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 12-13 17:22:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 12-13 17:22:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-13 17:22:35 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1408] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1\n",
      "INFO 12-13 17:22:35 [parallel_state.py:1408] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:22:35 [gpu_model_runner.py:3467] Starting to load model /kaggle/input/qwen-3/transformers/32b-awq/1...\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:22:35 [gpu_model_runner.py:3467] Starting to load model /kaggle/input/qwen-3/transformers/32b-awq/1...\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:22:36 [cuda.py:411] Using FLASHINFER attention backend out of potential backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(Worker_TP1 pid=1755)\u001b[0;0m INFO 12-13 17:22:36 [cuda.py:411] Using FLASHINFER attention backend out of potential backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:22:36 [cuda.py:411] Using FLASHINFER attention backend out of potential backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(Worker_TP1 pid=1755)\u001b[0;0m INFO 12-13 17:22:36 [cuda.py:411] Using FLASHINFER attention backend out of potential backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:23,  7.85s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:23,  7.85s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:21<00:23, 11.52s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:21<00:23, 11.52s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:36<00:12, 12.82s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:36<00:12, 12.82s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:48<00:00, 12.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:48<00:00, 12.03s/it]\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:48<00:00, 12.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:48<00:00, 12.03s/it]\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:25 [default_loader.py:308] Loading weights took 48.24 seconds\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:25 [gpu_model_runner.py:3549] Model loading took 9.0570 GiB memory and 48.815444 seconds\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:25 [gpu_model_runner.py:3549] Model loading took 9.0570 GiB memory and 48.815444 seconds\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:53 [gpu_worker.py:359] Available KV cache memory: 2.84 GiB\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:53 [gpu_worker.py:359] Available KV cache memory: 2.84 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m INFO 12-13 17:23:54 [kv_cache_utils.py:1286] GPU KV cache size: 23,264 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m INFO 12-13 17:23:54 [kv_cache_utils.py:1291] Maximum concurrency for 5,120 tokens per request: 4.54x\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:54 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "\u001b[0;36m(Worker_TP1 pid=1755)\u001b[0;0m INFO 12-13 17:23:54 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m INFO 12-13 17:23:54 [kv_cache_utils.py:1286] GPU KV cache size: 23,264 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m INFO 12-13 17:23:54 [kv_cache_utils.py:1291] Maximum concurrency for 5,120 tokens per request: 4.54x\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m INFO 12-13 17:23:54 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "\u001b[0;36m(Worker_TP1 pid=1755)\u001b[0;0m INFO 12-13 17:23:54 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] WorkerProc hit an exception.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 287, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     subprocess.run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/root/.cache/flashinfer/0.5.3/75/cached_ops', '-f', '/root/.cache/flashinfer/0.5.3/75/cached_ops/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 817, in worker_busy_loop\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     output = func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 423, in compile_or_warm_up_model\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     kernel_warmup(self)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/warmup/kernel_warmup.py\", line 68, in kernel_warmup\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     worker.model_runner._dummy_run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     return func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 3995, in _dummy_run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata, _ = self._build_attention_metadata(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1692, in _build_attention_metadata\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata_i = builder.build(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                       ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/attention/backends/flashinfer.py\", line 909, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata.prefill_wrapper.plan(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 1838, in plan\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self._cached_module = get_batch_prefill_module(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 374, in get_batch_prefill_module\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     module = gen_batch_prefill_module(backend, *args).build_and_load()\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 309, in build_and_load\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self.build(verbose, need_lock=False)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 295, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 299, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise RuntimeError(msg) from e\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 287, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     subprocess.run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/root/.cache/flashinfer/0.5.3/75/cached_ops', '-f', '/root/.cache/flashinfer/0.5.3/75/cached_ops/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 817, in worker_busy_loop\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     output = func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 423, in compile_or_warm_up_model\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     kernel_warmup(self)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/warmup/kernel_warmup.py\", line 68, in kernel_warmup\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     worker.model_runner._dummy_run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     return func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 3995, in _dummy_run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata, _ = self._build_attention_metadata(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1692, in _build_attention_metadata\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata_i = builder.build(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                       ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/attention/backends/flashinfer.py\", line 909, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata.prefill_wrapper.plan(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 1838, in plan\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self._cached_module = get_batch_prefill_module(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 374, in get_batch_prefill_module\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     module = gen_batch_prefill_module(backend, *args).build_and_load()\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 309, in build_and_load\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self.build(verbose, need_lock=False)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 295, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 299, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise RuntimeError(msg) from e\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] EngineCore failed to start.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 834, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 610, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 109, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 251, in _initialize_kv_caches\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 361, in collective_rpc\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     return aggregate(get_response())\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                      ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 344, in get_response\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     raise RuntimeError(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] RuntimeError: Worker failed with error 'Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ', please check the stack trace above for the root cause\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] WorkerProc hit an exception.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 287, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     subprocess.run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/root/.cache/flashinfer/0.5.3/75/cached_ops', '-f', '/root/.cache/flashinfer/0.5.3/75/cached_ops/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 817, in worker_busy_loop\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     output = func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 423, in compile_or_warm_up_model\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     kernel_warmup(self)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/warmup/kernel_warmup.py\", line 68, in kernel_warmup\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     worker.model_runner._dummy_run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     return func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 3995, in _dummy_run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata, _ = self._build_attention_metadata(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1692, in _build_attention_metadata\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata_i = builder.build(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                       ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/attention/backends/flashinfer.py\", line 909, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata.prefill_wrapper.plan(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 1838, in plan\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self._cached_module = get_batch_prefill_module(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 374, in get_batch_prefill_module\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     module = gen_batch_prefill_module(backend, *args).build_and_load()\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 309, in build_and_load\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self.build(verbose, need_lock=False)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 295, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 299, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise RuntimeError(msg) from e\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 287, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     subprocess.run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/root/.cache/flashinfer/0.5.3/75/cached_ops', '-f', '/root/.cache/flashinfer/0.5.3/75/cached_ops/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] Traceback (most recent call last):\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 817, in worker_busy_loop\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     output = func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 423, in compile_or_warm_up_model\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     kernel_warmup(self)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/warmup/kernel_warmup.py\", line 68, in kernel_warmup\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     worker.model_runner._dummy_run(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     return func(*args, **kwargs)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 3995, in _dummy_run\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata, _ = self._build_attention_metadata(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1692, in _build_attention_metadata\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata_i = builder.build(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                       ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/attention/backends/flashinfer.py\", line 909, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     attn_metadata.prefill_wrapper.plan(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 1838, in plan\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self._cached_module = get_batch_prefill_module(\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/prefill.py\", line 374, in get_batch_prefill_module\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     module = gen_batch_prefill_module(backend, *args).build_and_load()\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 309, in build_and_load\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     self.build(verbose, need_lock=False)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/core.py\", line 295, in build\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]   File \"/usr/local/lib/python3.11/dist-packages/flashinfer/jit/cpp_ext.py\", line 299, in run_ninja\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822]     raise RuntimeError(msg) from e\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(Worker_TP0 pid=1754)\u001b[0;0m ERROR 12-13 17:23:54 [multiproc_executor.py:822] \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] EngineCore failed to start.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 834, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 610, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 109, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 251, in _initialize_kv_caches\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 361, in collective_rpc\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     return aggregate(get_response())\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]                      ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 344, in get_response\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843]     raise RuntimeError(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] RuntimeError: Worker failed with error 'Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:54 [core.py:843] ', please check the stack trace above for the root cause\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:57 [multiproc_executor.py:233] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ERROR 12-13 17:23:57 [multiproc_executor.py:233] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     self.run()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 847, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     raise e\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 834, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 610, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 109, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 251, in _initialize_kv_caches\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 361, in collective_rpc\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     return aggregate(get_response())\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m                      ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 344, in get_response\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m     raise RuntimeError(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m RuntimeError: Worker failed with error 'Ninja build failed. Ninja output:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ninja: Entering directory `/root/.cache/flashinfer/0.5.3/75/cached_ops'\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m [1/1] c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m FAILED: [code=1] batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m c++ batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_0.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_1.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_2.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_paged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_ragged_kernel_mask_3.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_binding.cuda.o -shared -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -lcudart -lcuda -o batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False.so\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m /usr/bin/ld: cannot find -lcuda: No such file or directory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m collect2: error: ld returned 1 exit status\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ninja: build stopped: subcommand failed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1743)\u001b[0;0m ', please check the stack trace above for the root cause\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/4168623209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model_path = \"/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/qwen-3/transformers/32b-awq/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m llm = vllm.LLM(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mquantization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'awq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mlog_non_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mengine_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUsageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLM_CLASS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# Create the LLMEngine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         self.engine_core = EngineCoreClient.make_client(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmultiprocess_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSyncMPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInprocClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     ):\n\u001b[0;32m--> 642\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;31m# Engines are managed by this client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0m\u001b[1;32m    472\u001b[0m                     \u001b[0mengine_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0mcoordinator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Now wait for engines to start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         wait_for_engine_startup(\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mhandshake_socket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoord_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    961\u001b[0m                 \u001b[0;34m\"Engine core initialization failed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                 \u001b[0;34m\"See root cause above. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "# vLLM V1 does not currently accept logits processor so we need to disable it\n",
    "# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "#model_path = \"/kaggle/input/qwen2.5-coder/transformers/32b-instruct-awq/1\"\n",
    "# Use 14B model (32B causes linker failures with libcuda on Kaggle T4 GPUs)\n",
    "model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.189707Z",
     "iopub.status.idle": "2025-12-13T13:14:48.190032Z",
     "shell.execute_reply": "2025-12-13T13:14:48.189881Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.189866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_72/2742850253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m Assistant:\"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def stream_generate(llm, prompt):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    for output in llm.generate(\n",
    "        [prompt],\n",
    "        sampling_params,\n",
    "        \n",
    "    ):\n",
    "        yield output.outputs[0].text\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "User: Explain tensor parallelism in simple terms.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-13T13:14:48.191299Z",
     "iopub.status.idle": "2025-12-13T13:14:48.191605Z",
     "shell.execute_reply": "2025-12-13T13:14:48.191474Z",
     "shell.execute_reply.started": "2025-12-13T13:14:48.191458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is quantum computing?\"\"\"\n",
    "\n",
    "for token in stream_generate(llm, prompt):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê LOADING SECRETS FROM KAGGLE\n",
      "============================================================\n",
      "‚úÖ GROQ_API_KEY loaded successfully\n",
      "   Key length: 56 characters\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN loaded successfully\n",
      "   Token length: 93 characters\n",
      "\n",
      "‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load secrets from Kaggle's secure environment\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîê LOADING SECRETS FROM KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to load GROQ_API_KEY\n",
    "try:\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    print(\"‚úÖ GROQ_API_KEY loaded successfully\")\n",
    "    print(f\"   Key length: {len(GROQ_API_KEY)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GROQ_API_KEY not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GROQ_API_KEY = None\n",
    "\n",
    "# Try to load GITHUB_TOKEN\n",
    "try:\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    os.environ[\"GITHUB_TOKEN\"] = GITHUB_TOKEN\n",
    "    print(\"‚úÖ GITHUB_TOKEN loaded successfully\")\n",
    "    print(f\"   Token length: {len(GITHUB_TOKEN)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GITHUB_TOKEN not found: {e}\")\n",
    "    print(\"   Add it in Kaggle Settings ‚Üí Secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "\n",
    "# Set LLM backend to use vLLM (local model on Kaggle GPU)\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"  # We'll use the vLLM model loaded above\n",
    "print(\"\\n‚úÖ LLM_BACKEND set to 'vllm' (using Kaggle GPU)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç ENVIRONMENT CHECK\n",
      "============================================================\n",
      "‚úÖ PyTorch version: 2.6.0+cu124\n",
      "‚úÖ CUDA available: False\n",
      "‚úÖ CUDA version: 12.4\n",
      "‚úÖ Number of GPUs: 0\n",
      "\n",
      "‚úÖ Current working directory: /kaggle/working\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and configuration\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç ENVIRONMENT CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Current working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• CLONING REPOSITORY\n",
      "============================================================\n",
      "‚ö†Ô∏è  Removing existing Subchat-Trees directory...\n",
      "üîÑ Cloning kaggle-run branch (skipping LFS files)...\n",
      "   No authentication required for cloning (public repo)\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n",
      "‚úÖ Successfully cloned kaggle-run branch!\n",
      "üìÇ Repository location: /kaggle/working/Subchat-Trees\n",
      "\n",
      "üìÅ Key directories found:\n",
      "   ‚úÖ backend\n",
      "   ‚úÖ backend/src\n",
      "   ‚úÖ backend/dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clone the kaggle-run branch from GitHub (PUBLIC READ - no auth needed)\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "REPO_DIR = \"Subchat-Trees\"\n",
    "BRANCH = \"kaggle-run\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• CLONING REPOSITORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone the specific branch (no authentication needed for public repos)\n",
    "# Skip LFS files to avoid bandwidth quota issues\n",
    "print(f\"üîÑ Cloning {BRANCH} branch (skipping LFS files)...\")\n",
    "print(\"   No authentication required for cloning (public repo)\")\n",
    "\n",
    "# Set environment variable to skip LFS files\n",
    "clone_env = os.environ.copy()\n",
    "clone_env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", BRANCH, \"--single-branch\", REPO_URL, REPO_DIR],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env=clone_env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Successfully cloned {BRANCH} branch!\")\n",
    "    print(f\"üìÇ Repository location: {os.path.abspath(REPO_DIR)}\")\n",
    "    \n",
    "    # List key directories to verify\n",
    "    print(\"\\nüìÅ Key directories found:\")\n",
    "    key_dirs = [\"backend\", \"backend/src\", \"backend/dataset\"]\n",
    "    for dir_path in key_dirs:\n",
    "        full_path = os.path.join(REPO_DIR, dir_path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   ‚úÖ {dir_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {dir_path} (not found)\")\n",
    "else:\n",
    "    print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!git config user.name \"moonmehedi\"\n",
    "!git config user.email \"the.mehedi.hasan.moon@gmail.com\"\n",
    "\n",
    "print(\"‚úÖ Git identity configured!\")\n",
    "print(f\"   User: moonmehedi\")\n",
    "print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "\n",
    "# Verify current branch\n",
    "branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.chdir(\"..\")  # Return to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING GIT PUSH CAPABILITY\n",
      "============================================================\n",
      "‚úÖ Created detailed test log: kaggle_logs/connection_test.log\n",
      "[kaggle-run 7a97502] Test: Kaggle GPU verification - 2025-12-13 16:32:42\n",
      " 1 file changed, 27 insertions(+)\n",
      " create mode 100644 kaggle_logs/connection_test.log\n",
      "[kaggle-run 7a97502] Test: Kaggle GPU verification - 2025-12-13 16:32:42\n",
      " 1 file changed, 27 insertions(+)\n",
      " create mode 100644 kaggle_logs/connection_test.log\n",
      "\n",
      "üîÑ Pushing to GitHub...\n",
      "\n",
      "üîÑ Pushing to GitHub...\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-13 16:32:42\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n",
      "‚úÖ Successfully pushed to GitHub!\n",
      "   üìÅ Check: kaggle_logs/connection_test.log\n",
      "   üìÖ Pushed at: 2025-12-13 16:32:42\n",
      "   üí° Pull on your local machine to verify sync\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def create_test_log(log_dir=\"kaggle_logs\", log_file=\"connection_test.log\"):\n",
    "    \"\"\"Create a detailed test log with GPU and environment info\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üî¨ KAGGLE GPU TEST RUN - CONNECTION VERIFICATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"üìÖ Test Date: {current_time.strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"‚è∞ Test Time: {current_time.strftime('%H:%M:%S UTC')}\\n\")\n",
    "        f.write(f\"üìç Timestamp: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"üéÆ GPU CONFIGURATION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"GPU Count: {torch.cuda.device_count()}\\n\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                f.write(f\"\\nGPU {i}:\\n\")\n",
    "                f.write(f\"  - Name: {torch.cuda.get_device_name(i)}\\n\")\n",
    "                f.write(f\"  - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\\n\")\n",
    "        else:\n",
    "            f.write(\"‚ö†Ô∏è  No GPU detected\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"üìä ENVIRONMENT INFO\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "        f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
    "        f.write(f\"Working Directory: {os.getcwd()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        f.write(\"‚úÖ TEST STATUS: SUCCESS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"\\nThis log was generated from Kaggle notebook\\n\")\n",
    "        f.write(f\"Push attempt at: {current_time.isoformat()}\\n\")\n",
    "    \n",
    "    return log_path, current_time\n",
    "\n",
    "\n",
    "def git_commit_and_push(file_path, commit_message, branch=\"kaggle-run\"):\n",
    "    \"\"\"Commit a file and push to GitHub\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Add file\n",
    "    add_result = subprocess.run([\"git\", \"add\", file_path], capture_output=True, text=True)\n",
    "    if add_result.returncode != 0:\n",
    "        return False, f\"Git add failed: {add_result.stderr}\"\n",
    "    \n",
    "    # Commit\n",
    "    commit_result = subprocess.run([\"git\", \"commit\", \"-m\", commit_message], capture_output=True, text=True)\n",
    "    if commit_result.returncode != 0:\n",
    "        return False, f\"Git commit failed: {commit_result.stderr}\"\n",
    "    \n",
    "    # Push with token\n",
    "    if \"GITHUB_TOKEN\" not in os.environ:\n",
    "        return False, \"GITHUB_TOKEN not found in environment\"\n",
    "    \n",
    "    repo_url_with_token = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/moonmehedi/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM.git\"\n",
    "    \n",
    "    # Set remote URL\n",
    "    subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", repo_url_with_token], capture_output=True)\n",
    "    \n",
    "    # Push\n",
    "    push_result = subprocess.run([\"git\", \"push\", \"origin\", branch], capture_output=True, text=True)\n",
    "    \n",
    "    if push_result.returncode == 0:\n",
    "        return True, \"Push successful\"\n",
    "    else:\n",
    "        return False, f\"Push failed: {push_result.stderr}\"\n",
    "\n",
    "\n",
    "# Main execution\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING GIT PUSH CAPABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Change to repo directory\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Create test log\n",
    "    log_path, timestamp = create_test_log()\n",
    "    print(f\"‚úÖ Created detailed test log: {log_path}\")\n",
    "    \n",
    "    # Commit and push\n",
    "    commit_msg = f\"Test: Kaggle GPU verification - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    success, message = git_commit_and_push(log_path, commit_msg, BRANCH)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Successfully pushed to GitHub!\")\n",
    "        print(f\"   üìÅ Check: {log_path}\")\n",
    "        print(f\"   üìÖ Pushed at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"   üí° Pull on your local machine to verify sync\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Always return to parent directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Step 5: Integrate vLLM with Backend\n",
    "\n",
    "**This connects the loaded vLLM model to your backend code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the vLLM model with the backend\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó INTEGRATING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Register the globally loaded vLLM model\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(\"‚úÖ vLLM model is now available to backend services\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Backend will use LLM_BACKEND={os.getenv('LLM_BACKEND')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Step 6: Test vLLM Integration\n",
    "\n",
    "**Quick test to verify backend can use vLLM on Kaggle GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the backend with vLLM\n",
    "from src.services.simple_llm import SimpleLLMClient\n",
    "from src.models.tree import TreeNode, LocalBuffer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING BACKEND WITH vLLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple test\n",
    "llm_client = SimpleLLMClient()\n",
    "\n",
    "# Create a test node with buffer\n",
    "buffer = LocalBuffer(buffer_size=5)\n",
    "root = TreeNode(id=\"test\", title=\"Test Conversation\", buffer=buffer)\n",
    "\n",
    "# Add a message to buffer\n",
    "buffer.add_message(\"user\", \"Hello, test message\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nüìù Testing response generation...\")\n",
    "response = llm_client.generate_response(root, \"What is 2+2?\")\n",
    "\n",
    "print(f\"\\n‚úÖ Response: {response}\")\n",
    "print(f\"üìä Token usage: {llm_client.get_last_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Backend integration successful!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing ends"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310545,
     "sourceId": 375832,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
