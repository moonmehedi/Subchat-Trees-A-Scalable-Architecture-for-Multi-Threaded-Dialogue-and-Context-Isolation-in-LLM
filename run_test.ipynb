{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d4ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# For Qwen\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fe3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "‚úÖ VLLM_MODEL_PATH: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# ‚úÖ FIXED: Use correct model path (must match hirachical-subchat.ipynb)\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566aebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ LOADING vLLM MODEL ON THIS KERNEL\n",
      "============================================================\n",
      "üìÇ Model: /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\n",
      "üéÆ GPUs: 2\n",
      "‚è≥ This takes 2-3 minutes...\n",
      "============================================================\n",
      "INFO 12-19 12:57:42 [config.py:717] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 12-19 12:57:43 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-19 12:57:43 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "WARNING 12-19 12:57:43 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-19 12:57:44 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 12-19 12:57:44 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 12-19 12:57:44 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-19 12:57:44 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-19 12:57:44 [cuda.py:289] Using XFormers backend.\n",
      "INFO 12-19 12:57:50 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:00 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:01 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:01 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 12:58:02.514702118 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 12:58:02.515458078 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:58:02 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:02 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:02 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-19 12:58:02 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1219 12:58:02.786607639 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1219 12:58:02.787286852 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-19 12:58:03 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:03 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-19 12:58:03 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cd016a94'), local_subscribe_addr='ipc:///tmp/cd659b8c-d934-445d-aed0-86810567181d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-19 12:58:03 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:03 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 12-19 12:58:03 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:58:03 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 232, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     output = run_method(worker, method, args, kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 2456, in run_method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 203, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\", line 1111, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     model = _initialize_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 436, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 305, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                                                      ^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 307, in <lambda>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     lambda prefix: decoder_layer_type(config=config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 217, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.mlp = Qwen2MLP(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                ^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 74, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     self.quant_method.create_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/quantization/awq.py\", line 118, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     data=torch.empty(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]          ^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m ERROR 12-19 12:58:03 [multiproc_worker_utils.py:238] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 34.19 MiB is free. Process 5396 has 12.52 GiB memory in use. Process 9705 has 2.17 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 93.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 32.19 MiB is free. Process 4056 has 12.52 GiB memory in use. Process 7146 has 2.17 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 93.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_471/2005061026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m llm = vllm.LLM(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquantization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'awq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_worker_tasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAwaitable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     def execute_model(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         self._run_workers(\"load_model\",\n\u001b[0m\u001b[1;32m    126\u001b[0m                           \u001b[0mmax_concurrent_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                           max_parallel_loading_workers)\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/mp_distributed_executor.py\u001b[0m in \u001b[0;36m_run_workers\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    183\u001b[0m         ]\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         driver_worker_output = run_method(self.driver_worker, sent_method,\n\u001b[0m\u001b[1;32m    186\u001b[0m                                           args, kwargs)\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def save_sharded_state(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtime_before_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 assert supports_lora(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mweights_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix, model_class)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# new-style model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         self.model = Qwen2Model(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    437\u001b[0m                                 prefix=maybe_prefix(prefix, \"model\"))\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, decoder_layer_type)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Use the provided decoder layer type or default to Qwen2DecoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mdecoder_layer_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_layer_type\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mQwen2DecoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             lambda prefix: decoder_layer_type(config=config,\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                             get_pp_group().world_size)\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[0;32m--> 609\u001b[0;31m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[1;32m    609\u001b[0m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    305\u001b[0m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[1;32m    306\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             lambda prefix: decoder_layer_type(config=config,\n\u001b[0m\u001b[1;32m    308\u001b[0m                                               \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                                               \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mattn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[0;32m--> 217\u001b[0;31m         self.mlp = Qwen2MLP(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mintermediate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config, prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m     ) -> None:\n\u001b[1;32m     73\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         self.gate_up_proj = MergedColumnParallelLinear(\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix, return_bias)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mtp_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tensor_model_parallel_world_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtp_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         super().__init__(input_size=input_size,\n\u001b[0m\u001b[1;32m    545\u001b[0m                          \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                          \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix, return_bias)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         self.quant_method.create_weights(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/quantization/awq.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mweight_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_weight_attrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight_loader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         qweight = PackedvLLMParameter(\n\u001b[0;32m--> 118\u001b[0;31m             data=torch.empty(\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0moutput_size_per_partition\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 32.19 MiB is free. Process 4056 has 12.52 GiB memory in use. Process 7146 has 2.17 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 93.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=510)\u001b[0;0m INFO 12-19 12:59:21 [multiproc_worker_utils.py:259] Worker exiting\n",
      "ERROR 12-19 12:59:21 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 510 died, exit code: -15\n",
      "INFO 12-19 12:59:21 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ STEP 2: Load vLLM model (RUN THIS BEFORE STEP 3!)\n",
    "# Disable vLLM V1 (doesn't support logits processors yet)\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ LOADING vLLM MODEL ON THIS KERNEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Model: {model_path}\")\n",
    "print(f\"üéÆ GPUs: {torch.cuda.device_count()}\")\n",
    "print(\"‚è≥ This takes 2-3 minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization='awq',\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    gpu_memory_utilization=0.91,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ vLLM model loaded successfully!\")\n",
    "print(f\"   Memory per GPU: ~{torch.cuda.get_device_properties(0).total_memory / 1024**3 * 0.91:.1f}GB used\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ STEP 3: Register vLLM with backend (RUN AFTER STEP 2!)\n",
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working/Subchat-Trees/backend\")\n",
    "\n",
    "from src.services.vllm_client import VLLMClient\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó REGISTERING vLLM WITH BACKEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "VLLMClient.set_model(llm)\n",
    "\n",
    "print(f\"‚úÖ vLLM registered: {VLLMClient.is_available()}\")\n",
    "print(\"   ‚úÖ Response generation will use vLLM\")\n",
    "print(\"   ‚úÖ Summarization will use vLLM\")\n",
    "print(\"   ‚úÖ Judge/Classification will use vLLM\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT FOR KAGGLE\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "\n",
      "‚úÖ GITHUB_TOKEN available: gith...tWfg\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity for Kaggle git push\n",
    "import subprocess\n",
    "\n",
    "REPO_DIR = \"/kaggle/working/Subchat-Trees\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT FOR KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Set git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"moonmehedi\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"the.mehedi.hasan.moon@gmail.com\"], check=True)\n",
    "    \n",
    "    print(\"‚úÖ Git identity configured!\")\n",
    "    print(f\"   User: moonmehedi\")\n",
    "    print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "    \n",
    "    # Verify current branch\n",
    "    branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "    print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "    \n",
    "    # Check if GITHUB_TOKEN is available\n",
    "    if \"GITHUB_TOKEN\" in os.environ:\n",
    "        token_preview = os.environ[\"GITHUB_TOKEN\"]\n",
    "        print(f\"\\n‚úÖ GITHUB_TOKEN available: {token_preview[:4]}...{token_preview[-4:]}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: GITHUB_TOKEN not set - git push will fail!\")\n",
    "    \n",
    "    # ‚úÖ VERIFY vLLM is still registered after all imports\n",
    "    sys.path.insert(0, os.path.join(REPO_DIR, \"backend\"))\n",
    "    from src.services.vllm_client import VLLMClient\n",
    "    print(f\"\\n‚úÖ vLLM still available: {VLLMClient.is_available()}\")\n",
    "    \n",
    "    os.chdir(\"/kaggle/working\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbe15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a92be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py\", line 1182, in <module>\n",
      "    runner = KaggleMetricsTestRunner()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py\", line 39, in __init__\n",
      "    self.classifier = ContextClassifier()\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/context_classifier.py\", line 68, in __init__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: ‚ùå vLLM not available for classification!\n",
      "   Judge/classifier requires vLLM to avoid Groq API quota limits.\n",
      "   Please ensure VLLMClient.set_model(llm) was called in your notebook.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ‚úÖ RUN KAGGLE BUFFER TEST RUNNER (with automatic git push after each buffer)\n",
    "! python /kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9357495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üõë SHUTTING DOWN SERVER AND KERNEL\n",
      "============================================================\n",
      "\n",
      "üî¥ Stopping backend server...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"üõë SHUTTING DOWN SERVER AND KERNEL\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # 1. Kill the backend server\n",
    "# try:\n",
    "#     print(\"\\nüî¥ Stopping backend server...\")\n",
    "#     result = subprocess.run(\n",
    "#         [\"pkill\", \"-f\", \"uvicorn.*src.main:app\"],\n",
    "#         capture_output=True,\n",
    "#         text=True\n",
    "#     )\n",
    "#     time.sleep(2)\n",
    "#     print(\"‚úÖ Backend server stopped\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è  Error stopping server: {e}\")\n",
    "\n",
    "# # 2. Kill the kernel\n",
    "# print(\"\\nüî¥ Terminating kernel...\")\n",
    "# print(\"‚úÖ Kernel will shut down now - this saves GPU quota!\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Force exit the kernel\n",
    "# os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
