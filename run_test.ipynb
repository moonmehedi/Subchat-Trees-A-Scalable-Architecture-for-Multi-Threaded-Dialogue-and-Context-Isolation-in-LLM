{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2fe3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "‚úÖ VLLM_MODEL_PATH: /kaggle/input/qwen-3/transformers/14b-awq/1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# ‚úÖ NEW: Set vLLM model path (must match hirachical-subchat.ipynb)\n",
    "model_path = \"/kaggle/input/qwen-3/transformers/14b-awq/1\"\n",
    "os.environ[\"VLLM_MODEL_PATH\"] = model_path\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(f\"‚úÖ VLLM_MODEL_PATH: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9ef0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è  CONFIGURING GIT FOR KAGGLE\n",
      "============================================================\n",
      "‚úÖ Git identity configured!\n",
      "   User: moonmehedi\n",
      "   Email: the.mehedi.hasan.moon@gmail.com\n",
      "\n",
      "‚úÖ Current branch: kaggle-run\n",
      "\n",
      "‚úÖ GITHUB_TOKEN available: gith...tWfg\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure git identity for Kaggle git push\n",
    "import subprocess\n",
    "\n",
    "REPO_DIR = \"/kaggle/working/Subchat-Trees\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURING GIT FOR KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    os.chdir(REPO_DIR)\n",
    "    \n",
    "    # Set git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"moonmehedi\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"the.mehedi.hasan.moon@gmail.com\"], check=True)\n",
    "    \n",
    "    print(\"‚úÖ Git identity configured!\")\n",
    "    print(f\"   User: moonmehedi\")\n",
    "    print(f\"   Email: the.mehedi.hasan.moon@gmail.com\")\n",
    "    \n",
    "    # Verify current branch\n",
    "    branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
    "    print(f\"\\n‚úÖ Current branch: {branch_result.stdout.strip()}\")\n",
    "    \n",
    "    # Check if GITHUB_TOKEN is available\n",
    "    if \"GITHUB_TOKEN\" in os.environ:\n",
    "        token_preview = os.environ[\"GITHUB_TOKEN\"]\n",
    "        print(f\"\\n‚úÖ GITHUB_TOKEN available: {token_preview[:4]}...{token_preview[-4:]}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: GITHUB_TOKEN not set - git push will fail!\")\n",
    "    \n",
    "    os.chdir(\"/kaggle/working\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbe15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a92be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py\", line 1182, in <module>\n",
      "    runner = KaggleMetricsTestRunner()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py\", line 39, in __init__\n",
      "    self.classifier = ContextClassifier()\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/context_classifier.py\", line 68, in __init__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: ‚ùå vLLM not available for classification!\n",
      "   Judge/classifier requires vLLM to avoid Groq API quota limits.\n",
      "   Please ensure VLLMClient.set_model(llm) was called in your notebook.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ‚úÖ RUN KAGGLE BUFFER TEST RUNNER (with automatic git push after each buffer)\n",
    "! python /kaggle/working/Subchat-Trees/backend/dataset/kaggle_buffer_test_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9357495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üõë SHUTTING DOWN SERVER AND KERNEL\n",
      "============================================================\n",
      "\n",
      "üî¥ Stopping backend server...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üõë SHUTTING DOWN SERVER AND KERNEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Kill the backend server\n",
    "try:\n",
    "    print(\"\\nüî¥ Stopping backend server...\")\n",
    "    result = subprocess.run(\n",
    "        [\"pkill\", \"-f\", \"uvicorn.*src.main:app\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    time.sleep(2)\n",
    "    print(\"‚úÖ Backend server stopped\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error stopping server: {e}\")\n",
    "\n",
    "# 2. Kill the kernel\n",
    "print(\"\\nüî¥ Terminating kernel...\")\n",
    "print(\"‚úÖ Kernel will shut down now - this saves GPU quota!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force exit the kernel\n",
    "os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
