{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b02594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fe3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîê SECRETS LOADED AND SET IN ENVIRONMENT\n",
      "============================================================\n",
      "‚úÖ GITHUB_TOKEN: gith...tWfg\n",
      "‚úÖ GROQ_API_KEY: gsk_...l6gr\n",
      "‚úÖ HuggingFACEHUB_access_token: hf_E...GaQC\n",
      "‚úÖ LANGCHAIN_API_KEY: lsv2...ea2f\n",
      "‚úÖ LLM_BACKEND: vllm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "secret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "secret_value_2 = user_secrets.get_secret(\"HuggingFACEHUB_access_token\")\n",
    "secret_value_3 = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ IMPORTANT: Set them in os.environ so other code can access them\n",
    "os.environ[\"GITHUB_TOKEN\"] = secret_value_0\n",
    "os.environ[\"GROQ_API_KEY\"] = secret_value_1\n",
    "os.environ[\"HuggingFACEHUB_access_token\"] = secret_value_2\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = secret_value_3\n",
    "os.environ[\"LLM_BACKEND\"] = \"vllm\"\n",
    "\n",
    "# Print the tokens (first 4 and last 4 characters for security)\n",
    "print(\"=\"*60)\n",
    "print(\"üîê SECRETS LOADED AND SET IN ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ GITHUB_TOKEN: {secret_value_0[:4]}...{secret_value_0[-4:]}\")\n",
    "print(f\"‚úÖ GROQ_API_KEY: {secret_value_1[:4]}...{secret_value_1[-4:]}\")\n",
    "print(f\"‚úÖ HuggingFACEHUB_access_token: {secret_value_2[:4]}...{secret_value_2[-4:]}\")\n",
    "print(f\"‚úÖ LANGCHAIN_API_KEY: {secret_value_3[:4]}...{secret_value_3[-4:]}\")\n",
    "print(f\"‚úÖ LLM_BACKEND: vllm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ef0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì• PULLING GIT LFS FILES FOR SCENARIOS\n",
      "============================================================\n",
      "‚úÖ Successfully pulled scenario files from Git LFS\n",
      "\n",
      "\n",
      "üìÅ Scenario file sizes after LFS pull:\n",
      "01_python_confusion.json\n",
      "02_nested_depth_test.json\n",
      "03_long_context_mercury_rust_java.json\n",
      "04_ambignq_entertainment_sports_chaos.json\n",
      "04_python_species_variants.json\n",
      "05_ambignq_tech_history_entity_chaos.json\n",
      "05_jaguar_multi_domain.json\n",
      "06_buffer_overflow_passwords.json\n",
      "06_lost_in_conversation_sharded_humaneval.json\n",
      "07_pizza_variants.json\n",
      "08_quantum_computing.json\n",
      "6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pull Git LFS files for scenario JSON files only\n",
    "# This downloads the actual file content instead of LFS pointers\n",
    "import subprocess\n",
    "\n",
    "REPO_DIR = \"/kaggle/working/Subchat-Trees\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• PULLING GIT LFS FILES FOR SCENARIOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    os.chdir(REPO_DIR)\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"lfs\", \"pull\", \"--include=backend/dataset/scenarios/*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully pulled scenario files from Git LFS\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Git LFS pull returned code {result.returncode}\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "    # Verify files are no longer pointers\n",
    "    os.chdir(f\"{REPO_DIR}/backend/dataset/scenarios\")\n",
    "    check_result = subprocess.run(\n",
    "        [\"ls\", \"-lh\", \"*.json\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"\\nüìÅ Scenario file sizes after LFS pull:\")\n",
    "    print(check_result.stdout)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during Git LFS pull: {e}\")\n",
    "finally:\n",
    "    os.chdir(\"/kaggle/working\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca6ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:28:32] [INFO] ================================================================================\n",
      "[16:28:32] [INFO] üöÄ STARTING MULTI-BUFFER COMPARISON EVALUATION\n",
      "[16:28:32] [INFO]    Buffer sizes: [5, 10, 20, 40]\n",
      "[16:28:32] [INFO] ================================================================================\n",
      "[16:28:32] [INFO] \n",
      "================================================================================\n",
      "[16:28:32] [INFO] üì¶ TESTING BUFFER SIZE: 5\n",
      "[16:28:32] [INFO] ================================================================================\n",
      "[16:28:32] [INFO] ================================================================================\n",
      "[16:28:32] [INFO] üöÄ STARTING METRICS-BASED EVALUATION (buffer_size=5)\n",
      "[16:28:32] [INFO] ================================================================================\n",
      "[16:28:32] [INFO] ‚è≥ Checking if server is ready...\n",
      "[16:28:32] [INFO] ‚úÖ Server is ready!\n",
      "[16:28:32] [INFO] \n",
      "üîµ BASELINE TEST: 06_lost_in_conversation_sharded_humaneval.json (buffer_size=5)\n",
      "[16:28:32] [INFO] üîÑ Restarting server to clear ChromaDB...\n",
      "[16:28:32] [INFO]   üõë Stopping server...\n",
      "[16:28:34] [INFO]   ‚úÖ Server stopped\n",
      "[16:28:34] [INFO]   üóëÔ∏è  Clearing ChromaDB...\n",
      "[16:28:34] [INFO]   ‚úÖ ChromaDB cleared\n",
      "[16:28:34] [INFO]   üöÄ Starting server...\n",
      "[16:28:39] [INFO] ‚è≥ Checking if server is ready...\n",
      "[16:28:39] [INFO] ‚úÖ Server is ready!\n",
      "[16:28:39] [INFO]   ‚úÖ Server restarted successfully!\n",
      "[16:28:39] [INFO] ================================================================================\n",
      "[16:28:39] [INFO] üîµ BASELINE TEST: Lost in Conversation: Multi-Turn Shard Confusion from HumanEval (buffer_size=5)\n",
      "[16:28:39] [INFO]    Strategy: Single conversation for all topics (traditional chatbot)\n",
      "[16:28:39] [INFO] ================================================================================\n",
      "[16:28:39] [INFO]   üìù Created single conversation for all topics\n",
      "[16:28:39] [INFO] \n",
      "[Step 1] Context: intro\n",
      "[16:28:39] [INFO]   üí¨ User: I'm working on four coding problems: by_length, odd_count, move_one_ball, and histogram. Let me explain each one step by step. IMPORTANT: When responding to my questions, always start your response with the function name followed by a colon (e.g., 'by_length: ...' or 'odd_count: ...'). This helps track which problem context you're answering about.\n",
      "[16:29:17] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:29:17] [ERROR]   ‚ùå No response received\n",
      "[16:29:17] [INFO] \n",
      "[Step 2] Context: by_length_intro\n",
      "[16:29:17] [INFO]   üí¨ User: First problem: by_length where you have to turn digits into names in a list, with input [2, 1, 1, 4, 5, 8, 2, 3]\n",
      "[16:30:04] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:30:04] [ERROR]   ‚ùå No response received\n",
      "[16:30:04] [INFO] \n",
      "[Step 3] Context: by_length_step1\n",
      "[16:30:04] [INFO]   üí¨ User: Starting with a list of numbers\n",
      "[16:30:26] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:30:26] [ERROR]   ‚ùå No response received\n",
      "[16:30:26] [INFO] \n",
      "[Step 4] Context: odd_count_intro\n",
      "[16:30:26] [INFO]   üí¨ User: Second problem: odd_count where you have to get a list result from digit strings, with input ['1234567']\n",
      "[16:31:01] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:31:01] [ERROR]   ‚ùå No response received\n",
      "[16:31:01] [INFO] \n",
      "[Step 5] Context: odd_count_step1\n",
      "[16:31:01] [INFO]   üí¨ User: We're starting with a bunch of strings\n",
      "[16:31:19] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:31:19] [ERROR]   ‚ùå No response received\n",
      "[16:31:19] [INFO] \n",
      "[Step 6] Context: by_length_step2\n",
      "[16:31:19] [INFO]   üí¨ User: Sort numbers if they're between 1 and 9\n",
      "[16:32:07] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:32:07] [ERROR]   ‚ùå No response received\n",
      "[16:32:07] [INFO] \n",
      "[Step 7] Context: move_one_ball_intro\n",
      "[16:32:07] [INFO]   üí¨ User: Third problem: move_one_ball where you need to check if we can sort the array with right shifts, with input [3, 4, 5, 1, 2]\n",
      "[16:32:54] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:32:54] [ERROR]   ‚ùå No response received\n",
      "[16:32:54] [INFO] \n",
      "[Step 8] Context: move_one_ball_step1\n",
      "[16:32:54] [INFO]   üí¨ User: We're dealing with an array of numbers\n",
      "[16:33:17] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:33:17] [ERROR]   ‚ùå No response received\n",
      "[16:33:17] [INFO] \n",
      "[Step 9] Context: odd_count_step2\n",
      "[16:33:17] [INFO]   üí¨ User: Each string is made up of just numbers\n",
      "[16:33:32] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:33:32] [ERROR]   ‚ùå No response received\n",
      "[16:33:32] [INFO] \n",
      "[Step 10] Context: histogram_intro\n",
      "[16:33:32] [INFO]   üí¨ User: Fourth problem: histogram where you need to return a dictionary of the letter with the most repetition and containing the corresponding count, with input 'a b b a'\n",
      "[16:34:19] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:34:19] [ERROR]   ‚ùå No response received\n",
      "[16:34:19] [INFO] \n",
      "[Step 11] Context: by_length_step3\n",
      "[16:34:19] [INFO]   üí¨ User: Then flip the list around\n",
      "[16:35:07] [ERROR] ‚ùå Failed to send message: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/conversations/fd3ce9fd-a340-4a09-8ccb-42357aa39b3a/messages\n",
      "[16:35:07] [ERROR]   ‚ùå No response received\n",
      "[16:35:07] [INFO] \n",
      "[Step 12] Context: move_one_ball_step2\n",
      "[16:35:07] [INFO]   üí¨ User: You can do right shifts as many times as you want\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py\", line 1163, in <module>\n",
      "    runner.run_buffer_comparison(\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py\", line 775, in run_buffer_comparison\n",
      "    self.run_full_evaluation(scenario_files, buffer_size=buffer_size)\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py\", line 700, in run_full_evaluation\n",
      "    baseline_results = self.run_baseline_test(scenario, buffer_size=buffer_size)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py\", line 287, in run_baseline_test\n",
      "    response = self.send_message(node_id, message, disable_rag=False)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py\", line 229, in send_message\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/http/client.py\", line 286, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python /kaggle/working/Subchat-Trees/backend/dataset/buffer_test_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open '/media/sda3/projects/simple build/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM/backend/dataset/scenarios/06_lost_in_conversation_sharded_humaneval.json' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! head -5 \"/media/sda3/projects/simple build/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM/backend/dataset/scenarios/06_lost_in_conversation_sharded_humaneval.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Subchat-Trees/backend/dataset\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/Subchat-Trees/backend/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ca1db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1007/1233491604.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": [
    "pwd\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_evaluator.py                 dataset_logger.py  \u001b[0m\u001b[01;34mscenarios\u001b[0m/\n",
      "BUFFER_SIZE_IMPACT_ANALYSIS.md  \u001b[01;34mdataset_sources\u001b[0m/   test_classifier.py\n",
      "buffer_test_runner.py           \u001b[01;34mlogs\u001b[0m/              test_runner.py\n",
      "context_classifier.py           \u001b[01;34mnested_dataset\u001b[0m/    transform_remaining_files.py\n",
      "\u001b[01;34mdataset_creation\u001b[0m/               \u001b[01;34m__pycache__\u001b[0m/       view_aida.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Subchat-Trees/backend/dataset/dataset_sources\n"
     ]
    }
   ],
   "source": [
    "cd dataset_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c50ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'aida-yago2-dataset (11)'\u001b[0m/   \u001b[01;34mdstc8-schema-guided-dialogue\u001b[0m/   \u001b[01;34mqasper\u001b[0m/\n",
      " \u001b[01;34mambignq_light\u001b[0m/              \u001b[01;34mlmsys-chat-1m\u001b[0m/                  \u001b[01;34mTaskmaster\u001b[0m/\n",
      " DATASETS.md                 \u001b[01;34mlost_in_conversation\u001b[0m/\n",
      " DATASET_SURVEY.md           \u001b[01;34mmultiwoz\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e492b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_evaluator.py                 dataset_logger.py  \u001b[0m\u001b[01;34mscenarios\u001b[0m/\n",
      "BUFFER_SIZE_IMPACT_ANALYSIS.md  \u001b[01;34mdataset_sources\u001b[0m/   test_classifier.py\n",
      "buffer_test_runner.py           \u001b[01;34mlogs\u001b[0m/              test_runner.py\n",
      "context_classifier.py           \u001b[01;34mnested_dataset\u001b[0m/    transform_remaining_files.py\n",
      "\u001b[01;34mdataset_creation\u001b[0m/               \u001b[01;34m__pycache__\u001b[0m/       view_aida.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfdaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_python_confusion.json\n",
      "02_nested_depth_test.json\n",
      "03_long_context_mercury_rust_java.json\n",
      "04_ambignq_entertainment_sports_chaos.json\n",
      "04_python_species_variants.json\n",
      "05_ambignq_tech_history_entity_chaos.json\n",
      "05_jaguar_multi_domain.json\n",
      "06_buffer_overflow_passwords.json\n",
      "06_lost_in_conversation_sharded_humaneval.json\n",
      "07_pizza_variants.json\n",
      "07_real_world_multi_topic_tree.json\n",
      "08_quantum_computing.json\n",
      "6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee7efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Subchat-Trees/backend/dataset/scenarios\n"
     ]
    }
   ],
   "source": [
    "# Navigate to scenarios directory\n",
    "%cd /kaggle/working/Subchat-Trees/backend/dataset/scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 129 Dec 13 20:19 01_python_confusion.json\n",
      "-rw-r--r-- 1 root root 129 Dec 13 20:19 02_nested_depth_test.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 03_long_context_mercury_rust_java.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 04_ambignq_entertainment_sports_chaos.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 04_python_species_variants.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 05_ambignq_tech_history_entity_chaos.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 05_jaguar_multi_domain.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 06_buffer_overflow_passwords.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 06_lost_in_conversation_sharded_humaneval.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 07_pizza_variants.json\n",
      "-rw-r--r-- 1 root root   0 Dec 13 20:35 07_real_world_multi_topic_tree.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 08_quantum_computing.json\n",
      "-rw-r--r-- 1 root root 130 Dec 13 20:19 6c4992f0aed04dd3bf9a4bc225bb4fb0_structured.json\n"
     ]
    }
   ],
   "source": [
    "# List all JSON files with sizes\n",
    "!ls -lh *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version https://git-lfs.github.com/spec/v1\n",
      "oid sha256:1837370e4df6aaf8aa6764bf96984e227f5c7282f2b7faaf814b472a93ff64ca\n",
      "size 2187\n"
     ]
    }
   ],
   "source": [
    "# Check first 10 lines of a specific file\n",
    "!head -10 01_python_confusion.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version https://git-lfs.github.com/spec/v1\n",
      "oid sha256:185d3ee5f1a74ff32195457ccc9324c98c79b96ed652ab38b00725b241151ad6\n",
      "size 19387\n"
     ]
    }
   ],
   "source": [
    "# Check if files are Git LFS pointers (should show \"version https://git-lfs...\" if still pointers)\n",
    "!head -3 06_lost_in_conversation_sharded_humaneval.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Count how many files exist\n",
    "!ls *.json | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ca035",
   "metadata": {},
   "source": [
    "## üîç Diagnosis Results\n",
    "\n",
    "**What to look for:**\n",
    "- ‚úÖ **Good**: Files are 2-73 KB each (actual JSON content)\n",
    "- ‚ùå **Bad**: Files are ~128 bytes (Git LFS pointers)\n",
    "\n",
    "If files are 128 bytes and show \"version https://git-lfs.github.com/spec/v1\", you need to:\n",
    "1. Go back and run **Cell 3** (Git LFS pull)\n",
    "2. Then re-run the test runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c4540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0082e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
