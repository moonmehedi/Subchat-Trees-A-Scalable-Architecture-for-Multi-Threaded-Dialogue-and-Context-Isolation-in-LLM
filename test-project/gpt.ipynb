{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f98462d",
   "metadata": {},
   "source": [
    "## core.py\n",
    "\n",
    "\n",
    "This is your core conversation graph and buffer management code. It covers:\n",
    "\n",
    "Message buffer (with fixed max length) per node\n",
    "\n",
    "Tree node with parent/child links\n",
    "\n",
    "Chat graph manager to create/switch nodes\n",
    "\n",
    "Forest manager for multiple main chats\n",
    "\n",
    "Ready for vector_index.py? Just say next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ee269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "import uuid\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class LocalBuffer:\n",
    "    \"\"\"\n",
    "    LocalBuffer manages a fixed-size queue of recent chat messages.\n",
    "\n",
    "    1. Stores recent chat messages with a fixed maximum capacity.\n",
    "    2. Adds new messages along with sender role and timestamp.\n",
    "    3. Retrieves the latest or all stored messages for context building.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_turns: int = 50):\n",
    "        self.turns: deque[Dict[str, Any]] = deque(maxlen=max_turns)\n",
    "\n",
    "    def add_message(self, role: str, text: str):\n",
    "        \"\"\"Add a new message with role and current timestamp.\"\"\"\n",
    "        self.turns.append({\n",
    "            \"role\": role,\n",
    "            \"text\": text,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "\n",
    "    def get_recent(self, n: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the most recent n messages, or all if n is None.\"\"\"\n",
    "        return list(self.turns)[-n:] if n is not None else list(self.turns)\n",
    "\n",
    "    \n",
    "    def get_cutoff_timestamp(self, exclude_recent: int = 10) -> float:\n",
    "        \"\"\"Get timestamp to exclude last N messages from retrieval.\"\"\"\n",
    "        if len(self.turns) <= exclude_recent:\n",
    "            return float('inf')  # All messages are recent\n",
    "        return list(self.turns)[-exclude_recent][\"timestamp\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    def clear(self, n: int):\n",
    "        \"\"\"Clear the last n messages from the buffer.\"\"\"\n",
    "        for _ in range(min(n, len(self.turns))):\n",
    "            self.turns.pop()\n",
    "    \n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, node_id: Optional[str] = None, title: str = \"Untitled\", parent: Optional['TreeNode'] = None):\n",
    "        self.node_id: str = node_id if node_id else str(uuid.uuid4())\n",
    "        self.title: str = title\n",
    "        self.parent: Optional['TreeNode'] = parent\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.buffer: LocalBuffer = LocalBuffer()\n",
    "        self.origin_summary: Optional[str] = None  # I have to fix this in child nodes remember\n",
    "        self.metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "    def get_path(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of titles from root to this node, for path display.\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        current = self\n",
    "        while current:\n",
    "            path.append(current.title)\n",
    "            current = current.parent\n",
    "        return list(reversed(path))\n",
    "\n",
    "    def add_child(self, child_node: 'TreeNode'):\n",
    "        \"\"\"Add a child node and set this node as its parent.\"\"\"\n",
    "        self.children.append(child_node)\n",
    "        child_node.parent = self\n",
    "\n",
    "\n",
    "class ChatGraphManager:\n",
    "    \"\"\"\n",
    "    Manages the entire chat graph with nodes and navigation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.node_map: Dict[str, TreeNode] = {}\n",
    "        self.active_node_id: Optional[str] = None\n",
    "\n",
    "    def create_node(self, title: str, parent_id: Optional[str] = None) -> TreeNode: # will i ever pass parent_id?\n",
    "        parent = self.node_map.get(parent_id) if parent_id else None\n",
    "        node = TreeNode(title=title, parent=parent)\n",
    "        if parent:\n",
    "            parent.add_child(node)\n",
    "            # Copy parent's buffer messages to child\n",
    "            parent_messages = parent.buffer.get_recent()\n",
    "            \n",
    "            for msg in parent_messages:\n",
    "                node.buffer.add_message(msg[\"role\"], msg[\"text\"])\n",
    "            \n",
    "        self.node_map[node.node_id] = node\n",
    "        self.active_node_id = node.node_id\n",
    "        return node\n",
    "\n",
    "    def switch_node(self, node_id: str) -> TreeNode:\n",
    "        if node_id not in self.node_map:\n",
    "            raise KeyError(f\"Node ID {node_id} does not exist\")\n",
    "        self.active_node_id = node_id\n",
    "        return self.node_map[node_id]\n",
    "\n",
    "    def get_active_node(self) -> TreeNode:\n",
    "        if not self.active_node_id:\n",
    "            raise ValueError(\"No active node selected\")\n",
    "        return self.node_map[self.active_node_id]\n",
    "\n",
    "    def get_node(self, node_id: str) -> TreeNode:\n",
    "        return self.node_map[node_id]\n",
    "\n",
    "    def set_title(self, node_id: str, title: str):\n",
    "        if node_id not in self.node_map:\n",
    "            raise KeyError(f\"Node ID {node_id} does not exist\")\n",
    "        self.node_map[node_id].title = title\n",
    "\n",
    "\n",
    "class Forest:\n",
    "    \"\"\"\n",
    "    Manage multiple root-level trees (e.g., multiple main conversations)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.trees_map: Dict[str, TreeNode] = {}\n",
    "        self.active_tree_id: Optional[str] = None\n",
    "\n",
    "    def create_tree(self, title: str = \"Root Conversation\", chat_manager: Optional[ChatGraphManager] = None) -> TreeNode:\n",
    "        \"\"\"Create new root-level conversation tree and coordinate with chat manager.\"\"\"\n",
    "        root = TreeNode(title=title)\n",
    "        self.trees_map[root.node_id] = root\n",
    "        self.active_tree_id = root.node_id\n",
    "        \n",
    "        # Auto-sync with chat manager if provided\n",
    "        if chat_manager:\n",
    "            chat_manager.node_map[root.node_id] = root\n",
    "            chat_manager.active_node_id = root.node_id\n",
    "            \n",
    "        return root\n",
    "\n",
    "    def switch_tree(self, tree_id: str) -> TreeNode:\n",
    "        if tree_id not in self.trees_map:\n",
    "            raise KeyError(f\"Tree ID {tree_id} not found\")\n",
    "        self.active_tree_id = tree_id\n",
    "        return self.trees_map[tree_id]\n",
    "\n",
    "    def get_active_tree(self) -> TreeNode:\n",
    "        if not self.active_tree_id:\n",
    "            raise ValueError(\"No active tree selected\")\n",
    "        return self.trees_map[self.active_tree_id]\n",
    "    \n",
    "    def set_title(self, tree_id: str, title: str):\n",
    "        if tree_id not in self.trees_map:\n",
    "            raise KeyError(f\"Tree ID {tree_id} does not exist\")\n",
    "        self.trees_map[tree_id].title = title\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99333ed",
   "metadata": {},
   "source": [
    "## vector_index.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class GlobalVectorIndex:\n",
    "    \"\"\"\n",
    "    Manages a global vector store for all chat nodes.\n",
    "\n",
    "    - Uses OpenAI embeddings (can be swapped).\n",
    "    - Stores document texts and metadata.\n",
    "    - Supports similarity search with metadata filters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, persist_dir: Optional[str] = None):\n",
    "        self.embeddings = OpenAIEmbeddings()  # You can replace with another embedding model\n",
    "        self.store = Chroma(\n",
    "            collection_name=\"hierarchical_chat\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "        )\n",
    "\n",
    "    def index_docs(self, docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Indexes a list of documents with text and metadata.\n",
    "\n",
    "        Each doc is a dict with:\n",
    "          - \"text\": content string\n",
    "          - \"metadata\": dict with extra info (e.g., node_id, source)\n",
    "        \"\"\"\n",
    "        texts = [doc[\"text\"] for doc in docs]\n",
    "        metadatas = [doc.get(\"metadata\", {}) for doc in docs]\n",
    "        self.store.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "    def query(self, query_text: str, top_k: int = 5, filter_meta: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Search for top_k most similar docs to query_text.\n",
    "\n",
    "        filter_meta: Optional metadata filter to narrow search (e.g., {\"node_id\": \"abc123\"})\n",
    "        \"\"\"\n",
    "        if filter_meta:\n",
    "            return self.store.similarity_search(query_text, k=top_k, filter=filter_meta)\n",
    "        else:\n",
    "            return self.store.similarity_search(query_text, k=top_k)\n",
    "\n",
    "    def as_retriever(self, k=3, fetch_k=10, filter_meta: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Returns a retriever that supports Max Marginal Relevance (MMR) search with metadata filtering.\n",
    "\n",
    "        k: number of results to return after reranking.\n",
    "        fetch_k: number of initial candidates fetched for reranking.....\n",
    "        \"\"\"\n",
    "        return self.store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": k,\n",
    "                \"fetch_k\": fetch_k,\n",
    "                \"filter\": filter_meta\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        \"\"\"Utility to delete/cleanup the collection (useful during development).\"\"\"\n",
    "        try:\n",
    "            self.store._collection.delete()\n",
    "        except Exception:\n",
    "            # Some Chroma versions differ; ignore safe failures\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45890c",
   "metadata": {},
   "source": [
    "## llm_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    Wrapper for interacting with the LLM via LangChain.\n",
    "\n",
    "    - Uses ChatOpenAI for chat completions.\n",
    "    - Supports system and user messages.\n",
    "    - Can be extended for prompt templates or few-shot learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4\", temperature: float = 0.7):\n",
    "        self.model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Sends messages to the model and returns the text response.\n",
    "\n",
    "        messages: list of dicts with {\"role\": \"user\"|\"assistant\"|\"system\", \"content\": \"...\"}\n",
    "        system_prompt: optional system prompt to prepend\n",
    "\n",
    "        Example messages:\n",
    "          [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain quantum computing.\"}\n",
    "          ]\n",
    "        \"\"\"\n",
    "        chat_messages = []\n",
    "\n",
    "        if system_prompt:\n",
    "            chat_messages.append(SystemMessage(content=system_prompt))\n",
    "\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                chat_messages.append(HumanMessage(content=content))\n",
    "            elif role == \"assistant\":\n",
    "                chat_messages.append(AIMessage(content=content))\n",
    "            elif role == \"system\":\n",
    "                chat_messages.append(SystemMessage(content=content))\n",
    "\n",
    "        response = self.model(chat_messages)\n",
    "        return response.content\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b10c6",
   "metadata": {},
   "source": [
    "## assembler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, List, Dict\n",
    "# from core import ChatGraphManager, TreeNode\n",
    "# from vector_index import GlobalVectorIndex\n",
    "# from llm_client import LLMClient\n",
    "import time\n",
    "\n",
    "class ChatAssembler:\n",
    "    \"\"\"\n",
    "    Orchestrates chat graph nodes, vector store retrieval, and LLM response generation.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Manage current chat node context.\n",
    "    - Retrieve relevant memory snippets from vector store.\n",
    "    - Compose prompt context with local and retrieved info.\n",
    "    - Send to LLM and update node buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, persist_dir: Optional[str] = None):\n",
    "        self.chat_manager = ChatGraphManager()\n",
    "        self.vector_index = GlobalVectorIndex(persist_dir=persist_dir)\n",
    "        self.llm_client = LLMClient()\n",
    "        self.forest = Forest()\n",
    "\n",
    "    def process_user_message(self, user_text: str) -> str:\n",
    "        node = self.chat_manager.get_active_node()\n",
    "\n",
    "        # Add user message locally\n",
    "        node.buffer.add_message(role=\"user\", text=user_text)\n",
    "\n",
    "        # Get timestamp cutoff to exclude last 10 messages\n",
    "        cutoff_time = node.buffer.get_cutoff_timestamp(10)\n",
    "        \n",
    "        # Filter logic remains the same\n",
    "        if cutoff_time == float('inf'):\n",
    "            filter_meta = {\"node_id\": {\"$ne\": node.node_id}}\n",
    "        else:\n",
    "            filter_meta = {\"$or\": [\n",
    "                {\"node_id\": {\"$ne\": node.node_id}},\n",
    "                {\"timestamp\": {\"$lt\": cutoff_time}}\n",
    "            ]}\n",
    "\n",
    "        retrieved_docs = self.vector_index.query(user_text, top_k=3, filter_meta=filter_meta)\n",
    "\n",
    "        # Build context: Origin + Retrieved + Recent buffer\n",
    "        prompt_messages = []\n",
    "        \n",
    "        if node.origin_summary:\n",
    "            prompt_messages.append({\"role\": \"system\", \"content\": f\"Context: {node.origin_summary}\"})\n",
    "        \n",
    "        for doc in retrieved_docs:\n",
    "            prompt_messages.append({\"role\": \"system\", \"content\": f\"Relevant info:\\n{doc.page_content}\"})\n",
    "\n",
    "        recent_turns = node.buffer.get_recent(10)\n",
    "        for turn in recent_turns:\n",
    "            prompt_messages.append({\"role\": turn[\"role\"], \"content\": turn[\"text\"]})\n",
    "\n",
    "        prompt_messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        # Generate response\n",
    "        response_text = self.llm_client.generate_response(prompt_messages)\n",
    "        node.buffer.add_message(role=\"assistant\", text=response_text)\n",
    "        \n",
    "        # üéØ Index combined Q&A for optimal retrieval\n",
    "        self.vector_index.index_docs([{\n",
    "            \"text\": f\"Question: {user_text}\\n\\nAnswer: {response_text}\",\n",
    "            \"metadata\": {\n",
    "                \"node_id\": node.node_id,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"type\": \"qa_pair\"\n",
    "            }\n",
    "        }])\n",
    "\n",
    "        return response_text\n",
    "\n",
    "\n",
    "\n",
    "    def create_subchat(self, title: str, parent_id: Optional[str] = None) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Create a new subchat node under parent (or root if parent_id None).\n",
    "        \"\"\"\n",
    "        return self.chat_manager.create_node(title=title, parent_id=parent_id)\n",
    "    \n",
    "\n",
    "    def get_active_node(self) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Get the currently active chat node.\n",
    "        \"\"\"\n",
    "        return self.chat_manager.get_active_node()\n",
    "\n",
    "    def get_node(self, node_id: str) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Get a specific chat node by ID.\n",
    "        \"\"\"\n",
    "        return self.chat_manager.get_node(node_id)\n",
    "\n",
    "    def get_node_path(self, node_id: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the path of titles from root to the specified node.\n",
    "        \"\"\"\n",
    "        node = self.chat_manager.get_node(node_id)\n",
    "        return node.get_path() if node else []\n",
    "\n",
    "    def switch_to_node(self, node_id: str) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Switch active chat context to another node.\n",
    "        \"\"\"\n",
    "        return self.chat_manager.switch_node(node_id)\n",
    "\n",
    "    def create_new_tree(self, title: str = \"Root Conversation\") -> TreeNode:\n",
    "        \"\"\"Create a new root-level tree (main conversation).\"\"\"\n",
    "        return self.forest.create_tree(title=title, chat_manager=self.chat_manager)\n",
    "\n",
    "    def switch_to_tree(self, tree_id: str) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Switch active chat context to another root-level tree.\n",
    "        \"\"\"\n",
    "        return self.forest.switch_tree(tree_id)\n",
    "\n",
    "    def get_active_tree(self) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Get the currently active root-level tree.\n",
    "        \"\"\"\n",
    "        return self.forest.get_active_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from assembler import ChatAssembler\n",
    "\n",
    "def process_message_with_ai(assembler, user_text: str) -> str:\n",
    "    \"\"\"Common function to process user messages and get AI responses\"\"\"\n",
    "    try:\n",
    "        response = assembler.process_user_message(user_text)\n",
    "        print(f\"ü§ñ Assistant: {response}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing message: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_title_from_question(assembler, question: str, fallback_prefix: str = \"Conversation\") -> str:\n",
    "    \"\"\"Generate AI title from user question with fallback\"\"\"\n",
    "    title_prompt = f\"Generate a short, descriptive title (max 5 words) for a conversation that starts with this question: '{question}'\"\n",
    "    try:\n",
    "        generated_title = assembler.llm_client.generate_response([\n",
    "            {\"role\": \"user\", \"content\": title_prompt}\n",
    "        ])\n",
    "        return generated_title.strip().strip('\"')  # Remove any quotes\n",
    "    except Exception:\n",
    "        # fails to connect to llm or other case may b\n",
    "        raise RuntimeError(\"Failed to generate title from question\")\n",
    "\n",
    "\n",
    "\n",
    "def create_node_with_title_and_question(assembler, is_tree: bool = True, parent_id: str = None):\n",
    "    \"\"\"Create tree or subchat with AI-generated title based on first question\"\"\"\n",
    "    node_type = \"tree\" if is_tree else \"subchat\"\n",
    "    first_question = input(f\"üìù Welcome to new chat! How can I help you? {node_type}: \").strip()\n",
    "\n",
    "    while not first_question:\n",
    "        first_question = input(f\"üìù Welcome to new chat! How can I help you? {node_type}: \").strip()\n",
    "\n",
    "\n",
    "    # Generate title from question\n",
    "    fallback_prefix = \"Conversation\" if is_tree else \"Subchat\"\n",
    "    title = generate_title_from_question(assembler, first_question, fallback_prefix)\n",
    "    \n",
    "    # Create the node\n",
    "    if is_tree:\n",
    "        new_node = assembler.create_new_tree(title)\n",
    "        # No manual sync needed!\n",
    "        print(f\"üå≥ New tree created: '{title}' (ID: {new_node.node_id[:8]}...)\")\n",
    "    else:\n",
    "        new_node = assembler.create_subchat(title=title, parent_id=parent_id)\n",
    "        print(f\"üåø Subchat '{title}' created (ID: {new_node.node_id[:8]}...)\")\n",
    "        print(f\"üìç New path: {' > '.join(assembler.get_node_path(new_node.node_id))}\")\n",
    "    \n",
    "    # Process the first question\n",
    "    process_message_with_ai(assembler, first_question)\n",
    "    return new_node\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_help():\n",
    "    \"\"\"Display help commands\"\"\"\n",
    "    print(\"\\nüìñ Available Commands:\")\n",
    "    print(\"  chat/new tree          - Create new conversation tree\")\n",
    "    print(\"  subchat or follow-up   - Create child node under current\")\n",
    "    print(\"  prev/parent            - Go to parent node\")\n",
    "    print(\"  next/child [n]        - Go to nth child (default: first)\")\n",
    "    print(\"  show subchats         - List current node's children\")\n",
    "    print(\"  show history          - Show conversation messages\")\n",
    "    print(\"  show path             - Show current location\")\n",
    "    print(\"  show trees            - List all conversation trees\")\n",
    "    print(\"  switch <id>          - Switch to node by ID\")\n",
    "    print(\"  switch tree <id>     - Switch to tree by ID\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    assembler = ChatAssembler()\n",
    "\n",
    "    print(\"üå≥ Welcome to Hierarchical Chat CLI!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"  'help'                   - Show available commands help\")\n",
    "    show_help()                       # this shows available commands\n",
    "    print(\"  'exit'                   - Quit\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # # Create initial root chat\n",
    "    # root_node = assembler.create_new_tree(\"Main Conversation\")\n",
    "    # assembler.chat_manager.active_node_id = root_node.node_id\n",
    "    # print(f\"üìÅ Initial tree created: '{root_node.title}' (ID: {root_node.node_id[:8]}...)\")\n",
    "    # print(f\"üìç Current path: {' > '.join(assembler.get_node_path(root_node.node_id))}\")\n",
    "\n",
    "    while True:\n",
    "        # Show current context\n",
    "        active_node = assembler.get_active_node()\n",
    "        if active_node :\n",
    "            path = ' > '.join(assembler.get_node_path(active_node.node_id))\n",
    "            cmd = input(f\"\\n[{path}] > \").strip()\n",
    "        else:\n",
    "            print(\"‚ùå No active node selected! Please create a new tree to start conversation.\")\n",
    "            cmd = input(\"\\n[No active node] > \").strip()\n",
    "\n",
    "        if not cmd:\n",
    "            continue\n",
    "\n",
    "        # Exit command\n",
    "        if cmd.lower() == \"exit\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Help command\n",
    "        elif cmd.lower() == \"help\":\n",
    "            show_help()\n",
    "            continue\n",
    "\n",
    "        # Create new conversation tree with AI title\n",
    "        elif cmd.lower() in [\"chat\", \"new tree\"]:\n",
    "            create_node_with_title_and_question(assembler, is_tree=True)\n",
    "            continue\n",
    "\n",
    "        # Create subchat with AI title  \n",
    "        elif cmd.lower() == \"subchat\":\n",
    "            parent_id = assembler.chat_manager.active_node_id\n",
    "            create_node_with_title_and_question(assembler, is_tree=False, parent_id=parent_id)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Navigate to parent\n",
    "        elif cmd.lower() in [\"prev\", \"parent\"]:\n",
    "            current_node = assembler.get_active_node()\n",
    "            if current_node.parent:\n",
    "                assembler.switch_to_node(current_node.parent.node_id)\n",
    "                print(f\"‚¨ÜÔ∏è  Moved to parent: '{current_node.parent.title}'\")\n",
    "                print(f\"üìç Current path: {' > '.join(assembler.get_node_path(current_node.parent.node_id))}\")\n",
    "            else:\n",
    "                print(\"‚ùå Already at root node!\")\n",
    "            continue\n",
    "\n",
    "        # Navigate to child\n",
    "        elif cmd.lower().startswith(\"next\") or cmd.lower().startswith(\"child\"):\n",
    "            current_node = assembler.get_active_node()\n",
    "            if not current_node.children:\n",
    "                print(\"‚ùå No children available!\")\n",
    "                continue\n",
    "            \n",
    "            # Parse child index\n",
    "            parts = cmd.split()\n",
    "            child_index = 0\n",
    "            if len(parts) > 1:\n",
    "                try:\n",
    "                    child_index = int(parts[1])\n",
    "                except ValueError:\n",
    "                    print(\"‚ùå Invalid child index!\")\n",
    "                    continue\n",
    "            \n",
    "            if 0 <= child_index < len(current_node.children):\n",
    "                child_node = current_node.children[child_index]\n",
    "                assembler.switch_to_node(child_node.node_id)\n",
    "                print(f\"‚¨áÔ∏è  Moved to child: '{child_node.title}'\")\n",
    "                print(f\"üìç Current path: {' > '.join(assembler.get_node_path(child_node.node_id))}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Child index {child_index} out of range (0-{len(current_node.children)-1})!\")\n",
    "            continue\n",
    "\n",
    "        # Show subchats\n",
    "        elif cmd.lower() == \"show subchats\":\n",
    "            current_node = assembler.get_active_node()\n",
    "            if current_node.children:\n",
    "                print(f\"\\nüåø Children of '{current_node.title}':\")\n",
    "                for i, child in enumerate(current_node.children):\n",
    "                    print(f\"  {i}: {child.title} (ID: {child.node_id[:8]}...)\")\n",
    "            else:\n",
    "                print(\"üì≠ No subchats found.\")\n",
    "            continue\n",
    "\n",
    "        # Show conversation history\n",
    "        elif cmd.lower() == \"show history\":\n",
    "            current_node = assembler.get_active_node()\n",
    "            history = current_node.buffer.get_recent()\n",
    "            if history:\n",
    "                print(f\"\\nüí¨ Conversation history for '{current_node.title}':\")\n",
    "                print(\"-\" * 40)\n",
    "                for turn in history:\n",
    "                    role_icon = \"üë§\" if turn[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "                    print(f\"{role_icon} {turn['role'].title()}: {turn['text']}\")\n",
    "                print(\"-\" * 40)\n",
    "            else:\n",
    "                print(\"üì≠ No conversation history.\")\n",
    "            continue\n",
    "\n",
    "        # Show current path\n",
    "        elif cmd.lower() == \"show path\":\n",
    "            current_node = assembler.get_active_node()\n",
    "            path = assembler.get_node_path(current_node.node_id)\n",
    "            print(f\"üìç Current path: {' > '.join(path)}\")\n",
    "            print(f\"üÜî Node ID: {current_node.node_id}\")\n",
    "            continue\n",
    "\n",
    "        # Show all trees\n",
    "        elif cmd.lower() == \"show trees\":\n",
    "            if assembler.forest.trees_map:\n",
    "                print(\"\\nüå≥ All conversation trees:\")\n",
    "                for tree_id, tree_node in assembler.forest.trees_map.items():\n",
    "                    active_marker = \"üéØ\" if tree_id == assembler.forest.active_tree_id else \"  \"\n",
    "                    print(f\"{active_marker} {tree_node.title} (ID: {tree_id[:8]}...)\")\n",
    "            else:\n",
    "                print(\"üì≠ No trees found.\")\n",
    "            continue\n",
    "\n",
    "        # Switch to specific node\n",
    "        elif cmd.startswith(\"switch \"):\n",
    "            parts = cmd.split(maxsplit=2)\n",
    "            if len(parts) < 2:\n",
    "                print(\"‚ùå Usage: switch <node_id> or switch tree <tree_id>\")\n",
    "                continue\n",
    "\n",
    "            if parts[1].lower() == \"tree\":\n",
    "                if len(parts) < 3:\n",
    "                    print(\"‚ùå Usage: switch tree <tree_id>\")\n",
    "                    continue\n",
    "                tree_id = parts[2]\n",
    "                try:\n",
    "                    tree_node = assembler.switch_to_tree(tree_id)\n",
    "                    assembler.chat_manager.active_node_id = tree_node.node_id\n",
    "                    print(f\"üå≥ Switched to tree: '{tree_node.title}'\")\n",
    "                    print(f\"üìç Current path: {' > '.join(assembler.get_node_path(tree_node.node_id))}\")\n",
    "                except KeyError:\n",
    "                    print(f\"‚ùå Tree ID '{tree_id}' not found.\")\n",
    "            else:\n",
    "                node_id = parts[1]\n",
    "                try:\n",
    "                    node = assembler.switch_to_node(node_id)\n",
    "                    print(f\"üéØ Switched to node: '{node.title}'\")\n",
    "                    print(f\"üìç Current path: {' > '.join(assembler.get_node_path(node.node_id))}\")\n",
    "                except KeyError:\n",
    "                    print(f\"‚ùå Node ID '{node_id}' not found.\")\n",
    "            continue\n",
    "\n",
    "        # Regular chat message\n",
    "        else:\n",
    "            process_message_with_ai(assembler, cmd)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97450d7d",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dont know what is this i have to undrestand this later\n",
    "\n",
    "\n",
    "\n",
    "from graphviz import Digraph\n",
    "from core import TreeNode  # your TreeNode class\n",
    "\n",
    "def visualize_tree(root: TreeNode, filename=\"chat_tree\", view=True):\n",
    "    dot = Digraph(comment=\"Hierarchical Chat Tree\")\n",
    "\n",
    "    def add_nodes_edges(node: TreeNode):\n",
    "        # Add current node with label (title and id)\n",
    "        label = f\"{node.title}\\n({node.node_id})\"\n",
    "        dot.node(node.node_id, label)\n",
    "\n",
    "        # Add edges recursively\n",
    "        for child in node.children:\n",
    "            dot.edge(node.node_id, child.node_id)\n",
    "            add_nodes_edges(child)\n",
    "\n",
    "    add_nodes_edges(root)\n",
    "    dot.render(filename, view=view, format=\"png\")  # saves and opens the PNG file\n",
    "\n",
    "# Usage example:\n",
    "# root_node = chat_graph_manager.get_active_node()\n",
    "# visualize_tree(root_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322629f",
   "metadata": {},
   "source": [
    "## evaluation scritpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46884b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(responses, references):\n",
    "    # Could use BLEU, ROUGE, or human evaluation\n",
    "    scores = []\n",
    "    for resp, ref in zip(responses, references):\n",
    "        score = compute_similarity_metric(resp, ref)  # e.g. ROUGE\n",
    "        scores.append(score)\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def measure_context_size(contexts):\n",
    "    return sum(len(ctx.split()) for ctx in contexts) / len(contexts)\n",
    "\n",
    "def measure_context_pollution(contexts, relevant_info):\n",
    "    # Could be manual annotation or automated metric checking irrelevant content\n",
    "    pass\n",
    "\n",
    "def measure_latency(response_times):\n",
    "    return sum(response_times)/len(response_times)\n",
    "\n",
    "def measure_retrieval_metrics(retrieved, relevant_docs):\n",
    "    precision = len(set(retrieved).intersection(relevant_docs)) / len(retrieved)\n",
    "    recall = len(set(retrieved).intersection(relevant_docs)) / len(relevant_docs)\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be219af8",
   "metadata": {},
   "source": [
    "main function\n",
    "\n",
    "just focus on shubchat and show the paht\n",
    "\n",
    "create chat (main new tree )\n",
    "create subchat ( ask follow-up, sub node in existing tree )\n",
    "\n",
    "switch node to previous chat ( parent ) ( traverse between previous chat )\n",
    "switch node to next chat ( child )\n",
    "switch tree \n",
    "\n",
    "ask query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208e079",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1034046088.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31msolution mechanism\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "solution mechanism\n",
    "\n",
    "options desing\n",
    "\n",
    "ask query\n",
    "create chat ( working with buffer  )\n",
    "create subcaht ( with buffer ,retrival and parent memory )\n",
    "\n",
    "lastly easy: switching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f7748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f780cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d83c7fff",
   "metadata": {},
   "source": [
    "# üéâ **IMPLEMENTATION STATUS: HIERARCHICAL SUBCHAT ACHIEVED!**\n",
    "\n",
    "## ‚úÖ **CORE FEATURES SUCCESSFULLY IMPLEMENTED:**\n",
    "\n",
    "### **1. Hierarchical Tree Structure**\n",
    "- ‚úÖ **TreeNode** with parent/child relationships\n",
    "- ‚úÖ **LocalBuffer** per node for message isolation\n",
    "- ‚úÖ **Path tracking** (breadcrumb navigation)\n",
    "- ‚úÖ **Multiple conversation trees** (Forest management)\n",
    "\n",
    "### **2. Context Inheritance & Isolation**\n",
    "- ‚úÖ **Parent-to-child message copying** when creating subchats\n",
    "- ‚úÖ **Independent message buffers** per conversation branch\n",
    "- ‚úÖ **Context preservation** through tree hierarchy\n",
    "- ‚úÖ **Multi-level inheritance** (tested 3+ levels deep)\n",
    "\n",
    "### **3. Real LLM Integration**\n",
    "- ‚úÖ **Groq API** working with openai/gpt-oss-20b model\n",
    "- ‚úÖ **Context-aware responses** using inherited conversation history\n",
    "- ‚úÖ **Real-time AI responses** (not just echo mode)\n",
    "\n",
    "### **4. REST API (Production Ready)**\n",
    "- ‚úÖ **FastAPI endpoints** for all CRUD operations\n",
    "- ‚úÖ **Complete test suite** with automated validation\n",
    "- ‚úÖ **Error handling** and proper HTTP status codes\n",
    "- ‚úÖ **Frontend-ready** JSON responses\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **VALIDATION RESULTS:**\n",
    "\n",
    "Our comprehensive test demonstrates:\n",
    "- **Main chat** ‚Üí **2 subchats** (Pasta Types, Sauce Recipes)\n",
    "- **Deep hierarchy** ‚Üí **3-level tree** (Main ‚Üí Pasta Types ‚Üí Italian Pasta Shapes)\n",
    "- **Context inheritance** ‚Üí Message count increases correctly at each level\n",
    "- **Real AI responses** ‚Üí Groq providing contextual cooking advice\n",
    "- **Tree navigation** ‚Üí Parent-child relationships maintained\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **RECOMMENDATION: JUMP TO FRONTEND!**\n",
    "\n",
    "**YES, absolutely correct!** You've successfully achieved the core hierarchical subchat architecture. The foundation is **solid and working**. \n",
    "\n",
    "### **Why Frontend First Makes Sense:**\n",
    "1. **Visual validation** ‚Üí See your tree structure in action\n",
    "2. **User experience** ‚Üí Test real-world usage patterns  \n",
    "3. **Integration testing** ‚Üí Ensure API works with UI\n",
    "4. **Demo ready** ‚Üí Show off your research prototype\n",
    "5. **Feedback loop** ‚Üí UI will reveal what backend features need refinement\n",
    "\n",
    "### **Advanced Features Can Wait:**\n",
    "- **Semantic search** (ChromaDB) ‚Üí Nice-to-have for later\n",
    "- **Memory summarization** ‚Üí Optimization, not core functionality\n",
    "- **Persistence** ‚Üí Can add after UI validates the concept\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **NEXT STEPS:**\n",
    "\n",
    "1. **Connect your existing Next.js frontend** to the REST API\n",
    "2. **Build tree visualization** components for hierarchical navigation\n",
    "3. **Create chat UI** that shows conversation paths\n",
    "4. **Test user workflows** in the visual interface\n",
    "5. **Then** add sophisticated memory features based on real usage\n",
    "\n",
    "**Your hierarchical subchat system is research-ready! Let's build the frontend! üé®**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1cdc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/media/sda3/projects/simple build/Subchat-Trees-A-Scalable-Architecture-for-Multi-Threaded-Dialogue-and-Context-Isolation-in-LLM/backend/venv/bin/python\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "arr =[]\n",
    "for i in range(1,9):\n",
    "    arr.append(i)\n",
    "    \n",
    "while(arr[-1]!=5):\n",
    "    arr.pop\n",
    "print(arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
